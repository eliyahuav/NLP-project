{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***תרגיל 5 של הפרוייקט***"
      ],
      "metadata": {
        "id": "BWDmLmxFJX1B"
      },
      "id": "BWDmLmxFJX1B"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Data and Basic Setup**"
      ],
      "metadata": {
        "id": "3AhMc7jzxsIT"
      },
      "id": "3AhMc7jzxsIT"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/train-filtered_question_level.csv\")\n",
        "\n",
        "# Remove duplicate questions\n",
        "df = df.drop_duplicates(subset=[\"question\"], keep=\"first\")\n",
        "\n",
        "# Extract text and difficulty levels\n",
        "texts = df[\"question\"].astype(str).tolist()\n",
        "levels = df[\"level\"].tolist()\n"
      ],
      "metadata": {
        "id": "1a_xSfMMx0fI"
      },
      "id": "1a_xSfMMx0fI",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Balancing Dataset (Undersampling to Minority Class)**"
      ],
      "metadata": {
        "id": "-u5QtAfCyDdF"
      },
      "id": "-u5QtAfCyDdF"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. נגדיר את גודל היעד לפי המחלקה הקטנה ביותר (Hard)\n",
        "target_size = 15657\n",
        "\n",
        "# 2. נבצע דגימה מכל מחלקה בנפרד\n",
        "df_hard = df[df['level'] == 'hard']\n",
        "# כאן אנחנו לא עושים sample כי זה כבר הגודל שאנחנו רוצים\n",
        "\n",
        "df_medium_downsampled = df[df['level'] == 'medium'].sample(n=target_size, random_state=42)\n",
        "df_easy_downsampled = df[df['level'] == 'easy'].sample(n=target_size, random_state=42)\n",
        "\n",
        "# 3. נחבר את שלושתן יחד\n",
        "df_balanced = pd.concat([df_hard, df_medium_downsampled, df_easy_downsampled])\n",
        "\n",
        "# 4. נערבב את הדאטה (חשוב מאוד!)\n",
        "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# בדיקת תוצאה\n",
        "print(\"התפלגות חדשה:\")\n",
        "print(df_balanced['level'].value_counts())"
      ],
      "metadata": {
        "id": "eVzpuQa2yELM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "506de23e-a76d-4ff5-d4fc-a6410cdfcbba"
      },
      "id": "eVzpuQa2yELM",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "התפלגות חדשה:\n",
            "level\n",
            "easy      15657\n",
            "hard      15657\n",
            "medium    15657\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **שלב 1**"
      ],
      "metadata": {
        "id": "bLuTpTziNjn-"
      },
      "id": "bLuTpTziNjn-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **א**"
      ],
      "metadata": {
        "id": "yfcY2plyNvTU"
      },
      "id": "yfcY2plyNvTU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Choosing Maximum Sequence Length (Documentation)**"
      ],
      "metadata": {
        "id": "0N8xodXyyq2J"
      },
      "id": "0N8xodXyyq2J"
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "\n",
        "# 1. הגדרת פרמטרים\n",
        "VOCAB_SIZE = 20000\n",
        "\n",
        "# 2. אתחול הטוקנייזר (יצירת האובייקט שהיה חסר)\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
        "\n",
        "# 3. חילוץ הטקסטים מהדאטה המאוזן (ודא ששם העמודה נכון, נניח 'text')\n",
        "texts = df_balanced['question'].astype(str).tolist()\n",
        "\n",
        "# 4. התאמת הטוקנייזר על הטקסטים (שלב קריטי!)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# 5. המרה לרצפים של מספרים\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# עכשיו הסטטיסטיקות שלך יעבדו:\n",
        "sequence_lengths = [len(seq) for seq in sequences]\n",
        "avg_len = np.mean(sequence_lengths)\n",
        "percentile_95 = np.percentile(sequence_lengths, 95)\n",
        "\n",
        "print(\"Average sequence length:\", round(avg_len, 2))\n",
        "print(\"95th percentile length:\", percentile_95)\n",
        "print(\"Vocabulary size (actual):\", len(tokenizer.word_index))"
      ],
      "metadata": {
        "id": "zpEiqRM9yruA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "801cef15-7b95-4682-b6d3-d29360fb0e4a"
      },
      "id": "zpEiqRM9yruA",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sequence length: 19.32\n",
            "95th percentile length: 44.0\n",
            "Vocabulary size (actual): 58011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Padding and Truncation**"
      ],
      "metadata": {
        "id": "vp92Fj-UzFxT"
      },
      "id": "vp92Fj-UzFxT"
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 1. הגדרת המילון (למקרה שלא הוגדר בתא הזה)\n",
        "label_dict = {'easy': 0, 'medium': 1, 'hard': 2}\n",
        "\n",
        "# 2. קביעת אורך הרצף\n",
        "MAX_SEQUENCE_LENGTH = int(percentile_95)\n",
        "\n",
        "# 3. יצירת X (Padding)\n",
        "X = pad_sequences(\n",
        "    sequences,\n",
        "    maxlen=MAX_SEQUENCE_LENGTH,\n",
        "    padding=\"post\",\n",
        "    truncating=\"post\"\n",
        ")\n",
        "\n",
        "# 4. יצירת y (הפיכת הקטגוריות למטריצה של 0 ו-1)\n",
        "y_integers = df_balanced['level'].map(label_dict).values\n",
        "y = to_categorical(y_integers, num_classes=3)\n",
        "\n",
        "print(f\"Data is ready! X shape: {X.shape}, y shape: {y.shape}\")"
      ],
      "metadata": {
        "id": "eS7v5hBMzGln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4abb97dc-1463-48df-8484-b505e490975c"
      },
      "id": "eS7v5hBMzGln",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data is ready! X shape: (46971, 44), y shape: (46971, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ב**"
      ],
      "metadata": {
        "id": "trNMJWkXzRMm"
      },
      "id": "trNMJWkXzRMm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ניסוי 1**"
      ],
      "metadata": {
        "id": "eYbv8BoIOvyC"
      },
      "id": "eYbv8BoIOvyC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Embedding Layer מאומן מאפס**"
      ],
      "metadata": {
        "id": "iizuR_5POOjs"
      },
      "id": "iizuR_5POOjs"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "# --- 1. הגדרת ארכיטקטורת המודל (בדיוק כמו ה-Keras שהיה לנו) ---\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        # ב-LSTM אנחנו לוקחים את ה-hidden state האחרון\n",
        "        _, (hidden, _) = self.lstm(embedded)\n",
        "        out = self.dropout(hidden[-1])\n",
        "        return self.fc(out)\n",
        "\n",
        "# --- 2. הכנת הנתונים (הפיכה מ-NumPy ל-PyTorch Tensors) ---\n",
        "# נשתמש ב-y_integers (מספרים 0,1,2) ולא ב-y (מטריצה), כי PyTorch מעדיף ככה\n",
        "X_tensor = torch.tensor(X, dtype=torch.long)\n",
        "y_tensor = torch.tensor(y_integers, dtype=torch.long)\n",
        "\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_data, val_data = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=32)\n",
        "\n",
        "# --- 3. יצירת המודל, האופטימייזר וה-Loss ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = LSTMClassifier(VOCAB_SIZE, 100, 64, 3).to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# --- 4. לולאת האימון (עם ה-print שביקשת) ---\n",
        "epochs = 10\n",
        "print(\"Starting training (PyTorch version)...\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    # חישוב דיוק על ה-Validation (בדיוק כמו ב-Keras)\n",
        "    model.eval()\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} \"\n",
        "          f\"━━━━━━━━━━━━━━━━━━━━ \"\n",
        "          f\"loss: {running_loss/len(train_loader):.4f} - \"\n",
        "          f\"acc: {100.*correct/total:.2f}% - \"\n",
        "          f\"val_acc: {100.*val_correct/val_total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLMeMF_jO063",
        "outputId": "822eed17-894e-4126-e95c-eb17f2c1b96c"
      },
      "id": "aLMeMF_jO063",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training (PyTorch version)...\n",
            "Epoch 1/10 ━━━━━━━━━━━━━━━━━━━━ loss: 0.9719 - acc: 46.42% - val_acc: 49.49%\n",
            "Epoch 2/10 ━━━━━━━━━━━━━━━━━━━━ loss: 0.8989 - acc: 52.15% - val_acc: 52.57%\n",
            "Epoch 3/10 ━━━━━━━━━━━━━━━━━━━━ loss: 0.8479 - acc: 54.84% - val_acc: 53.24%\n",
            "Epoch 4/10 ━━━━━━━━━━━━━━━━━━━━ loss: 0.7938 - acc: 58.29% - val_acc: 52.38%\n",
            "Epoch 5/10 ━━━━━━━━━━━━━━━━━━━━ loss: 0.7364 - acc: 62.12% - val_acc: 52.85%\n",
            "Epoch 6/10 ━━━━━━━━━━━━━━━━━━━━ loss: 0.6753 - acc: 65.73% - val_acc: 52.45%\n",
            "Epoch 7/10 ━━━━━━━━━━━━━━━━━━━━ loss: 0.6144 - acc: 69.94% - val_acc: 51.81%\n",
            "Epoch 8/10 ━━━━━━━━━━━━━━━━━━━━ loss: 0.5542 - acc: 74.33% - val_acc: 52.02%\n",
            "Epoch 9/10 ━━━━━━━━━━━━━━━━━━━━ loss: 0.4931 - acc: 78.35% - val_acc: 51.74%\n",
            "Epoch 10/10 ━━━━━━━━━━━━━━━━━━━━ loss: 0.4269 - acc: 82.53% - val_acc: 51.53%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**dounload GloVe**"
      ],
      "metadata": {
        "id": "JivifCkSUfsM"
      },
      "id": "JivifCkSUfsM"
    },
    {
      "cell_type": "code",
      "source": [
        "# הורדת קובץ ה-GloVe (זה עשוי לקחת דקה-שתיים)\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWMhwpw8TnCR",
        "outputId": "c9e60be6-554c-42a5-ad1b-332783c45100"
      },
      "id": "pWMhwpw8TnCR",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-01 20:47:00--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2026-01-01 20:47:00--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2026-01-01 20:47:01--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.04MB/s    in 2m 39s  \n",
            "\n",
            "2026-01-01 20:49:40 (5.19 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating the Embedding Matrix from GloVe**"
      ],
      "metadata": {
        "id": "neKYD8gXWr1u"
      },
      "id": "neKYD8gXWr1u"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def create_embedding_matrix(word_index, vocab_size, embedding_dim=100):\n",
        "    # 1. טעינת הוקטורים מהקובץ\n",
        "    embeddings_index = {}\n",
        "    with open('glove.6B.100d.txt', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "\n",
        "    # 2. בניית המטריצה עבור ה-Vocab שלנו\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        if i < vocab_size:\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "            else:\n",
        "                # מילים שלא נמצאו יקבלו וקטור רנדומלי (או אפסים)\n",
        "                embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
        "\n",
        "    return torch.tensor(embedding_matrix, dtype=torch.float32)\n",
        "\n",
        "# יצירת המטריצה\n",
        "embedding_weights = create_embedding_matrix(tokenizer.word_index, VOCAB_SIZE)"
      ],
      "metadata": {
        "id": "dQHDjARFWnPX"
      },
      "id": "dQHDjARFWnPX",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building the model in PyTorch with Frozen/Fine-tuned support**"
      ],
      "metadata": {
        "id": "muAHGYxvXTTh"
      },
      "id": "muAHGYxvXTTh"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class GloVeLSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, weights, freeze=True):\n",
        "        super(GloVeLSTMModel, self).__init__()\n",
        "\n",
        "        # טעינת המשקולות של GloVe\n",
        "        # ה-parameter 'freeze' קובע אם המודל יעדכן את הוקטורים (Fine-tune) או לא (Frozen)\n",
        "        self.embedding = nn.Embedding.from_pretrained(weights, freeze=freeze)\n",
        "\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        _, (hidden, _) = self.lstm(embedded)\n",
        "        out = self.dropout(hidden[-1])\n",
        "        return self.fc(out)"
      ],
      "metadata": {
        "id": "xr5SxYcAXUdC"
      },
      "id": "xr5SxYcAXUdC",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Frozen model**"
      ],
      "metadata": {
        "id": "p49gsgAMXXtz"
      },
      "id": "p49gsgAMXXtz"
    },
    {
      "cell_type": "code",
      "source": [
        "model_frozen = GloVeLSTMModel(VOCAB_SIZE, 100, 64, 3, embedding_weights, freeze=True).to(device)\n",
        "# כאן תבוא לולאת האימון (אותה לולאה שכתבנו קודם)\n",
        "print(\"Training Frozen GloVe Model...\")\n",
        "# train_model(model_frozen, train_loader)"
      ],
      "metadata": {
        "id": "DVpGt14RXYqM",
        "outputId": "8a122cfb-2f7e-4b32-e314-d05e4d89a6b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "DVpGt14RXYqM",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Frozen GloVe Model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fine-tuned**"
      ],
      "metadata": {
        "id": "DmV2Dnc3XaMb"
      },
      "id": "DmV2Dnc3XaMb"
    },
    {
      "cell_type": "code",
      "source": [
        "model_finetune = GloVeLSTMModel(VOCAB_SIZE, 100, 64, 3, embedding_weights, freeze=False).to(device)\n",
        "print(\"Training Fine-tuned GloVe Model...\")\n",
        "# train_model(model_finetune, train_loader)"
      ],
      "metadata": {
        "id": "ovJN59M8XdR4",
        "outputId": "6151ffc8-e0d6-4210-8f17-51152221d6e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ovJN59M8XdR4",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Fine-tuned GloVe Model...\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}