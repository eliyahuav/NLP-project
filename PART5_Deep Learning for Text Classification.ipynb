{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***×ª×¨×’×™×œ 4   ×©×œ ×”×¤×¨×•×™×™×§×˜***"
      ],
      "metadata": {
        "id": "BWDmLmxFJX1B"
      },
      "id": "BWDmLmxFJX1B"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***×—×œ×§ ×***"
      ],
      "metadata": {
        "id": "f_aW02ESJLOo"
      },
      "id": "f_aW02ESJLOo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Cell 1: Load data and basic inspection***\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5gm7encWEu_u"
      },
      "id": "5gm7encWEu_u"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the filtered dataset from disk (Colab path)\n",
        "filtered_df = pd.read_csv(\"/content/train-filtered_question_level.csv\")\n",
        "\n",
        "# Remove duplicate questions to avoid biasing the model with repeated texts\n",
        "filtered_df = filtered_df.drop_duplicates(subset=[\"question\"], keep=\"first\")\n",
        "\n",
        "# Sanity check: show columns and first rows to verify the structure\n",
        "print(\"Columns in DataFrame:\")\n",
        "print(filtered_df.columns)\n",
        "\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(filtered_df.head())\n",
        "\n",
        "# Show global label distribution for 'level' (if exists), to understand dataset balance\n",
        "if \"level\" in filtered_df.columns:\n",
        "    print(\"\\nGlobal distribution of 'level':\")\n",
        "    print(filtered_df[\"level\"].value_counts(normalize=True))\n",
        "else:\n",
        "    print(\"\\nColumn 'level' not found in DataFrame.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "sxymC_hSmq8K",
        "outputId": "a8bc86d7-d77c-40b0-e1e5-10e86e066a41"
      },
      "id": "sxymC_hSmq8K",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/train-filtered_question_level.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4214662762.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the filtered dataset from disk (Colab path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfiltered_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/train-filtered_question_level.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Remove duplicate questions to avoid biasing the model with repeated texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train-filtered_question_level.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Cell 2: Text preprocessing (tokenization + lemmatization)***"
      ],
      "metadata": {
        "id": "zeHPEQzQJsf6"
      },
      "id": "zeHPEQzQJsf6"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag\n",
        "\n",
        "# Download required NLTK resources (run once per runtime)\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "eng_stops = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Normalize all forms of the verb \"to be\" into a single token \"be\"\n",
        "BE_FORMS = {\"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\"}\n",
        "\n",
        "\n",
        "def get_wordnet_pos(tag: str):\n",
        "    \"\"\"\n",
        "    Map POS tag from nltk.pos_tag to a WordNet POS tag.\n",
        "    This helps the lemmatizer pick the correct base form.\n",
        "    \"\"\"\n",
        "    if tag.startswith(\"J\"):\n",
        "        return wordnet.ADJ\n",
        "    if tag.startswith(\"V\"):\n",
        "        return wordnet.VERB\n",
        "    if tag.startswith(\"N\"):\n",
        "        return wordnet.NOUN\n",
        "    if tag.startswith(\"R\"):\n",
        "        return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "\n",
        "# Regex patterns for cleaning\n",
        "# Remove URLs, emails, @handles, #hashtags\n",
        "url_email_handle_re = re.compile(r\"(https?://\\S+|www\\.\\S+|\\S+@\\S+|[@#]\\w+)\", re.IGNORECASE)\n",
        "\n",
        "# Detect any digit inside a token\n",
        "digits_re = re.compile(r\"\\d\")\n",
        "\n",
        "# For NON-numeric tokens: remove everything except [a-z] and spaces\n",
        "non_letter_re = re.compile(r\"[^a-z ]+\")\n",
        "\n",
        "\n",
        "def process_text_value(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Full preprocessing for a single text value:\n",
        "    - Remove URLs, emails, and @handles/#hashtags\n",
        "    - Tokenize\n",
        "    - POS tagging\n",
        "    - Lemmatization with POS\n",
        "    - Normalize all 'be' verb forms to 'be'\n",
        "    - Any token that contains at least one digit -> '_number' (entire token)\n",
        "    - For other tokens: strip punctuation/non-letters, keep only [a-z] and spaces\n",
        "    - Finally, any token that still contains the substring 'number' is collapsed to '_number'\n",
        "    - (Optional) Remove stopwords [currently commented out]\n",
        "    - Lowercase\n",
        "    Returns a cleaned string with space-separated tokens.\n",
        "    \"\"\"\n",
        "    # Safely handle missing or non-string values\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove URLs, emails, handles, hashtags\n",
        "    t = url_email_handle_re.sub(\" \", text)\n",
        "\n",
        "    # Tokenize and POS-tag on original (cleaned) text\n",
        "    tokens = word_tokenize(t)\n",
        "    tagged = pos_tag(tokens)\n",
        "\n",
        "    lemmas = []\n",
        "    for tok, pos in tagged:\n",
        "        # Normalize 'be' forms early to reduce sparsity\n",
        "        if tok.lower() in BE_FORMS:\n",
        "            lemmas.append(\"be\")\n",
        "            continue\n",
        "\n",
        "        # Map POS tag to WordNet POS tag and lemmatize\n",
        "        wn_pos = get_wordnet_pos(pos)\n",
        "        lemma = lemmatizer.lemmatize(tok, wn_pos)\n",
        "        lemmas.append(lemma)\n",
        "\n",
        "    # Lowercase all tokens\n",
        "    lemmas = [w.lower() for w in lemmas]\n",
        "\n",
        "    intermediate = []\n",
        "    for w in lemmas:\n",
        "        # If the token contains ANY digit, replace the entire token with '_number'\n",
        "        if digits_re.search(w):\n",
        "            intermediate.append(\"_number\")\n",
        "            continue\n",
        "\n",
        "        # For non-numeric tokens: remove punctuation and non-letters\n",
        "        w2 = non_letter_re.sub(\" \", w).strip()\n",
        "        if not w2:\n",
        "            # Skip tokens that became empty after cleaning\n",
        "            continue\n",
        "\n",
        "        # If cleaning produced multiple parts (e.g. \"word-word\" -> \"word word\")\n",
        "        for part in w2.split():\n",
        "            if not part:\n",
        "                continue\n",
        "            intermediate.append(part)\n",
        "\n",
        "    # Final pass: collapse any token that still contains 'number' into '_number'\n",
        "    # This guarantees we do not get '_numbera', '_numberkm', etc.\n",
        "    clean_lemmas = []\n",
        "    for w in intermediate:\n",
        "        if \"number\" in w:\n",
        "            clean_lemmas.append(\"_number\")\n",
        "        else:\n",
        "            clean_lemmas.append(w)\n",
        "\n",
        "    clean_lemmas = [w for w in clean_lemmas if w not in eng_stops]\n",
        "\n",
        "    # Join tokens back into a single cleaned string\n",
        "    return \" \".join(clean_lemmas)\n"
      ],
      "metadata": {
        "id": "toAKXIEoMcY3"
      },
      "id": "toAKXIEoMcY3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Cell 3: Apply preprocessing to all questions***"
      ],
      "metadata": {
        "id": "qwOP_ovgRKwO"
      },
      "id": "qwOP_ovgRKwO"
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the 'question' column exists before applying preprocessing\n",
        "if \"question\" not in filtered_df.columns:\n",
        "    raise KeyError(\"The DataFrame does not contain a 'question' column.\")\n",
        "\n",
        "# Apply the preprocessing function to every question in the dataset\n",
        "# This creates a new column 'question_clean' that contains the normalized text\n",
        "filtered_df[\"clean_text\"] = filtered_df[\"question\"].apply(process_text_value)\n",
        "\n",
        "# Inspect a few examples to verify that preprocessing works as expected\n",
        "print(\"Original vs. cleaned examples:\\n\")\n",
        "for i in range(5):\n",
        "    print(f\"--- Example {i+1} ---\")\n",
        "    print(\"Original :\", filtered_df.loc[filtered_df.index[i], \"question\"])\n",
        "    print(\"Cleaned  :\", filtered_df.loc[filtered_df.index[i], \"clean_text\"])\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "VPjsviBsQxIg"
      },
      "id": "VPjsviBsQxIg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Cell 4: TF-IDF vectorization of the preprocessed questions***"
      ],
      "metadata": {
        "id": "ovO3e28_RrXX"
      },
      "id": "ovO3e28_RrXX"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Safety check: make sure 'clean_text' exists\n",
        "if \"clean_text\" not in filtered_df.columns:\n",
        "    raise KeyError(\"The DataFrame does not contain a 'clean_text' column. \"\n",
        "                   \"Run the preprocessing cell first.\")\n",
        "\n",
        "# Define a TF-IDF vectorizer\n",
        "# max_features limits vocabulary size to the most frequent terms\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=10000,   # limit vocabulary size (you can tune this later)\n",
        "    ngram_range=(1, 1),   # unigrams only\n",
        ")\n",
        "\n",
        "# Fit TF-IDF on the entire cleaned corpus and transform it to a sparse matrix\n",
        "# Each row = one question, each column = one term from the vocabulary\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(filtered_df[\"clean_text\"])\n",
        "\n",
        "print(\"TF-IDF matrix shape (n_samples, n_features):\", X_tfidf.shape)\n",
        "print(\"(Num of documents, max_features)\")\n",
        "\n",
        "# Optional: extract labels if you need them later for supervised models / evaluation\n",
        "if \"level\" in filtered_df.columns:\n",
        "    y = filtered_df[\"level\"].values\n",
        "    print(\"Labels vector shape:\", y.shape)\n",
        "else:\n",
        "    y = None\n",
        "    print(\"No 'level' column found. y is set to None.\")\n",
        "\n",
        "# Show a small sample of feature names for sanity check\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "print(\"\\nVocabulary size (len(feature_names)):\", len(feature_names))\n",
        "print(\"First 30 features:\\n\", feature_names[:60])\n"
      ],
      "metadata": {
        "id": "tqDPZN-mRvtC"
      },
      "id": "tqDPZN-mRvtC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Cell 5: Run K-Means for different K values***"
      ],
      "metadata": {
        "id": "EcDOCcPqfNof"
      },
      "id": "EcDOCcPqfNof"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Choose several K values\n",
        "k_values = [2, 7 ,15]\n",
        "# k_values = [2]\n",
        "inertia_scores = []\n",
        "silhouette_scores = []\n",
        "\n",
        "print(\"Running K-Means on TF-IDF matrix... (may take a bit)\")\n",
        "\n",
        "for k in k_values:\n",
        "    print(f\"\\n--- K = {k} ---\")\n",
        "\n",
        "    # KMeans (using smart initialization k-means++)\n",
        "    kmeans = KMeans(\n",
        "        n_clusters=k,\n",
        "        init=\"k-means++\",\n",
        "        max_iter=300,\n",
        "        random_state=42,\n",
        "        n_init=10\n",
        "    )\n",
        "\n",
        "    # Fit on full TF-IDF matrix\n",
        "    kmeans.fit(X_tfidf)\n",
        "\n",
        "    # Inertia (Elbow)\n",
        "    inertia = kmeans.inertia_\n",
        "    inertia_scores.append(inertia)\n",
        "\n",
        "    # Silhouette score (requires >1 cluster)\n",
        "    sil_score = silhouette_score(X_tfidf, kmeans.labels_, metric='euclidean')\n",
        "    silhouette_scores.append(sil_score)\n",
        "\n",
        "    print(f\"Inertia: {inertia}\")\n",
        "    print(f\"Silhouette Score: {sil_score}\")\n"
      ],
      "metadata": {
        "id": "maiFWWISfQ8i"
      },
      "id": "maiFWWISfQ8i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cell 6: Dimensionality reduction for clustering (TruncatedSVD)**"
      ],
      "metadata": {
        "id": "aZHPgbrym66T"
      },
      "id": "aZHPgbrym66T"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# We reduce dimensionality because TF-IDF has many features and is sparse.\n",
        "# TruncatedSVD is PCA-like but works directly on sparse matrices.\n",
        "svd = TruncatedSVD(\n",
        "    n_components=50,   # number of latent dimensions (you can tune this)\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit SVD on the TF-IDF matrix and transform it to a dense lower-dimensional space\n",
        "X_svd = svd.fit_transform(X_tfidf)\n",
        "\n",
        "print(\"Original TF-IDF shape :\", X_tfidf.shape)\n",
        "print(\"Reduced SVD shape     :\", X_svd.shape)\n",
        "\n",
        "# Sum of explained variance ratio gives an idea how much information we kept\n",
        "explained = svd.explained_variance_ratio_.sum()\n",
        "print(f\"Total explained variance (approx): {explained:.3f}\")\n"
      ],
      "metadata": {
        "id": "hARCTg3ilzVP"
      },
      "id": "hARCTg3ilzVP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Cell 7: DBSCAN clustering on reduced space and comparison***"
      ],
      "metadata": {
        "id": "CaaQVZ42nC-r"
      },
      "id": "CaaQVZ42nC-r"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ×”×’×“×¨×ª ×’×•×“×œ ×”××“×’×\n",
        "SAMPLE_SIZE = 20000\n",
        "\n",
        "print(f\"Original dataset size: {X_svd.shape[0]}\")\n",
        "\n",
        "# 1. ×“×’×™××” (×× ×¦×¨×™×š)\n",
        "if X_svd.shape[0] > SAMPLE_SIZE:\n",
        "    print(f\"Dataset is too large. Sampling {SAMPLE_SIZE} random points...\")\n",
        "    np.random.seed(42)\n",
        "    indices = np.random.choice(X_svd.shape[0], SAMPLE_SIZE, replace=False)\n",
        "    X_subset = X_svd[indices]\n",
        "    df_subset = filtered_df.iloc[indices].copy()\n",
        "else:\n",
        "    X_subset = X_svd\n",
        "    df_subset = filtered_df.copy()\n",
        "\n",
        "print(f\"Running DBSCAN on {X_subset.shape[0]} samples...\")\n",
        "\n",
        "# 2. ×”×¨×¦×ª DBSCAN\n",
        "dbscan = DBSCAN(eps=0.1, min_samples=50, metric='cosine', n_jobs=-1) #.........................................................................\n",
        "dbscan_labels = dbscan.fit_predict(X_subset)\n",
        "print(\"DBSCAN finished!\")\n",
        "\n",
        "# 3. ×©××™×¨×ª ×ª×•×¦××•×ª\n",
        "df_subset['dbscan_cluster'] = dbscan_labels\n",
        "\n",
        "# 4. × ×™×ª×•×— ×‘×¡×™×¡×™\n",
        "unique_labels, counts = np.unique(dbscan_labels, return_counts=True)\n",
        "print(\"\\nCluster counts (Label -1 represents 'Noise'):\")\n",
        "print(dict(zip(unique_labels, counts)))\n",
        "\n",
        "# 5. ×—×™×©×•×‘ Silhouette Score (×‘×˜×•×—)\n",
        "# ×‘×“×™×§×” ×›××” ××©×›×•×œ×•×ª ×™×© ×©××™× × ×¨×¢×©\n",
        "n_clusters_real = len(set(dbscan_labels) - {-1})\n",
        "\n",
        "if n_clusters_real >= 2:\n",
        "    non_noise_mask = dbscan_labels != -1\n",
        "    sil = silhouette_score(X_subset[non_noise_mask], dbscan_labels[non_noise_mask])\n",
        "    print(f\"\\nDBSCAN Silhouette Score (excluding noise): {sil:.4f}\")\n",
        "else:\n",
        "    print(f\"\\nCould not calculate Silhouette Score: Found {n_clusters_real} real clusters.\")\n",
        "    print(\"Silhouette Score requires at least 2 distinct clusters.\")\n",
        "    print(\"Try adjusting 'eps' (lower it to split clusters) or 'min_samples'.\")"
      ],
      "metadata": {
        "id": "U7aVlLXJnE4y"
      },
      "id": "U7aVlLXJnE4y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Cell 8: Analyze DBSCAN Clusters (Top Keywords & Sample Questions)***"
      ],
      "metadata": {
        "id": "amY2JQJ69tLK"
      },
      "id": "amY2JQJ69tLK"
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 8: Questions by Category (Clusters) ===\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"Printing Questions by Cluster (DBSCAN Results)...\")\n",
        "\n",
        "# 1. ×•×™×“×•× ×©××©×ª× ×™ ×”××“×’× ×§×™×™××™×\n",
        "if 'df_subset' not in locals():\n",
        "    df_subset = filtered_df\n",
        "\n",
        "# 2. ×”×•×¡×¤×ª ×”×ª×•×•×™×•×ª ×œ××“×’×\n",
        "df_subset['dbscan_cluster'] = dbscan_labels\n",
        "\n",
        "unique_labels = np.sort(np.unique(dbscan_labels))\n",
        "\n",
        "# ××¢×‘×¨ ×¢×œ ×›×œ ××©×›×•×œ ×•×”×¦×’×ª ×”×©××œ×•×ª ×‘×œ×‘×“\n",
        "for label in unique_labels:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "\n",
        "    if label == -1:\n",
        "        print(f\"CATEGORY: NOISE / UNCLASSIFIED QUESTIONS\")\n",
        "    else:\n",
        "        print(f\"CATEGORY: CLUSTER {label}\")\n",
        "\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # ×©×œ×™×¤×ª ×›×œ ×”×©××œ×•×ª ×”×©×™×™×›×•×ª ×œ××©×›×•×œ ×”× ×•×›×—×™\n",
        "    cluster_questions = df_subset[df_subset['dbscan_cluster'] == label]['question'].values\n",
        "    cluster_size = len(cluster_questions)\n",
        "\n",
        "    print(f\"Total questions in this category: {cluster_size}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # ×”×¦×’×ª ×“×•×’×××•×ª (× ×¦×™×’ ×¢×“ 15 ×©××œ×•×ª ×›×“×™ ×œ× ×œ×”×¦×™×£, ××ª×” ×™×›×•×œ ×œ×©× ×•×ª ××ª ×”××¡×¤×¨)\n",
        "    display_count = min(15, cluster_size)\n",
        "    for i in range(display_count):\n",
        "        print(f\" {i+1}. {cluster_questions[i]}\")\n",
        "\n",
        "    if cluster_size > display_count:\n",
        "        print(f\" ... and {cluster_size - display_count} more questions.\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Analysis Complete.\")"
      ],
      "metadata": {
        "id": "vgP9kghQ8hcl"
      },
      "id": "vgP9kghQ8hcl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cell 9: Visualization by t-SNE & PCA**"
      ],
      "metadata": {
        "id": "b8tdge-A-PAZ"
      },
      "id": "b8tdge-A-PAZ"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ×‘×“×™×§×ª ××©×ª× ×™×\n",
        "if 'X_subset' not in locals() or 'dbscan_labels' not in locals():\n",
        "    raise ValueError(\"Missing variables! Please run the sampling & DBSCAN cell first.\")\n",
        "\n",
        "labels_to_plot = dbscan_labels\n",
        "X_plot_data = X_subset\n",
        "\n",
        "# ×”×’×‘×œ×ª ×“×’×™××” ×œ-t-SNE (××•×¤×˜×™××™×–×¦×™×”)\n",
        "TSNE_LIMIT = 5000\n",
        "if len(labels_to_plot) > TSNE_LIMIT:\n",
        "    print(f\"Subsampling to {TSNE_LIMIT} points for t-SNE calculation...\")\n",
        "    np.random.seed(42)\n",
        "    indices_tsne = np.random.choice(len(labels_to_plot), TSNE_LIMIT, replace=False)\n",
        "    X_tsne_input = X_plot_data[indices_tsne]\n",
        "    labels_tsne = labels_to_plot[indices_tsne]\n",
        "else:\n",
        "    X_tsne_input = X_plot_data\n",
        "    labels_tsne = labels_to_plot\n",
        "\n",
        "print(\"Running t-SNE... (This may take a minute)\")\n",
        "tsne = TSNE(n_components=2, random_state=42, init='pca', learning_rate='auto', perplexity=30)\n",
        "X_tsne_2d = tsne.fit_transform(X_tsne_input)\n",
        "\n",
        "# ×™×¦×™×¨×ª ×”×ª×¨×©×™×\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "# ×”×¤×¨×“×” ×‘×™×Ÿ ×¨×¢×© ×œ××©×›×•×œ×•×ª\n",
        "noise_mask = (labels_tsne == -1)\n",
        "cluster_mask = ~noise_mask\n",
        "\n",
        "# ×¦×™×•×¨ ×”×¨×¢×© (Noise) ×‘××¤×•×¨ ×‘×”×™×¨ ×××•×“\n",
        "plt.scatter(X_tsne_2d[noise_mask, 0], X_tsne_2d[noise_mask, 1],\n",
        "            c='lightgray', label='Noise (-1)', s=5, alpha=0.2)\n",
        "\n",
        "# ×¦×™×•×¨ ×”××©×›×•×œ×•×ª (Clusters)\n",
        "scatter = plt.scatter(X_tsne_2d[cluster_mask, 0], X_tsne_2d[cluster_mask, 1],\n",
        "            c=labels_tsne[cluster_mask], cmap='tab20', s=20, alpha=0.8, edgecolors='none')\n",
        "\n",
        "plt.title('DBSCAN Clusters Visualization - t-SNE Projection', fontsize=15)\n",
        "plt.xlabel('t-SNE dimension 1')\n",
        "plt.ylabel('t-SNE dimension 2')\n",
        "\n",
        "# ×”×•×¡×¤×ª Colorbar ×¨×§ ×× ×™×© ××©×›×•×œ×•×ª\n",
        "if np.any(cluster_mask):\n",
        "    plt.colorbar(scatter, label='Cluster ID')\n",
        "\n",
        "plt.legend(markerscale=3, loc='upper right')\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CEvVfVNW-Pit"
      },
      "id": "CEvVfVNW-Pit",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cell 10: Calculating Purity Score**"
      ],
      "metadata": {
        "id": "6gqN2THyTU7A"
      },
      "id": "6gqN2THyTU7A"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import adjusted_rand_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ×•×•×“× ×©××ª×” ×œ×•×§×— ××ª ×”×ª×•×•×™×•×ª ×”××§×•×¨×™×•×ª ××”××“×’× ×©×”×¨×¦×ª ×¢×œ×™×• ××ª ×”-DBSCAN\n",
        "true_labels = df_subset['level']\n",
        "\n",
        "# ×—×™×©×•×‘ ×”-ARI\n",
        "ari_score = adjusted_rand_score(true_labels, dbscan_labels)\n",
        "\n",
        "# ×¤×•× ×§×¦×™×” ×œ×—×™×©×•×‘ Purity\n",
        "def calculate_purity(y_true, y_pred):\n",
        "    contingency_matrix = pd.crosstab(y_true, y_pred)\n",
        "    return np.sum(contingency_matrix.max(axis=0)) / np.sum(contingency_matrix.values)\n",
        "\n",
        "purity_score = calculate_purity(true_labels, dbscan_labels)\n",
        "\n",
        "print(f\"Adjusted Rand Index (ARI): {ari_score:.4f}\")\n",
        "print(f\"Purity Score: {purity_score:.4f}\")"
      ],
      "metadata": {
        "id": "zm86jHqeSExp"
      },
      "id": "zm86jHqeSExp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **×—×œ×§ ×‘- ×—×™×œ×•×¥ ×§×©×¨×™×**"
      ],
      "metadata": {
        "id": "m8RpnwArCWMt"
      },
      "id": "m8RpnwArCWMt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cell 11: Run TF-IDF and find the top words**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "j_Y26QqtFcLM"
      },
      "id": "j_Y26QqtFcLM"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def print_just_keywords_by_cluster(df, cluster_col, text_col, top_n=50):\n",
        "    # 1. ×”×’×“×¨×ª ×”×•×§×˜×•×¨×™×™×–×¨ ×œ× ×™×§×•×™ ××™×œ×™× × ×¤×•×¦×•×ª\n",
        "    vec = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "    # 2. ××™×—×•×“ ×”×˜×§×¡×˜×™× ×œ×¤×™ ××©×›×•×œ (×“×™×œ×•×’ ×¢×œ ×¨×¢×©)\n",
        "    relevant_data = df[df[cluster_col] != -1]\n",
        "    docs = relevant_data.groupby(cluster_col)[text_col].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    # 3. ×—×™×©×•×‘ TF-IDF\n",
        "    tfidf_matrix = vec.fit_transform(docs)\n",
        "    features = vec.get_feature_names_out()\n",
        "\n",
        "    print(\"Printing Keywords for Each Cluster...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    for i, cluster_id in enumerate(docs.index):\n",
        "        # ×©×œ×™×¤×ª ×”×¦×™×•× ×™× ×•××™×•×Ÿ ×”××™×œ×™× ××”×—×–×§×” ×œ×—×œ×©×”\n",
        "        row = tfidf_matrix[i].toarray().flatten()\n",
        "        all_word_indices = row.argsort()[::-1]\n",
        "\n",
        "        # ×œ×§×™×—×ª ×”××™×œ×™× ×©×™×© ×œ×”×Ÿ ××©×§×œ ×—×™×•×‘×™ (×¢×“ ×”×›××•×ª ×©×‘×™×§×©× ×•)\n",
        "        keywords = [features[idx] for idx in all_word_indices if row[idx] > 0][:top_n]\n",
        "\n",
        "        cluster_size = len(df[df[cluster_col] == cluster_id])\n",
        "\n",
        "        # ×”×“×¤×¡×ª ×”×›×•×ª×¨×ª ×•×”××™×œ×™×\n",
        "        print(f\"CATEGORY: CLUSTER {cluster_id} ({cluster_size} questions)\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        # ×”×“×¤×¡×” ×©×œ ×”××™×œ×™× ××•×¤×¨×“×•×ª ×‘×¤×¡×™×§×™×\n",
        "        print(\", \".join(keywords))\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
        "\n",
        "# ×”×¨×¦×ª ×”×¤×•× ×§×¦×™×” - ×ª×“×¤×™×¡ ××ª 50 ×”××™×œ×™× ×”××•×‘×™×œ×•×ª ×œ×›×œ ××©×›×•×œ\n",
        "print_just_keywords_by_cluster(df_subset, 'dbscan_cluster', 'question', top_n=50)"
      ],
      "metadata": {
        "id": "ZykeZyGm7b2i"
      },
      "id": "ZykeZyGm7b2i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ·ï¸ Final Topic Identification per Cluster\n",
        "\n",
        "The following table summarizes the semantic interpretation of each cluster identified by DBSCAN. The labels were derived by analyzing the top TF-IDF keywords within each group:\n",
        "\n",
        "| Cluster ID | Identified Topic Title |\n",
        "| :--- | :--- |\n",
        "| **0** | Historical Chronology & Governance |\n",
        "| **1** | Dated Biographies & Global Events |\n",
        "| **2** | Music Bands & Rock Culture |\n",
        "| **3** | Authorship & Literary Works |\n",
        "| **4** | Geographic Landmarks & Districts |\n",
        "| **5** | Academic Institutions & Research |\n",
        "| **6** | Professional Peer Comparisons |\n",
        "| **7** | US Counties & Civil Geography |\n",
        "| **8** | Occupational Scopes & Shared Roles |\n",
        "| **9** | Aviation & International Airports |\n",
        "| **10** | Creative Works & Shared Nationalities |\n",
        "| **11** | Biographical Name Studies (Common Names) |\n",
        "| **12** | Gaming, Boards & Software Development |\n",
        "| **13** | Historical Battles & Operatic Works |\n",
        "| **14** | Biographical Milestones & Relationships |\n",
        "| **15** | Periodicals, Magazines & Publishing |\n",
        "| **16** | US Government & Military Service |\n",
        "| **17** | Global Urban Centers & Provinces |\n",
        "| **18** | Music Theory & Classical Genres |\n",
        "| **19** | Parliamentary Bodies & Political Parties |\n",
        "| **20** | Film Festivals & International Directors |\n",
        "| **21** | Territorial Origins & Bordering Regions |\n",
        "| **22** | Team Sports & Athletic Positions |\n",
        "| **23** | Corporate Foundations & Business Entities |\n",
        "| **24** | Scientific Laws & Cultural Formulas |\n",
        "| **25** | Comparative Birth Dates & Genealogy |\n",
        "| **26** | Entertainment Media & Commodity Types |\n",
        "| **27** | Biological Taxonomy (Genus & Species) |\n",
        "| **28** | Cinematic Genres & Film Productions |\n",
        "| **29** | Vocalists & Contemporary Songwriters |\n",
        "| **30** | Social Activism & Political Figures |\n",
        "| **31** | Public Recognition & Historical Legacy |\n",
        "| **32** | Awards, Honors & Academic Prizes |\n",
        "| **33** | Music Studio Albums & Discography |\n",
        "| **34** | State Governance & Regional Landmarks |\n",
        "| **35** | Professional Athlete Rankings (Tennis/Hockey) |\n",
        "| **36** | Work Environments & Intellectual Contributions |\n",
        "| **37** | Botany & Floral Species |\n",
        "| **38** | Institutional Foundations & Societies |\n",
        "| **39** | Screenwriting & Stage Direction |\n",
        "| **40** | Animated Television & Series Production |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "61tylsxOR-Oe"
      },
      "id": "61tylsxOR-Oe"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}