{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***תרגיל 5 של הפרוייקט***"
      ],
      "metadata": {
        "id": "BWDmLmxFJX1B"
      },
      "id": "BWDmLmxFJX1B"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Data and Basic Setup**"
      ],
      "metadata": {
        "id": "3AhMc7jzxsIT"
      },
      "id": "3AhMc7jzxsIT"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/train-filtered_question_level.csv\")\n",
        "\n",
        "# Remove duplicate questions\n",
        "df = df.drop_duplicates(subset=[\"question\"], keep=\"first\")\n",
        "\n",
        "# Extract text and difficulty levels\n",
        "texts = df[\"question\"].astype(str).tolist()\n",
        "levels = df[\"level\"].tolist()\n"
      ],
      "metadata": {
        "id": "1a_xSfMMx0fI"
      },
      "id": "1a_xSfMMx0fI",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Balancing Dataset (Undersampling to Minority Class)**"
      ],
      "metadata": {
        "id": "-u5QtAfCyDdF"
      },
      "id": "-u5QtAfCyDdF"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. נגדיר את גודל היעד לפי המחלקה הקטנה ביותר (Hard)\n",
        "target_size = 15657\n",
        "\n",
        "# 2. נבצע דגימה מכל מחלקה בנפרד\n",
        "df_hard = df[df['level'] == 'hard']\n",
        "# כאן אנחנו לא עושים sample כי זה כבר הגודל שאנחנו רוצים\n",
        "\n",
        "df_medium_downsampled = df[df['level'] == 'medium'].sample(n=target_size, random_state=42)\n",
        "df_easy_downsampled = df[df['level'] == 'easy'].sample(n=target_size, random_state=42)\n",
        "\n",
        "# 3. נחבר את שלושתן יחד\n",
        "df_balanced = pd.concat([df_hard, df_medium_downsampled, df_easy_downsampled])\n",
        "\n",
        "# 4. נערבב את הדאטה (חשוב מאוד!)\n",
        "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# בדיקת תוצאה\n",
        "print(\"התפלגות חדשה:\")\n",
        "print(df_balanced['level'].value_counts())"
      ],
      "metadata": {
        "id": "eVzpuQa2yELM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "506de23e-a76d-4ff5-d4fc-a6410cdfcbba"
      },
      "id": "eVzpuQa2yELM",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "התפלגות חדשה:\n",
            "level\n",
            "easy      15657\n",
            "hard      15657\n",
            "medium    15657\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **שלב 1**"
      ],
      "metadata": {
        "id": "bLuTpTziNjn-"
      },
      "id": "bLuTpTziNjn-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **א**"
      ],
      "metadata": {
        "id": "yfcY2plyNvTU"
      },
      "id": "yfcY2plyNvTU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Choosing Maximum Sequence Length (Documentation)**"
      ],
      "metadata": {
        "id": "0N8xodXyyq2J"
      },
      "id": "0N8xodXyyq2J"
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "\n",
        "# 1. הגדרת פרמטרים\n",
        "VOCAB_SIZE = 20000\n",
        "\n",
        "# 2. אתחול הטוקנייזר (יצירת האובייקט שהיה חסר)\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
        "\n",
        "# 3. חילוץ הטקסטים מהדאטה המאוזן (ודא ששם העמודה נכון, נניח 'text')\n",
        "texts = df_balanced['question'].astype(str).tolist()\n",
        "\n",
        "# 4. התאמת הטוקנייזר על הטקסטים (שלב קריטי!)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# 5. המרה לרצפים של מספרים\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# עכשיו הסטטיסטיקות שלך יעבדו:\n",
        "sequence_lengths = [len(seq) for seq in sequences]\n",
        "avg_len = np.mean(sequence_lengths)\n",
        "percentile_95 = np.percentile(sequence_lengths, 95)\n",
        "\n",
        "print(\"Average sequence length:\", round(avg_len, 2))\n",
        "print(\"95th percentile length:\", percentile_95)\n",
        "print(\"Vocabulary size (actual):\", len(tokenizer.word_index))"
      ],
      "metadata": {
        "id": "zpEiqRM9yruA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "801cef15-7b95-4682-b6d3-d29360fb0e4a"
      },
      "id": "zpEiqRM9yruA",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average sequence length: 19.32\n",
            "95th percentile length: 44.0\n",
            "Vocabulary size (actual): 58011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Padding and Truncation**"
      ],
      "metadata": {
        "id": "vp92Fj-UzFxT"
      },
      "id": "vp92Fj-UzFxT"
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Chosen maximum sequence length\n",
        "MAX_SEQUENCE_LENGTH = int(percentile_95)\n",
        "\n",
        "# Apply padding and truncation\n",
        "X_padded = pad_sequences(\n",
        "    sequences,\n",
        "    maxlen=MAX_SEQUENCE_LENGTH,\n",
        "    padding=\"post\",\n",
        "    truncating=\"post\"\n",
        ")\n",
        "\n",
        "print(\"Final input shape:\", X_padded.shape)\n"
      ],
      "metadata": {
        "id": "eS7v5hBMzGln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c87fe547-a40b-4c4d-bda2-feaec35eb5de"
      },
      "id": "eS7v5hBMzGln",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final input shape: (46971, 44)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ב**"
      ],
      "metadata": {
        "id": "trNMJWkXzRMm"
      },
      "id": "trNMJWkXzRMm"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}