{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***תרגיל 5 של הפרוייקט***"
      ],
      "metadata": {
        "id": "BWDmLmxFJX1B"
      },
      "id": "BWDmLmxFJX1B"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Data and Basic Setup**"
      ],
      "metadata": {
        "id": "3AhMc7jzxsIT"
      },
      "id": "3AhMc7jzxsIT"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/train-filtered_question_level.csv\")\n",
        "\n",
        "# Remove duplicate questions\n",
        "df = df.drop_duplicates(subset=[\"question\"], keep=\"first\")\n",
        "\n",
        "# Extract text and difficulty levels\n",
        "texts = df[\"question\"].astype(str).tolist()\n",
        "levels = df[\"level\"].tolist()\n"
      ],
      "metadata": {
        "id": "1a_xSfMMx0fI"
      },
      "id": "1a_xSfMMx0fI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Balancing Dataset (Undersampling to Minority Class)**"
      ],
      "metadata": {
        "id": "-u5QtAfCyDdF"
      },
      "id": "-u5QtAfCyDdF"
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# # 1. נגדיר את גודל היעד לפי המחלקה הקטנה ביותר (Hard)\n",
        "# target_size = 15657\n",
        "\n",
        "# # 2. נבצע דגימה מכל מחלקה בנפרד\n",
        "# df_hard = df[df['level'] == 'hard']\n",
        "# # כאן אנחנו לא עושים sample כי זה כבר הגודל שאנחנו רוצים\n",
        "\n",
        "# df_medium_downsampled = df[df['level'] == 'medium'].sample(n=target_size, random_state=42)\n",
        "# df_easy_downsampled = df[df['level'] == 'easy'].sample(n=target_size, random_state=42)\n",
        "\n",
        "# # 3. נחבר את שלושתן יחד\n",
        "# df_balanced = pd.concat([df_hard, df_medium_downsampled, df_easy_downsampled])\n",
        "\n",
        "# # 4. נערבב את הדאטה (חשוב מאוד!)\n",
        "# df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# # בדיקת תוצאה\n",
        "# print(\"התפלגות חדשה:\")\n",
        "# print(df_balanced['level'].value_counts())"
      ],
      "metadata": {
        "id": "eVzpuQa2yELM"
      },
      "id": "eVzpuQa2yELM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. הגדרת גודל המדגם לכל קטגוריה (היפר-פרמטר של שלב הניסויים)\n",
        "# התחלה עם 1000 מכל אחת עוזרת לבדיקה מהירה (\"מתחילים בקטן\" לפי ההוראות)\n",
        "target_size = 7000\n",
        "\n",
        "# 2. דגימה מאוזנת מכל הקטגוריות בבת אחת\n",
        "# groupby מבטיח שנתייחס לכל רמת קושי בנפרד\n",
        "df_balanced = df.groupby('level').apply(lambda x: x.sample(n=target_size, random_state=42)).reset_index(drop=True)\n",
        "\n",
        "# 3. ערבוב הדאטה (Shuffle) - קריטי בלמידה עמוקה!\n",
        "# כדי שה-Batch לא יכיל רק מחלקה אחת בזמן האימון, מה שיהרוס את הלמידה\n",
        "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# בדיקת התפלגות כפי שנדרש בשלב ה-EDA ובניתוח הדאטה\n",
        "print(\"התפלגות חדשה ומאוזנת:\")\n",
        "print(df_balanced['level'].value_counts())"
      ],
      "metadata": {
        "id": "7NWLJ-kVHMFU"
      },
      "id": "7NWLJ-kVHMFU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **שלב 1**"
      ],
      "metadata": {
        "id": "bLuTpTziNjn-"
      },
      "id": "bLuTpTziNjn-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **א**"
      ],
      "metadata": {
        "id": "yfcY2plyNvTU"
      },
      "id": "yfcY2plyNvTU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Choosing Maximum Sequence Length (Documentation)**"
      ],
      "metadata": {
        "id": "0N8xodXyyq2J"
      },
      "id": "0N8xodXyyq2J"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "zbtPlPa_H-ht"
      },
      "id": "zbtPlPa_H-ht",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 1. חילוץ הטקסטים\n",
        "texts = df_balanced['question'].astype(str).tolist()\n",
        "\n",
        "# 2. אתחול והתאמת הטוקנייזר על כל המילים (בלי הגבלה שרירותית של 20,000)\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# 3. הגדרת VOCAB_SIZE האמיתי (קריטי למטריצת ה-Embedding)\n",
        "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "print(f\"Actual Vocabulary size: {VOCAB_SIZE}\")\n",
        "\n",
        "# 4. המרה לרצפים וביצוע Padding\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "MAX_LEN = int(np.percentile([len(seq) for seq in sequences], 95)) # אורך שחוסם 95% מהמשפטים\n",
        "X = pad_sequences(sequences, maxlen=MAX_LEN, padding='post')\n"
      ],
      "metadata": {
        "id": "zpEiqRM9yruA"
      },
      "id": "zpEiqRM9yruA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Padding and Truncation**"
      ],
      "metadata": {
        "id": "vp92Fj-UzFxT"
      },
      "id": "vp92Fj-UzFxT"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. הגדרת המילון (כפי שעשית)\n",
        "label_dict = {'easy': 0, 'medium': 1, 'hard': 2}\n",
        "y_integers = df_balanced['level'].map(label_dict).values\n",
        "\n",
        "# 2. חלוקה מרובדת (Stratify) כדי לשמור על איזון באחוזים\n",
        "# חלוקה ראשונה: מוציאים 15% לטסט סופי\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y_integers,\n",
        "    test_size=0.15,\n",
        "    random_state=42,\n",
        "    stratify=y_integers # מבטיח איזון\n",
        ")\n",
        "\n",
        "# חלוקה שנייה: פיצול היתרה לאימון וולידציה (15% מהסך הכל)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val,\n",
        "    test_size=0.176, # 0.15 / 0.85\n",
        "    random_state=42,\n",
        "    stratify=y_train_val # מבטיח איזון\n",
        ")\n",
        "\n",
        "print(f\"Train size: {len(X_train)} | Val size: {len(X_val)} | Test size: {len(X_test)}\")"
      ],
      "metadata": {
        "id": "eS7v5hBMzGln"
      },
      "id": "eS7v5hBMzGln",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# המרה ל-Tensors\n",
        "# X הוא LongTensor כי הוא מכיל אינדקסים של מילים\n",
        "# y הוא LongTensor כי CrossEntropyLoss מצפה לאינדקסים של מחלקות\n",
        "train_ds = TensorDataset(torch.LongTensor(X_train), torch.LongTensor(y_train))\n",
        "val_ds   = TensorDataset(torch.LongTensor(X_val), torch.LongTensor(y_val))\n",
        "test_ds  = TensorDataset(torch.LongTensor(X_test), torch.LongTensor(y_test))\n",
        "\n",
        "# יצירת Loaders\n",
        "# shuffle=True רק באימון כדי שהמודל לא ילמד את סדר השאלות\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds, batch_size=32, shuffle=False)\n",
        "\n",
        "print(\"PyTorch DataLoaders are ready!\")"
      ],
      "metadata": {
        "id": "57jJSQgaANqX"
      },
      "id": "57jJSQgaANqX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ב**"
      ],
      "metadata": {
        "id": "trNMJWkXzRMm"
      },
      "id": "trNMJWkXzRMm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ניסוי 1**"
      ],
      "metadata": {
        "id": "eYbv8BoIOvyC"
      },
      "id": "eYbv8BoIOvyC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Embedding Layer מאומן מאפס**"
      ],
      "metadata": {
        "id": "iizuR_5POOjs"
      },
      "id": "iizuR_5POOjs"
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# הכנת המשפטים לאימון (רשימה של רשימות מילים)\n",
        "sentences_for_w2v = [text.split() for text in texts]\n",
        "\n",
        "# אימון Word2Vec - לומד את הקשרים בין המילים בדאטה שלך\n",
        "w2v_model = Word2Vec(sentences=sentences_for_w2v, vector_size=100, window=5, min_count=1, sg=1)\n",
        "\n",
        "# יצירת מטריצת המשקולות (הגשר בין Word2Vec ל-PyTorch)\n",
        "EMBED_DIM = 100\n",
        "# VOCAB_SIZE צריך להיות שווה ל- len(tokenizer.word_index) + 1\n",
        "embedding_matrix = torch.zeros((VOCAB_SIZE, EMBED_DIM))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < VOCAB_SIZE:\n",
        "        if word in w2v_model.wv:\n",
        "            embedding_matrix[i] = torch.tensor(w2v_model.wv[word].copy())\n",
        "        else:\n",
        "            # מילים שלא קיימות ב-W2V מקבלות ערך אקראי קטן\n",
        "            embedding_matrix[i] = torch.randn(EMBED_DIM) * 0.1\n",
        "\n",
        "EMBED_MAT = embedding_matrix # נשמור את זה בשם ברור לניסויים"
      ],
      "metadata": {
        "id": "2ezrP0ZO7lm8"
      },
      "id": "2ezrP0ZO7lm8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DifficultyModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, weights=None, is_frozen=False):\n",
        "        super().__init__()\n",
        "        if weights is not None:\n",
        "            self.embedding = torch.nn.Embedding.from_pretrained(weights, freeze=is_frozen)\n",
        "        else:\n",
        "            self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        self.classifier = torch.nn.Sequential(\n",
        "            torch.nn.Linear(embed_dim, 64),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(0.2),\n",
        "            torch.nn.Linear(64, 3) # 3 קטגוריות\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).mean(dim=1) # Pooling\n",
        "        return self.classifier(x)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, epochs=5):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for texts_batch, labels_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(texts_batch), labels_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # חישוב דיוק סופי\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for texts_batch, labels_batch in val_loader:\n",
        "            _, predicted = torch.max(model(texts_batch), 1)\n",
        "            total += labels_batch.size(0)\n",
        "            correct += (predicted == labels_batch).sum().item()\n",
        "    return 100 * correct / total"
      ],
      "metadata": {
        "id": "39EwRmQ77ohI"
      },
      "id": "39EwRmQ77ohI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_scratch = DifficultyModel(VOCAB_SIZE, EMBED_DIM)\n",
        "acc_scratch = train_model(model_scratch, train_loader, val_loader)\n",
        "print(f\"Accuracy (Scratch): {acc_scratch:.2f}%\")"
      ],
      "metadata": {
        "id": "xhDIVjO2-yoN"
      },
      "id": "xhDIVjO2-yoN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Frozen model**"
      ],
      "metadata": {
        "id": "sCEORmGUCOY0"
      },
      "id": "sCEORmGUCOY0"
    },
    {
      "cell_type": "code",
      "source": [
        "# ניסוי 2א: Word2Vec קפוא (Frozen)\n",
        "model_frozen = DifficultyModel(VOCAB_SIZE, EMBED_DIM, weights=embedding_matrix, is_frozen=True)\n",
        "acc_frozen = train_model(model_frozen, train_loader, val_loader)\n",
        "print(f\"Accuracy (W2V Frozen): {acc_frozen:.2f}%\")"
      ],
      "metadata": {
        "id": "xr5SxYcAXUdC"
      },
      "id": "xr5SxYcAXUdC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fine-tuned**"
      ],
      "metadata": {
        "id": "50sI0JDkCXO4"
      },
      "id": "50sI0JDkCXO4"
    },
    {
      "cell_type": "code",
      "source": [
        "# ניסוי 2ב: Word2Vec מכוונן (Fine-tuned)\n",
        "model_tuned = DifficultyModel(VOCAB_SIZE, EMBED_DIM, weights=embedding_matrix, is_frozen=False)\n",
        "acc_tuned = train_model(model_tuned, train_loader, val_loader)\n",
        "print(f\"Accuracy (W2V Fine-tuned): {acc_tuned:.2f}%\")"
      ],
      "metadata": {
        "id": "wY0MXBiBtTR0"
      },
      "id": "wY0MXBiBtTR0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **שלב 2**"
      ],
      "metadata": {
        "id": "-RW5xevrZphS"
      },
      "id": "-RW5xevrZphS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basic Settings**"
      ],
      "metadata": {
        "id": "3XHTdnNCEyvQ"
      },
      "id": "3XHTdnNCEyvQ"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# =========================\n",
        "# סעיף א: ארכיטקטורת המודל\n",
        "# =========================\n",
        "class FlexibleModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, n_layers,\n",
        "                 model_type='LSTM', bidirectional=False, dropout_p=0.0,\n",
        "                 embedding_matrix=None, is_frozen=False):\n",
        "        super(FlexibleModel, self).__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        # 1. יצירת שכבת ה-Embedding הבסיסית\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # 2. טיפול ב-Word2Vec (אם נשלחה מטריצה)\n",
        "        if embedding_matrix is not None:\n",
        "            # טעינת המטריצה המוכנה\n",
        "            self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
        "\n",
        "            # קביעה האם להקפיא (Frozen) או לאפשר שינוי (Fine-tuned)\n",
        "            self.embedding.weight.requires_grad = not is_frozen\n",
        "        else:\n",
        "            # מצב \"מאומן מאפס\": אין מטריצה, לכן המשקולות אקראיות וחייבות להיות פתוחות לאימון\n",
        "            self.embedding.weight.requires_grad = True\n",
        "\n",
        "        # --- שאר חלקי המודל כפי שהגדרת ---\n",
        "        rnn_dropout = dropout_p if n_layers > 1 else 0.0\n",
        "        if model_type == 'RNN':\n",
        "            self.rnn = nn.RNN(embed_dim, hidden_dim, n_layers, batch_first=True,\n",
        "                              bidirectional=bidirectional, dropout=rnn_dropout)\n",
        "        else:\n",
        "            self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first=True,\n",
        "                               bidirectional=bidirectional, dropout=rnn_dropout)\n",
        "\n",
        "        num_directions = 2 if bidirectional else 1\n",
        "        self.fc = nn.Linear(hidden_dim * num_directions, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        output, hidden = self.rnn(embedded)\n",
        "        if isinstance(hidden, tuple): hidden = hidden[0]\n",
        "        if self.rnn.bidirectional:\n",
        "            hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
        "        else:\n",
        "            hidden = hidden[-1, :, :]\n",
        "        return self.fc(hidden)\n",
        "\n",
        "# ==========================================\n",
        "# סעיף ב: תהליך האימון - שמירת היסטוריה מלאה\n",
        "# ==========================================\n",
        "def run_experiment(model, train_loader, val_loader, epochs=5, lr=0.001):\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # אימון\n",
        "        model.train()\n",
        "        total_train_loss = 0.0\n",
        "\n",
        "        for texts, labels in train_loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(texts)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        # ולידציה\n",
        "        model.eval()\n",
        "        total_val_loss = 0.0\n",
        "        correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for texts, labels in val_loader:\n",
        "                texts, labels = texts.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(texts)\n",
        "                loss = criterion(outputs, labels)\n",
        "                total_val_loss += loss.item()\n",
        "                correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        acc = correct / len(val_loader.dataset)\n",
        "\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['val_acc'].append(acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | \"\n",
        "              f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {acc:.4f}\")\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "# ======================\n",
        "# סעיף ג: הערכת ביצועים\n",
        "# ======================\n",
        "def print_evaluation_section_c(model, loader):\n",
        "    print(\"\\n\" + \"=\" * 30)\n",
        "    print(\"סעיף ג: הערכת ביצועים\")\n",
        "    print(\"=\" * 30)\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in loader:\n",
        "            texts = texts.to(device)\n",
        "            outputs = model(texts)\n",
        "\n",
        "            preds = outputs.argmax(1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    print(classification_report(all_labels, all_preds, target_names=['Easy', 'Medium', 'Hard']))\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(4, 3))\n",
        "    sns.heatmap(\n",
        "        cm, annot=True, fmt='d', cmap='Blues',\n",
        "        xticklabels=['Easy', 'Medium', 'Hard'],\n",
        "        yticklabels=['Easy', 'Medium', 'Hard']\n",
        "    )\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ======================\n",
        "# גרפים: Loss ו-Accuracy\n",
        "# ======================\n",
        "def plot_training_history(history, model_name=\"Model\"):\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, history['train_loss'], label='Training Loss')\n",
        "    plt.plot(epochs, history['val_loss'], label='Validation Loss')\n",
        "    plt.title(f'{model_name} - Loss (Error) Curve')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, history['val_acc'], label='Validation Accuracy')\n",
        "    plt.title(f'{model_name} - Accuracy Curve')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "8c33Gk_3ZofD"
      },
      "id": "8c33Gk_3ZofD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_text(text_tensor, tokenizer):\n",
        "    # שימוש במיפוי של Keras (index_word)\n",
        "    # i.item() > 0 כי 0 הוא בדרך כלל ה-Padding\n",
        "    index_to_word = tokenizer.index_word\n",
        "    words = [index_to_word.get(i.item(), \"\") for i in text_tensor if i.item() > 0]\n",
        "    return \" \".join(words).strip()\n",
        "\n",
        "def print_misclassifications(model, dataloader, device, tokenizer, label_map, target_true='Medium', target_pred='Hard', num_examples=5):\n",
        "    model.eval()\n",
        "    results = []\n",
        "\n",
        "    # מיפוי שמות לקטגוריות (למשל 'Medium' -> 1)\n",
        "    inv_label_map = {v: k for k, v in label_map.items()}\n",
        "    true_idx = inv_label_map[target_true]\n",
        "    pred_idx = inv_label_map[target_pred]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in dataloader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            outputs = model(texts)\n",
        "            preds = outputs.argmax(1)\n",
        "\n",
        "            for i in range(len(labels)):\n",
        "                if labels[i].item() == true_idx and preds[i].item() == pred_idx:\n",
        "                    # שימוש ב-decode_text החדש שמתאים לטוקנייזר שלך\n",
        "                    original_text = decode_text(texts[i], tokenizer)\n",
        "                    results.append(original_text)\n",
        "\n",
        "                if len(results) >= num_examples:\n",
        "                    break\n",
        "            if len(results) >= num_examples:\n",
        "                break\n",
        "\n",
        "    print(f\"\\n--- דוגמאות של {target_true} שסווגו בטעות כ-{target_pred} ---\")\n",
        "    if not results:\n",
        "        print(\"לא נמצאו דוגמאות כאלו בניסוי הנוכחי.\")\n",
        "    else:\n",
        "        for j, text in enumerate(results):\n",
        "            print(f\"{j+1}) {text}\\n\")"
      ],
      "metadata": {
        "id": "gSu-n61CE07C"
      },
      "id": "gSu-n61CE07C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RNN - Regular embedding**"
      ],
      "metadata": {
        "id": "prOOOvsCFExc"
      },
      "id": "prOOOvsCFExc"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# ניסוי מעודכן: שליטה מלאה בפרמטרים\n",
        "# ==========================================\n",
        "MODEL_TYPE = 'RNN'          # 'RNN' או 'LSTM'\n",
        "IS_BIDIRECTIONAL = False\n",
        "DROPOUT_P = 0.0\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 15                  # מספר איטרציות מקסימלי (העצירה המוקדמת תעצור לפני)\n",
        "\n",
        "# --- שליטה על גודל המודל (Model Size) ---\n",
        "HIDDEN_DIM = 128              # אופציה להקטין את המודל ל64\n",
        "N_LAYERS = 1                 # שכבה אחת בלבד (פחות פרמטרים)\n",
        "\n",
        "# --- שליטה על Batch Size ---\n",
        "BATCH_SIZE = 32              # גודל קבוצה קטן יותר (עוזר לעיבוד מדויק יותר ולפעמים למניעת Overfitting)\n",
        "\n",
        "# --- פרמטרים לעצירה מוקדמת (Early Stopping) ---\n",
        "PATIENCE = 3                 # כמה איטרציות לחכות בלי שיפור ב-Val Loss לפני שעוצרים\n",
        "\n",
        "# הגדרות לניסוי מכוונן\n",
        "IS_FROZEN = False\n",
        "\n",
        "model_tuned = FlexibleModel(\n",
        "    vocab_size=36543,\n",
        "    embed_dim=100,\n",
        "    hidden_dim=128,\n",
        "    output_dim=3,\n",
        "    n_layers=1,\n",
        "    model_type='RNN',\n",
        "    bidirectional=False,\n",
        "    #embedding_matrix=EMBED_MAT, # אותה מטריצה בדיוק\n",
        "    #is_frozen=IS_FROZEN,        # פתיחה לעדכונים\n",
        "    dropout_p=0.0\n",
        ")\n",
        "\n",
        "print(f\"--- מריץ ניסוי: Word2Vec Fine-tuned (מכוונן) ---\")\n",
        "history_tuned = run_experiment(model_tuned, train_loader, val_loader, epochs=15)\n",
        "print_evaluation_section_c(model_tuned, test_loader)\n",
        "\n",
        "# ב. הרצת האימון\n",
        "print(f\"--- מריץ ניסוי: {MODEL_TYPE} | Bi={IS_BIDIRECTIONAL} | Dropout={DROPOUT_P} | LR={LEARNING_RATE} ---\")\n",
        "history = run_experiment(\n",
        "    model_experiment,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    epochs=EPOCHS,\n",
        "    lr=LEARNING_RATE\n",
        ")\n",
        "\n",
        "# ג. הצגת תוצאות\n",
        "print_evaluation_section_c(model_experiment, test_loader)\n",
        "plot_training_history(history, model_name=f\"{MODEL_TYPE} (Dropout={DROPOUT_P})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad3dd649-c020-4257-ac3c-633a4b8576ce",
        "id": "GN9481rrgaEA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- מריץ ניסוי: Word2Vec Fine-tuned (מכוונן) ---\n",
            "Epoch 1/15 | Train Loss: 1.0482 | Val Loss: 1.0363 | Val Acc: 0.3953\n",
            "Epoch 2/15 | Train Loss: 1.0404 | Val Loss: 0.9959 | Val Acc: 0.4322\n",
            "Epoch 3/15 | Train Loss: 1.0680 | Val Loss: 1.0419 | Val Acc: 0.3332\n",
            "Epoch 4/15 | Train Loss: 1.0373 | Val Loss: 1.0337 | Val Acc: 0.3959\n",
            "Epoch 5/15 | Train Loss: 1.0439 | Val Loss: 1.0622 | Val Acc: 0.3921\n",
            "Epoch 6/15 | Train Loss: 1.0421 | Val Loss: 1.0386 | Val Acc: 0.3924\n",
            "Epoch 7/15 | Train Loss: 1.0401 | Val Loss: 1.0562 | Val Acc: 0.3934\n",
            "Epoch 8/15 | Train Loss: 1.0400 | Val Loss: 1.0398 | Val Acc: 0.3931\n",
            "Epoch 9/15 | Train Loss: 1.0388 | Val Loss: 1.0346 | Val Acc: 0.3950\n"
          ]
        }
      ],
      "id": "GN9481rrgaEA"
    },
    {
      "cell_type": "code",
      "source": [
        "# הגדרת המפה לפי הסדר של המחלקות אצלך\n",
        "label_map = {0: 'Easy', 1: 'Medium', 2: 'Hard'}\n",
        "\n",
        "# קריאה לפונקציה\n",
        "print_misclassifications(\n",
        "    model_experiment,\n",
        "    test_loader,\n",
        "    device,\n",
        "    tokenizer,   # הטוקנייזר שהגדרת קודם\n",
        "    label_map,\n",
        "    target_true='Hard',\n",
        "    target_pred='Medium',\n",
        "    num_examples=5\n",
        ")"
      ],
      "metadata": {
        "id": "cK0sEcSgIIBR"
      },
      "id": "cK0sEcSgIIBR",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}