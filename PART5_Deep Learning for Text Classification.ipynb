{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***תרגיל 5 של הפרוייקט***"
      ],
      "metadata": {
        "id": "BWDmLmxFJX1B"
      },
      "id": "BWDmLmxFJX1B"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Data and Basic Setup**"
      ],
      "metadata": {
        "id": "3AhMc7jzxsIT"
      },
      "id": "3AhMc7jzxsIT"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/train-filtered_question_level.csv\")\n",
        "\n",
        "# Remove duplicate questions\n",
        "df = df.drop_duplicates(subset=[\"question\"], keep=\"first\")\n",
        "\n",
        "# Extract text and difficulty levels\n",
        "texts = df[\"question\"].astype(str).tolist()\n",
        "levels = df[\"level\"].tolist()\n"
      ],
      "metadata": {
        "id": "1a_xSfMMx0fI"
      },
      "id": "1a_xSfMMx0fI",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Balancing Dataset (Undersampling to Minority Class)**"
      ],
      "metadata": {
        "id": "-u5QtAfCyDdF"
      },
      "id": "-u5QtAfCyDdF"
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# # 1. נגדיר את גודל היעד לפי המחלקה הקטנה ביותר (Hard)\n",
        "# target_size = 15657\n",
        "\n",
        "# # 2. נבצע דגימה מכל מחלקה בנפרד\n",
        "# df_hard = df[df['level'] == 'hard']\n",
        "# # כאן אנחנו לא עושים sample כי זה כבר הגודל שאנחנו רוצים\n",
        "\n",
        "# df_medium_downsampled = df[df['level'] == 'medium'].sample(n=target_size, random_state=42)\n",
        "# df_easy_downsampled = df[df['level'] == 'easy'].sample(n=target_size, random_state=42)\n",
        "\n",
        "# # 3. נחבר את שלושתן יחד\n",
        "# df_balanced = pd.concat([df_hard, df_medium_downsampled, df_easy_downsampled])\n",
        "\n",
        "# # 4. נערבב את הדאטה (חשוב מאוד!)\n",
        "# df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# # בדיקת תוצאה\n",
        "# print(\"התפלגות חדשה:\")\n",
        "# print(df_balanced['level'].value_counts())"
      ],
      "metadata": {
        "id": "eVzpuQa2yELM"
      },
      "id": "eVzpuQa2yELM",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. הגדרת גודל המדגם לכל קטגוריה (היפר-פרמטר של שלב הניסויים)\n",
        "# התחלה עם 1000 מכל אחת עוזרת לבדיקה מהירה (\"מתחילים בקטן\" לפי ההוראות)\n",
        "target_size = 7000\n",
        "\n",
        "# 2. דגימה מאוזנת מכל הקטגוריות בבת אחת\n",
        "# groupby מבטיח שנתייחס לכל רמת קושי בנפרד\n",
        "df_balanced = df.groupby('level').apply(lambda x: x.sample(n=target_size, random_state=42)).reset_index(drop=True)\n",
        "\n",
        "# 3. ערבוב הדאטה (Shuffle) - קריטי בלמידה עמוקה!\n",
        "# כדי שה-Batch לא יכיל רק מחלקה אחת בזמן האימון, מה שיהרוס את הלמידה\n",
        "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# בדיקת התפלגות כפי שנדרש בשלב ה-EDA ובניתוח הדאטה\n",
        "print(\"התפלגות חדשה ומאוזנת:\")\n",
        "print(df_balanced['level'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NWLJ-kVHMFU",
        "outputId": "5095752c-64e1-418e-c06d-707d691a041e"
      },
      "id": "7NWLJ-kVHMFU",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "התפלגות חדשה ומאוזנת:\n",
            "level\n",
            "easy      7000\n",
            "medium    7000\n",
            "hard      7000\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4281571597.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df_balanced = df.groupby('level').apply(lambda x: x.sample(n=target_size, random_state=42)).reset_index(drop=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **שלב 1**"
      ],
      "metadata": {
        "id": "bLuTpTziNjn-"
      },
      "id": "bLuTpTziNjn-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **א**"
      ],
      "metadata": {
        "id": "yfcY2plyNvTU"
      },
      "id": "yfcY2plyNvTU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Choosing Maximum Sequence Length (Documentation)**"
      ],
      "metadata": {
        "id": "0N8xodXyyq2J"
      },
      "id": "0N8xodXyyq2J"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbtPlPa_H-ht",
        "outputId": "77a79841-f0fe-48ba-cf6d-e759d32a40db"
      },
      "id": "zbtPlPa_H-ht",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.4)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 1. חילוץ הטקסטים\n",
        "texts = df_balanced['question'].astype(str).tolist()\n",
        "\n",
        "# 2. אתחול והתאמת הטוקנייזר על כל המילים (בלי הגבלה שרירותית של 20,000)\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# 3. הגדרת VOCAB_SIZE האמיתי (קריטי למטריצת ה-Embedding)\n",
        "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "print(f\"Actual Vocabulary size: {VOCAB_SIZE}\")\n",
        "\n",
        "# 4. המרה לרצפים וביצוע Padding\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "MAX_LEN = int(np.percentile([len(seq) for seq in sequences], 95)) # אורך שחוסם 95% מהמשפטים\n",
        "X = pad_sequences(sequences, maxlen=MAX_LEN, padding='post')\n"
      ],
      "metadata": {
        "id": "zpEiqRM9yruA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1164b723-4a63-4ea1-adcc-d930dfa88b99"
      },
      "id": "zpEiqRM9yruA",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual Vocabulary size: 36543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Padding and Truncation**"
      ],
      "metadata": {
        "id": "vp92Fj-UzFxT"
      },
      "id": "vp92Fj-UzFxT"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. הגדרת המילון (כפי שעשית)\n",
        "label_dict = {'easy': 0, 'medium': 1, 'hard': 2}\n",
        "y_integers = df_balanced['level'].map(label_dict).values\n",
        "\n",
        "# 2. חלוקה מרובדת (Stratify) כדי לשמור על איזון באחוזים\n",
        "# חלוקה ראשונה: מוציאים 15% לטסט סופי\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y_integers,\n",
        "    test_size=0.15,\n",
        "    random_state=42,\n",
        "    stratify=y_integers # מבטיח איזון\n",
        ")\n",
        "\n",
        "# חלוקה שנייה: פיצול היתרה לאימון וולידציה (15% מהסך הכל)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val,\n",
        "    test_size=0.176, # 0.15 / 0.85\n",
        "    random_state=42,\n",
        "    stratify=y_train_val # מבטיח איזון\n",
        ")\n",
        "\n",
        "print(f\"Train size: {len(X_train)} | Val size: {len(X_val)} | Test size: {len(X_test)}\")"
      ],
      "metadata": {
        "id": "eS7v5hBMzGln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78e9efbc-7141-42a7-9cc5-df1c3a7a611d"
      },
      "id": "eS7v5hBMzGln",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 14708 | Val size: 3142 | Test size: 3150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# המרה ל-Tensors\n",
        "# X הוא LongTensor כי הוא מכיל אינדקסים של מילים\n",
        "# y הוא LongTensor כי CrossEntropyLoss מצפה לאינדקסים של מחלקות\n",
        "train_ds = TensorDataset(torch.LongTensor(X_train), torch.LongTensor(y_train))\n",
        "val_ds   = TensorDataset(torch.LongTensor(X_val), torch.LongTensor(y_val))\n",
        "test_ds  = TensorDataset(torch.LongTensor(X_test), torch.LongTensor(y_test))\n",
        "\n",
        "# יצירת Loaders\n",
        "# shuffle=True רק באימון כדי שהמודל לא ילמד את סדר השאלות\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds, batch_size=32, shuffle=False)\n",
        "\n",
        "print(\"PyTorch DataLoaders are ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57jJSQgaANqX",
        "outputId": "88536385-7143-4837-95ba-71ad452911b5"
      },
      "id": "57jJSQgaANqX",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch DataLoaders are ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ב**"
      ],
      "metadata": {
        "id": "trNMJWkXzRMm"
      },
      "id": "trNMJWkXzRMm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ניסוי 1**"
      ],
      "metadata": {
        "id": "eYbv8BoIOvyC"
      },
      "id": "eYbv8BoIOvyC"
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# הכנת המשפטים לאימון (רשימה של רשימות מילים)\n",
        "sentences_for_w2v = [text.split() for text in texts]\n",
        "\n",
        "# אימון Word2Vec - לומד את הקשרים בין המילים בדאטה שלך\n",
        "w2v_model = Word2Vec(sentences=sentences_for_w2v, vector_size=100, window=5, min_count=1, sg=1)\n",
        "\n",
        "# יצירת מטריצת המשקולות (הגשר בין Word2Vec ל-PyTorch)\n",
        "EMBED_DIM = 100\n",
        "# VOCAB_SIZE צריך להיות שווה ל- len(tokenizer.word_index) + 1\n",
        "embedding_matrix = torch.zeros((VOCAB_SIZE, EMBED_DIM))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < VOCAB_SIZE:\n",
        "        if word in w2v_model.wv:\n",
        "            embedding_matrix[i] = torch.tensor(w2v_model.wv[word].copy())\n",
        "        else:\n",
        "            # מילים שלא קיימות ב-W2V מקבלות ערך אקראי קטן\n",
        "            embedding_matrix[i] = torch.randn(EMBED_DIM) * 0.1\n",
        "\n",
        "EMBED_MAT = embedding_matrix # נשמור את זה בשם ברור לניסויים"
      ],
      "metadata": {
        "id": "2ezrP0ZO7lm8"
      },
      "id": "2ezrP0ZO7lm8",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DifficultyModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, weights=None, is_frozen=False):\n",
        "        super().__init__()\n",
        "        if weights is not None:\n",
        "            self.embedding = torch.nn.Embedding.from_pretrained(weights, freeze=is_frozen)\n",
        "        else:\n",
        "            self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        self.classifier = torch.nn.Sequential(\n",
        "            torch.nn.Linear(embed_dim, 64),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(0.2),\n",
        "            torch.nn.Linear(64, 3) # 3 קטגוריות\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).mean(dim=1) # Pooling\n",
        "        return self.classifier(x)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, epochs=5):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for texts_batch, labels_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(texts_batch), labels_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # חישוב דיוק סופי\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for texts_batch, labels_batch in val_loader:\n",
        "            _, predicted = torch.max(model(texts_batch), 1)\n",
        "            total += labels_batch.size(0)\n",
        "            correct += (predicted == labels_batch).sum().item()\n",
        "    return 100 * correct / total"
      ],
      "metadata": {
        "id": "39EwRmQ77ohI"
      },
      "id": "39EwRmQ77ohI",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embedding Layer מאומן מאפס**"
      ],
      "metadata": {
        "id": "VoRPmoOLLDp4"
      },
      "id": "VoRPmoOLLDp4"
    },
    {
      "cell_type": "code",
      "source": [
        "model_scratch = DifficultyModel(VOCAB_SIZE, EMBED_DIM)\n",
        "acc_scratch = train_model(model_scratch, train_loader, val_loader)\n",
        "print(f\"Accuracy (Scratch): {acc_scratch:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhDIVjO2-yoN",
        "outputId": "d575d04e-8e96-40e4-90af-6410bac4afad"
      },
      "id": "xhDIVjO2-yoN",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Scratch): 47.55%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Frozen model**"
      ],
      "metadata": {
        "id": "sCEORmGUCOY0"
      },
      "id": "sCEORmGUCOY0"
    },
    {
      "cell_type": "code",
      "source": [
        "# ניסוי 2א: Word2Vec קפוא (Frozen)\n",
        "model_frozen = DifficultyModel(VOCAB_SIZE, EMBED_DIM, weights=embedding_matrix, is_frozen=True)\n",
        "acc_frozen = train_model(model_frozen, train_loader, val_loader)\n",
        "print(f\"Accuracy (W2V Frozen): {acc_frozen:.2f}%\")"
      ],
      "metadata": {
        "id": "xr5SxYcAXUdC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbb06f01-99b6-4122-b696-1bbf8cbdb1d8"
      },
      "id": "xr5SxYcAXUdC",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (W2V Frozen): 50.73%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fine-tuned**"
      ],
      "metadata": {
        "id": "50sI0JDkCXO4"
      },
      "id": "50sI0JDkCXO4"
    },
    {
      "cell_type": "code",
      "source": [
        "# ניסוי 2ב: Word2Vec מכוונן (Fine-tuned)\n",
        "model_tuned = DifficultyModel(VOCAB_SIZE, EMBED_DIM, weights=embedding_matrix, is_frozen=False)\n",
        "acc_tuned = train_model(model_tuned, train_loader, val_loader)\n",
        "print(f\"Accuracy (W2V Fine-tuned): {acc_tuned:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wY0MXBiBtTR0",
        "outputId": "bb43ec4b-8e8e-401e-8377-66a6dea07553"
      },
      "id": "wY0MXBiBtTR0",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (W2V Fine-tuned): 49.08%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **שלב 2**"
      ],
      "metadata": {
        "id": "-RW5xevrZphS"
      },
      "id": "-RW5xevrZphS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basic Settings**"
      ],
      "metadata": {
        "id": "3XHTdnNCEyvQ"
      },
      "id": "3XHTdnNCEyvQ"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# =========================\n",
        "# סעיף א: ארכיטקטורת המודל\n",
        "# =========================\n",
        "class FlexibleModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, n_layers,\n",
        "                 model_type='LSTM', bidirectional=False, dropout_p=0.0,\n",
        "                 embedding_matrix=None, is_frozen=False):\n",
        "        super(FlexibleModel, self).__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        # 1. יצירת שכבת ה-Embedding הבסיסית\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # 2. טיפול ב-Word2Vec (אם נשלחה מטריצה)\n",
        "        if embedding_matrix is not None:\n",
        "            # טעינת המטריצה המוכנה\n",
        "            self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
        "\n",
        "            # קביעה האם להקפיא (Frozen) או לאפשר שינוי (Fine-tuned)\n",
        "            self.embedding.weight.requires_grad = not is_frozen\n",
        "        else:\n",
        "            # מצב \"מאומן מאפס\": אין מטריצה, לכן המשקולות אקראיות וחייבות להיות פתוחות לאימון\n",
        "            self.embedding.weight.requires_grad = True\n",
        "\n",
        "        # --- שאר חלקי המודל כפי שהגדרת ---\n",
        "        rnn_dropout = dropout_p if n_layers > 1 else 0.0\n",
        "        if model_type == 'RNN':\n",
        "            self.rnn = nn.RNN(embed_dim, hidden_dim, n_layers, batch_first=True,\n",
        "                              bidirectional=bidirectional, dropout=rnn_dropout)\n",
        "        else:\n",
        "            self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first=True,\n",
        "                               bidirectional=bidirectional, dropout=rnn_dropout)\n",
        "\n",
        "        num_directions = 2 if bidirectional else 1\n",
        "        self.fc = nn.Linear(hidden_dim * num_directions, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        output, hidden = self.rnn(embedded)\n",
        "        if isinstance(hidden, tuple): hidden = hidden[0]\n",
        "        if self.rnn.bidirectional:\n",
        "            hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
        "        else:\n",
        "            hidden = hidden[-1, :, :]\n",
        "        return self.fc(hidden)\n",
        "\n",
        "# ==========================================\n",
        "# סעיף ב: תהליך האימון - שמירת היסטוריה מלאה\n",
        "# ==========================================\n",
        "def run_experiment(model, train_loader, val_loader, epochs=5, lr=0.001):\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # אימון\n",
        "        model.train()\n",
        "        total_train_loss = 0.0\n",
        "\n",
        "        for texts, labels in train_loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(texts)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        # ולידציה\n",
        "        model.eval()\n",
        "        total_val_loss = 0.0\n",
        "        correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for texts, labels in val_loader:\n",
        "                texts, labels = texts.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(texts)\n",
        "                loss = criterion(outputs, labels)\n",
        "                total_val_loss += loss.item()\n",
        "                correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        acc = correct / len(val_loader.dataset)\n",
        "\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['val_acc'].append(acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | \"\n",
        "              f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {acc:.4f}\")\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "# ======================\n",
        "# סעיף ג: הערכת ביצועים\n",
        "# ======================\n",
        "def print_evaluation_section_c(model, loader):\n",
        "    print(\"\\n\" + \"=\" * 30)\n",
        "    print(\"סעיף ג: הערכת ביצועים\")\n",
        "    print(\"=\" * 30)\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in loader:\n",
        "            texts = texts.to(device)\n",
        "            outputs = model(texts)\n",
        "\n",
        "            preds = outputs.argmax(1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    print(classification_report(all_labels, all_preds, target_names=['Easy', 'Medium', 'Hard']))\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(4, 3))\n",
        "    sns.heatmap(\n",
        "        cm, annot=True, fmt='d', cmap='Blues',\n",
        "        xticklabels=['Easy', 'Medium', 'Hard'],\n",
        "        yticklabels=['Easy', 'Medium', 'Hard']\n",
        "    )\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ======================\n",
        "# גרפים: Loss ו-Accuracy\n",
        "# ======================\n",
        "def plot_training_history(history, model_name=\"Model\"):\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, history['train_loss'], label='Training Loss')\n",
        "    plt.plot(epochs, history['val_loss'], label='Validation Loss')\n",
        "    plt.title(f'{model_name} - Loss (Error) Curve')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, history['val_acc'], label='Validation Accuracy')\n",
        "    plt.title(f'{model_name} - Accuracy Curve')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "8c33Gk_3ZofD"
      },
      "id": "8c33Gk_3ZofD",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_text(text_tensor, tokenizer):\n",
        "    # שימוש במיפוי של Keras (index_word)\n",
        "    # i.item() > 0 כי 0 הוא בדרך כלל ה-Padding\n",
        "    index_to_word = tokenizer.index_word\n",
        "    words = [index_to_word.get(i.item(), \"\") for i in text_tensor if i.item() > 0]\n",
        "    return \" \".join(words).strip()\n",
        "\n",
        "def print_misclassifications(model, dataloader, device, tokenizer, label_map, target_true='Medium', target_pred='Hard', num_examples=5):\n",
        "    model.eval()\n",
        "    results = []\n",
        "\n",
        "    # מיפוי שמות לקטגוריות (למשל 'Medium' -> 1)\n",
        "    inv_label_map = {v: k for k, v in label_map.items()}\n",
        "    true_idx = inv_label_map[target_true]\n",
        "    pred_idx = inv_label_map[target_pred]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in dataloader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            outputs = model(texts)\n",
        "            preds = outputs.argmax(1)\n",
        "\n",
        "            for i in range(len(labels)):\n",
        "                if labels[i].item() == true_idx and preds[i].item() == pred_idx:\n",
        "                    # שימוש ב-decode_text החדש שמתאים לטוקנייזר שלך\n",
        "                    original_text = decode_text(texts[i], tokenizer)\n",
        "                    results.append(original_text)\n",
        "\n",
        "                if len(results) >= num_examples:\n",
        "                    break\n",
        "            if len(results) >= num_examples:\n",
        "                break\n",
        "\n",
        "    print(f\"\\n--- דוגמאות של {target_true} שסווגו בטעות כ-{target_pred} ---\")\n",
        "    if not results:\n",
        "        print(\"לא נמצאו דוגמאות כאלו בניסוי הנוכחי.\")\n",
        "    else:\n",
        "        for j, text in enumerate(results):\n",
        "            print(f\"{j+1}) {text}\\n\")"
      ],
      "metadata": {
        "id": "gSu-n61CE07C"
      },
      "id": "gSu-n61CE07C",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RNN - Regular embedding**"
      ],
      "metadata": {
        "id": "prOOOvsCFExc"
      },
      "id": "prOOOvsCFExc"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# ניסוי מעודכן: שליטה מלאה בפרמטרים\n",
        "# ==========================================\n",
        "MODEL_TYPE = 'RNN'          # 'RNN' או 'LSTM'\n",
        "IS_BIDIRECTIONAL = False\n",
        "DROPOUT_P = 0.0\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 15                  # מספר איטרציות מקסימלי (העצירה המוקדמת תעצור לפני)\n",
        "\n",
        "# --- שליטה על גודל המודל (Model Size) ---\n",
        "HIDDEN_DIM = 128              # אופציה להקטין את המודל ל64\n",
        "N_LAYERS = 1                 # שכבה אחת בלבד (פחות פרמטרים)\n",
        "\n",
        "# --- שליטה על Batch Size ---\n",
        "BATCH_SIZE = 32              # גודל קבוצה קטן יותר (עוזר לעיבוד מדויק יותר ולפעמים למניעת Overfitting)\n",
        "\n",
        "# --- פרמטרים לעצירה מוקדמת (Early Stopping) ---\n",
        "PATIENCE = 3                 # כמה איטרציות לחכות בלי שיפור ב-Val Loss לפני שעוצרים\n",
        "\n",
        "# הגדרות לניסוי מכוונן\n",
        "IS_FROZEN = False\n",
        "\n",
        "model_experiment = FlexibleModel(\n",
        "    vocab_size=36543,\n",
        "    embed_dim=100,\n",
        "    hidden_dim=128,\n",
        "    output_dim=3,\n",
        "    n_layers=1,\n",
        "    model_type='RNN',\n",
        "    bidirectional=False,\n",
        "    #embedding_matrix=EMBED_MAT, # אותה מטריצה בדיוק\n",
        "    #is_frozen=IS_FROZEN,        # פתיחה לעדכונים\n",
        "    dropout_p=0.0\n",
        ")\n",
        "\n",
        "history_tuned = run_experiment(model_tuned, train_loader, val_loader, epochs=15)\n",
        "print_evaluation_section_c(model_tuned, test_loader)\n",
        "\n",
        "# ב. הרצת האימון\n",
        "print(f\"--- מריץ ניסוי: {MODEL_TYPE} | Bi={IS_BIDIRECTIONAL} | Dropout={DROPOUT_P} | LR={LEARNING_RATE} ---\")\n",
        "history = run_experiment(\n",
        "    model_experiment,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    epochs=EPOCHS,\n",
        "    lr=LEARNING_RATE\n",
        ")\n",
        "\n",
        "# ג. הצגת תוצאות\n",
        "print_evaluation_section_c(model_experiment, test_loader)\n",
        "plot_training_history(history, model_name=f\"{MODEL_TYPE} (Dropout={DROPOUT_P})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb764c21-af4e-4dca-b8a8-75a332d1169f",
        "id": "GN9481rrgaEA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15 | Train Loss: 1.0351 | Val Loss: 1.0096 | Val Acc: 0.4153\n",
            "Epoch 2/15 | Train Loss: 0.9970 | Val Loss: 0.9974 | Val Acc: 0.4427\n"
          ]
        }
      ],
      "id": "GN9481rrgaEA"
    },
    {
      "cell_type": "code",
      "source": [
        "# הגדרת המפה לפי הסדר של המחלקות אצלך\n",
        "label_map = {0: 'Easy', 1: 'Medium', 2: 'Hard'}\n",
        "\n",
        "# קריאה לפונקציה\n",
        "print_misclassifications(\n",
        "    model_experiment,\n",
        "    test_loader,\n",
        "    device,\n",
        "    tokenizer,   # הטוקנייזר שהגדרת קודם\n",
        "    label_map,\n",
        "    target_true='Hard',\n",
        "    target_pred='Medium',\n",
        "    num_examples=5\n",
        ")"
      ],
      "metadata": {
        "id": "cK0sEcSgIIBR"
      },
      "id": "cK0sEcSgIIBR",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}