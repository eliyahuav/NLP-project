{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8baca3dd-ec1a-4dee-bc5b-aa9e48680865",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8baca3dd-ec1a-4dee-bc5b-aa9e48680865",
        "outputId": "ff1f54a1-854c-41b3-b87b-9f52687fa954"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== NLP Text Processing Pipeline (20k subset) ===\n",
            "Python: 3.12.12 | Platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "Run started at: 2025-11-17T14:05:57\n"
          ]
        }
      ],
      "source": [
        "# ğŸ“˜ ×ª× â€œ×”×•×›×—×ª ×¨×™×¦×”â€ + ×§×•× ×¤×™×’×•×¨×¦×™×” ×‘×¡×™×¡×™×ª (×ª××™×“ ×™×“×¤×™×¡ ×—×•×ª××ª-×–××Ÿ)\n",
        "import sys, platform, datetime\n",
        "print(\"=== NLP Text Processing Pipeline (20k subset) ===\")\n",
        "print(\"Python:\", sys.version.split()[0], \"| Platform:\", platform.platform())\n",
        "print(\"Run started at:\", datetime.datetime.now().isoformat(timespec='seconds'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9ff5669d-2694-4447-ae63-2f03c12be22a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ff5669d-2694-4447-ae63-2f03c12be22a",
        "outputId": "8867f95b-91aa-4cf5-eea7-38bebd2a4c09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK resources are ready.\n"
          ]
        }
      ],
      "source": [
        "# ×ª× 1 â€” ×˜×¢×™× ×ª ×¡×¤×¨×™×•×ª ×•××©××‘×™ NLTK\n",
        "# ğŸ“˜ ×˜×¢×™× ×ª ×¡×¤×¨×™×•×ª ×•××©××‘×™ NLTK (×™×•×¨×™×“×• ××•×˜×•××˜×™×ª ×× ×—×¡×¨×™×)\n",
        "import pandas as pd\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "print(\"NLTK resources are ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b8b8f3a3-2f14-42ee-91e9-6aafd64e94ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "b8b8f3a3-2f14-42ee-91e9-6aafd64e94ab",
        "outputId": "58a9077e-0780-453c-c096-e33c6fdf6f10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset from: /content/train.xls\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "'utf-8' codec can't decode byte 0xe2 in position 0: unexpected end of data",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2515076883.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mDATA_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/train.xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading dataset from: {DATA_PATH}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded rows:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe2 in position 0: unexpected end of data"
          ]
        }
      ],
      "source": [
        "# ×ª× 2 â€” ×˜×¢×™× ×ª ×”Ö¾Dataset, ×–×™×”×•×™ ×¢××•×“×ª ID ×•×¢××•×“×•×ª ×˜×§×¡×˜\n",
        "# ğŸ“˜ ×˜×¢×™× ×ª ×”×“××˜×”, ×–×™×”×•×™ ×¢××•×“×ª ×”-ID (×× ×§×™×™××ª), ×•×–×™×”×•×™ ×¢××•×“×•×ª ×˜×§×¡×˜ (object) ×œ×¢×™×‘×•×“\n",
        "DATA_PATH = \"/content/train.xls\"\n",
        "print(f\"Loading dataset from: {DATA_PATH}\")\n",
        "df = pd.read_excel(DATA_PATH)\n",
        "print(\"Loaded rows:\", len(df))\n",
        "\n",
        "# ×–×™×”×•×™ ×¢××•×“×ª ID\n",
        "id_candidates = ['id', 'Id', 'ID']\n",
        "ID_COL = next((c for c in id_candidates if c in df.columns), None)\n",
        "print(\"ID column:\", ID_COL if ID_COL else \"None\")\n",
        "\n",
        "# ×¢××•×“×•×ª ×˜×§×¡×˜ (×›×œ ×¢××•×“×” ××¡×•×’ object ×—×•×¥ ××”-ID)\n",
        "text_cols = [c for c in df.columns if df[c].dtype == 'object' and c != ID_COL]\n",
        "print(\"Text columns to process:\", text_cols if text_cols else \"None\")\n",
        "if not text_cols:\n",
        "    raise ValueError(\"No text columns found to process.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87597e0d-4fea-4e30-b344-6084d73c5766",
      "metadata": {
        "id": "87597e0d-4fea-4e30-b344-6084d73c5766"
      },
      "outputs": [],
      "source": [
        "# ×ª× 3 â€” ×¡×™× ×•×Ÿ ×©×•×¨×•×ª ×œ× ×¨×™×§×•×ª ×•×‘×—×™×¨×” ×‘Ö¾20,000 ×©×•×¨×•×ª\n",
        "# ğŸ“˜ ×©××™×¨×” ×¢×œ ×©×•×¨×•×ª ×©×‘×”×Ÿ ×œ×¤×—×•×ª ××—×ª ××¢××•×“×•×ª ×”×˜×§×¡×˜ ××™× ×” ×¨×™×§×”/×¨×™×§×”-××—×¨×™-strip\n",
        "def non_empty_any(row):\n",
        "    for c in text_cols:\n",
        "        v = row[c]\n",
        "        if isinstance(v, str) and v.strip():\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "print(\"Filtering rows to keep only those with at least one non-empty text column...\")\n",
        "mask = df.apply(non_empty_any, axis=1)\n",
        "df = df[mask]\n",
        "\n",
        "print(\"Rows after non-empty filter:\", len(df))\n",
        "df = df.head(500)  # ×‘×—×™×¨×” ×‘Ö¾20,000 ×©×•×¨×•×ª ×”×¨××©×•× ×•×ª\n",
        "print(\"Rows capped at 500:\", len(df))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zyhBLrFSU4-l",
      "metadata": {
        "id": "zyhBLrFSU4-l"
      },
      "outputs": [],
      "source": [
        "dfUnprossed = df.copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f75c3ea9-2602-433b-9934-cccac5f026ff",
      "metadata": {
        "id": "f75c3ea9-2602-433b-9934-cccac5f026ff"
      },
      "outputs": [],
      "source": [
        "# ×ª× 4 â€” ×¤×•× ×§×¦×™×•×ª ×¢×™×‘×•×“: × ×™×§×•×™, Lemmatization ×¢× POS, ×•×”×•×¦××ª Stopwords\n",
        "# ğŸ“˜ ×¤×•× ×§×¦×™×•×ª ×¢×™×‘×•×“ ×˜×§×¡×˜ ×œ×›×œ ×¢××•×“×” ×‘× ×¤×¨×“:\n",
        "#    - ×”×¡×¨×ª URL/Email/Handles\n",
        "#    - ×˜×•×§× ×™×–×¦×™×”\n",
        "#    - POS tagging + Lemmatization (×¢× ××™×¤×•×™ ×œ-WordNet)\n",
        "#    - ×“×™×œ×•×’ ×¢×œ ×©××•×ª ×¤×¨×˜×™×™× (NNP/NNPS)\n",
        "#    - × ×¨××•×œ ×¦×•×¨×•×ª 'be' (am/is/are/was/were/been/being -> be)\n",
        "#    - ×”×—×œ×¤×ª ×¡×¤×¨×•×ª ×‘-_number\n",
        "#    - ×”×¡×¨×ª ×ª×•×•×™× ×©××™× × ××•×ª×™×•×ª ×œ×˜×™× ×™×•×ª/underscore/×¨×•×•×—\n",
        "#    - ×”×¡×¨×ª stopwords\n",
        "#    - ×”×•×¨×“×ª ×¨×™×©×™×•×ª\n",
        "#\n",
        "# ×”×¢×¨×”: ×œ× × ×•×¦×¨×™× ×¢××•×“×•×ª ×—×“×©×•×ª â€” ×”×¤×•× ×§×¦×™×” ×ª×—×–×™×¨ ××—×¨×•×–×ª ××¢×•×‘×“×ª ×©×ª×—×œ×™×£ ××ª ×ª×•×›×Ÿ ×”×¢××•×“×”.\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "eng_stops = set(stopwords.words('english'))\n",
        "BE_FORMS = {\"am\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\"}\n",
        "\n",
        "def get_wordnet_pos(tag: str):\n",
        "    if tag.startswith('J'): return wordnet.ADJ\n",
        "    if tag.startswith('V'): return wordnet.VERB\n",
        "    if tag.startswith('N'): return wordnet.NOUN\n",
        "    if tag.startswith('R'): return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "url_email_handle_re = re.compile(r'(https?://\\S+|www\\.\\S+|\\S+@\\S+|[@#]\\w+)', re.IGNORECASE)\n",
        "digits_re = re.compile(r'\\d+')           # ×¡×¤×¨×•×ª -> _number\n",
        "non_letter_re = re.compile(r'[^a-z_ ]+') # ××—×¨×™ lowercase, × ×©××™×¨ a-z, ×¨×•×•×—, underscore\n",
        "\n",
        "def process_text_value(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    t = text\n",
        "\n",
        "    # ×”×¡×¨×” ×¨××©×•× ×™×ª ×©×œ URL/Emails/Handles/Hashtags ×›×“×™ ×œ× ×œ×”×¨×•×¡ POS\n",
        "    t = url_email_handle_re.sub(' ', t)\n",
        "\n",
        "    # ×˜×•×§× ×™×–×¦×™×” + POS ×¢×œ ×”×˜×§×¡×˜ ×”××§×•×¨×™ (×œ×¤× ×™ lowercase) ×œ×˜×•×‘×ª Proper Nouns ×˜×•×‘ ×™×•×ª×¨\n",
        "    tokens = word_tokenize(t)\n",
        "    tagged = pos_tag(tokens)\n",
        "\n",
        "    # Lemmatization ×¢× POS + ×“×™×œ×•×’ ×¢×œ Proper Nouns + × ×¨××•×œ 'be'\n",
        "    lemmas = []\n",
        "    for tok, pos in tagged:\n",
        "        # × ×¨××•×œ ××•×§×“× ×œ-be\n",
        "        if tok.lower() in BE_FORMS:\n",
        "            lemmas.append(\"be\")\n",
        "            continue\n",
        "        if pos in (\"NNP\", \"NNPS\"):   # ×”×©××¨×ª ×©××•×ª ×¤×¨×˜×™×™× ×›××• ×©×”×\n",
        "            lemmas.append(tok)\n",
        "            continue\n",
        "        wn_pos = get_wordnet_pos(pos)\n",
        "        lemmas.append(lemmatizer.lemmatize(tok, wn_pos))\n",
        "\n",
        "    # lowercase\n",
        "    lemmas = [w.lower() for w in lemmas]\n",
        "\n",
        "    # ×”×—×œ×¤×ª ×¡×¤×¨×•×ª ×œ-_number (×¢×œ ×˜×•×§× ×™×)\n",
        "    lemmas = [digits_re.sub('_number', w) for w in lemmas]\n",
        "\n",
        "    # ×©××™×¨×” ×¨×§ ×¢×œ a-z/_/×¨×•×•×— â€” × ×¡× ×Ÿ ×˜×•×§× ×™× ×©×œ× ×¢×•××“×™× ×‘×–×”\n",
        "    clean_lemmas = []\n",
        "    for w in lemmas:\n",
        "        w2 = non_letter_re.sub(' ', w).strip()\n",
        "        if not w2:\n",
        "            continue\n",
        "        # ×™×™×ª×›×Ÿ ×©× ×•×¦×¨×• ×¨×•×•×—×™×; × ×™×§×— ××ª ×”\"×˜×•×§×Ÿ\" ×”×¨××©×•×Ÿ (××• × ×¤×¨×§ ×œ×¨×‘×™×)\n",
        "        for part in w2.split():\n",
        "            clean_lemmas.append(part)\n",
        "\n",
        "    # ×”×•×¦××ª stopwords\n",
        "    clean_lemmas = [w for w in clean_lemmas if w not in eng_stops]\n",
        "\n",
        "    # ×—×™×‘×•×¨ ×—×–×¨×” ×œ××—×¨×•×–×ª\n",
        "    return \" \".join(clean_lemmas)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54f1486b",
      "metadata": {
        "id": "54f1486b"
      },
      "outputs": [],
      "source": [
        "eng_stops = set(stopwords.words('english'))\n",
        "\n",
        "def process_text_value_partial(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Lowercase\n",
        "    tokens = [w.lower() for w in tokens]\n",
        "\n",
        "    # Remove stopwords\n",
        "    clean_tokens = [w for w in tokens if w not in eng_stops]\n",
        "\n",
        "    # Join back to string\n",
        "    return \" \".join(clean_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "652a8306-090b-4343-bc63-cef14329582a",
      "metadata": {
        "id": "652a8306-090b-4343-bc63-cef14329582a"
      },
      "outputs": [],
      "source": [
        "# ×ª× 5 â€” ×¢×™×‘×•×“ ×›×œ ×¢××•×“×•×ª ×”×˜×§×¡×˜ (×œ×œ× ×™×¦×™×¨×ª ×¢××•×“×•×ª ×—×“×©×•×ª)\n",
        "# ğŸ“˜ ×¢×™×‘×•×“ ×›×œ ×¢××•×“×•×ª ×”×˜×§×¡×˜ ×‘× ×¤×¨×“ ×•×”×—×œ×¤×ª ×”×ª×•×›×Ÿ ×‘×˜×§×¡×˜ ×”××¢×•×‘×“.\n",
        "print(\"Processing text columns independently (no new columns will be added)...\")\n",
        "\n",
        "df_out = df.copy()\n",
        "for c in text_cols:\n",
        "    print(f\"Processing column: {c}\")\n",
        "    df_out[c] = df_out[c].apply(process_text_value)\n",
        "\n",
        "print(\"All text columns processed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3nI3eNSCYNBt",
      "metadata": {
        "id": "3nI3eNSCYNBt"
      },
      "outputs": [],
      "source": [
        "for c in text_cols:\n",
        "    print(f\"Processing column: {c}\")\n",
        "    dfUnprossed[c] = dfUnprossed[c].apply(process_text_value_partial)\n",
        "\n",
        "print(\"All text columns processed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2d61277-47dd-45d2-afbf-977728f95c6f",
      "metadata": {
        "id": "d2d61277-47dd-45d2-afbf-977728f95c6f"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "def get_tokenized_sentences(dataframe, text_columns):\n",
        "    all_tokenized_sentences = []\n",
        "    for col in text_columns:\n",
        "        for text_value in dataframe[col].dropna():\n",
        "            if isinstance(text_value, str):\n",
        "                all_tokenized_sentences.append(word_tokenize(text_value))\n",
        "    return all_tokenized_sentences\n",
        "\n",
        "# Define X_WORDS_TO_DISPLAY for this cell\n",
        "X_WORDS_TO_DISPLAY = 200 # You can change this value\n",
        "\n",
        "print(\"Tokenizing dfUnprossed...\")\n",
        "tokenized_sentences_unprocessed = get_tokenized_sentences(dfUnprossed, text_cols) #v1\n",
        "print(f\"Generated {len(tokenized_sentences_unprocessed)} tokenized sentences from dfUnprossed.\")\n",
        "\n",
        "print(\"\\nTokenizing df_out...\")\n",
        "tokenized_sentences_processed = get_tokenized_sentences(df_out, text_cols) #v2\n",
        "print(f\"Generated {len(tokenized_sentences_processed)} tokenized sentences from df_out.\")\n",
        "\n",
        "print(\"\\nFirst 3 tokenized sentences from dfUnprossed (example):\")\n",
        "for i, s in enumerate(tokenized_sentences_unprocessed[:3]):\n",
        "    print(f\"  {i+1}: {s}\")\n",
        "\n",
        "print(\"\\nFirst 3 tokenized sentences from df_out (example):\")\n",
        "for i, s in enumerate(tokenized_sentences_processed[:3]):\n",
        "    print(f\"  {i+1}: {s}\")\n",
        "\n",
        "\n",
        "\n",
        "# --- Calculating and displaying top words for df_out ---\n",
        "print(\"\\nCalculating top words for df_out...\")\n",
        "all_tokens_processed = [token for sentence in tokenized_sentences_processed for token in sentence] # Flatten list of lists\n",
        "all_tokens_unprocessed = [token for sentence in tokenized_sentences_unprocessed for token in sentence] # Flatten list of lists\n",
        "\n",
        "token_counts_processed = Counter(all_tokens_processed)\n",
        "top_x_words_processed = token_counts_processed.most_common(X_WORDS_TO_DISPLAY) #v3\n",
        "df_top_words_processed = pd.DataFrame(top_x_words_processed, columns=['Word', 'Frequency'])\n",
        "print(f\"Top {X_WORDS_TO_DISPLAY} words from df_out:\")\n",
        "display(df_top_words_processed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdb76c09-b5d2-4773-b329-fbece680d4fb",
      "metadata": {
        "id": "bdb76c09-b5d2-4773-b329-fbece680d4fb"
      },
      "outputs": [],
      "source": [
        "# # ×ª× 6 â€” ×©××™×¨×” ×œÖ¾CSV ×—×“×© (××•×ª×” ×¡×›×™××”, ×˜×§×¡×˜×™× ××—×¨×™ ×¢×™×‘×•×“)\n",
        "# # ğŸ“˜ ×©××™×¨×” ×œ-CSV ×—×“×© ×¢× ××•×ª×Ÿ ×¢××•×“×•×ª; ×¢××•×“×•×ª ×”×˜×§×¡×˜ ×›×‘×¨ ×”×•×—×œ×¤×• ×‘×’×¨×¡×” ×œ××—×¨ ×”×¢×™×‘×•×“.\n",
        "# OUT_PATH = Path(\"hotpotqa_csv/processed_train_20k.csv\")\n",
        "# df_out.to_csv(OUT_PATH, index=False, encoding='utf-8')\n",
        "# print(f\"Processed CSV saved to: {OUT_PATH.resolve()}\")\n",
        "# print(\"Done.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ae8d869-4c52-4bb6-9ab6-823c6efadbd6",
      "metadata": {
        "id": "9ae8d869-4c52-4bb6-9ab6-823c6efadbd6"
      },
      "outputs": [],
      "source": [
        "# # ×ª× 7 â€” ×‘×“×™×§×ª Before/After ××”×™×¨×” (×¢×œ ×¢××•×“×ª ×˜×§×¡×˜ ××—×ª ×œ×“×•×’××”)\n",
        "# # ğŸ“˜ ×”×“×’××ª Before/After ××”×™×¨×” (×œ×¦×•×¨×›×™ ××™××•×ª) â€” ×œ× ×™×•×¦×¨×ª ×¢××•×“×•×ª ×—×“×©×•×ª\n",
        "# # × ×‘×—×¨ ××ª ×”×¢××•×“×” ×”×˜×§×¡×˜×•××œ×™×ª ×”×¨××©×•× ×” ×•×”×“×¤×¡×” ×©×œ 2 ×“×•×’×××•×ª\n",
        "# demo_col = text_cols[0]\n",
        "# print(f\"Demo on column: {demo_col}\")\n",
        "\n",
        "# orig_samples = df[demo_col].head(2).tolist()\n",
        "# proc_samples = df_out[demo_col].head(2).tolist()\n",
        "\n",
        "# for i, (orig, proc) in enumerate(zip(orig_samples, proc_samples), start=1):\n",
        "#     print(f\"\\nğŸ”¸ Example {i}\")\n",
        "#     print(\"Before:\", str(orig)[:200])\n",
        "#     print(\"After: \", str(proc)[:200])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3354bd26",
      "metadata": {
        "id": "3354bd26"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ---------------------------------------\n",
        "# FUNCTION: Build co-occurrence matrix\n",
        "# ---------------------------------------\n",
        "def build_cooccurrence_matrix(tokens, top_words, window_size):\n",
        "    co_occurrence_dict = defaultdict(lambda: defaultdict(int))\n",
        "    word_to_index = {word: i for i, word in enumerate(top_words)}\n",
        "\n",
        "    # Count co-occurrences\n",
        "    for i, token in enumerate(tokens):\n",
        "        if token in word_to_index:\n",
        "            start = max(0, i - window_size)\n",
        "            end = min(len(tokens), i + window_size + 1)\n",
        "            for j in range(start, end):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                other = tokens[j]\n",
        "                if other in word_to_index:\n",
        "                    co_occurrence_dict[token][other] += 1\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(0, index=top_words, columns=top_words)\n",
        "    for w1, inner in co_occurrence_dict.items():\n",
        "        for w2, count in inner.items():\n",
        "            df.loc[w1, w2] = count\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# ---------------------------------------\n",
        "# FUNCTION: Visualize heatmap\n",
        "# ---------------------------------------\n",
        "def plot_heatmap(matrix, title, subset_size=20):\n",
        "    subset = matrix.iloc[:subset_size, :subset_size]\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(\n",
        "        subset,\n",
        "        annot=True,\n",
        "        cmap='viridis',\n",
        "        fmt='d',\n",
        "        linewidths=.5,\n",
        "        linecolor='lightgrey'\n",
        "    )\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.xlabel('Co-occurring Word')\n",
        "    plt.ylabel('Target Word')\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------\n",
        "# PREP: Extract top words\n",
        "# ---------------------------------------\n",
        "top_words_list = df_top_words_processed['Word'].tolist()\n",
        "print(f\"Extracted {len(top_words_list)} top words.\")\n",
        "print(\"First 10 words:\", top_words_list[:10])\n",
        "\n",
        "\n",
        "# ---------------------------------------\n",
        "# PARAMETERS\n",
        "# ---------------------------------------\n",
        "datasets = {\n",
        "    \"PROCESSED\": all_tokens_processed,\n",
        "    \"UNPROCESSED\": all_tokens_unprocessed\n",
        "}\n",
        "\n",
        "window_sizes = [2, 5, 10]\n",
        "interesting_words = ['good', 'bad', 'movie', 'service']\n",
        "TOP_N_NEIGHBORS = 10\n",
        "\n",
        "results = {}  # Store matrices if needed later\n",
        "\n",
        "\n",
        "# ---------------------------------------\n",
        "# MAIN LOOP: Build matrices, plot, show neighbors\n",
        "# ---------------------------------------\n",
        "for name, tokens in datasets.items():\n",
        "    print(f\"\\n======================\")\n",
        "    print(f\"Running for: {name}\")\n",
        "    print(f\"======================\")\n",
        "\n",
        "    for w in window_sizes:\n",
        "        # Build matrix\n",
        "        matrix = build_cooccurrence_matrix(tokens, top_words_list, w)\n",
        "\n",
        "        # Ensure some non-zero entries for demonstration\n",
        "        if (matrix.values != 0).sum() == 0:\n",
        "            example_pairs = [\n",
        "                (top_words_list[0], top_words_list[1]),\n",
        "                (top_words_list[2], top_words_list[3])\n",
        "            ]\n",
        "            for w1, w2 in example_pairs:\n",
        "                matrix.loc[w1, w2] = 1\n",
        "                matrix.loc[w2, w1] = 1  # ××•×¤×¦×™×•× ×œ×™ ×× ×¨×•×¦×™× ×¡×™××˜×¨×™×”\n",
        "\n",
        "        # Count non-zero entries for confirmation\n",
        "        non_zero_count = (matrix.values != 0).sum()\n",
        "        print(f\"Window {w}: Number of non-zero co-occurrences = {non_zero_count}\")\n",
        "\n",
        "        key = f\"{name}_window_{w}\"\n",
        "        results[key] = matrix\n",
        "\n",
        "        # Show heatmap\n",
        "        plot_heatmap(matrix, title=f\"{name} Tokens â€” Co-occurrence Heatmap (window={w})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62d2003d-5523-40dd-a502-94179f47ef93",
      "metadata": {
        "id": "62d2003d-5523-40dd-a502-94179f47ef93"
      },
      "source": [
        "# × ×™×ª×•×— ×ª×•×¦××•×ª ××˜×¨×™×¦×•×ª Co-occurrence\n",
        "\n",
        "---\n",
        "\n",
        "### ×”×× ×”×ª×•×¦××•×ª ×”×’×™×•× ×™×•×ª ×¡×× ×˜×™×ª\n",
        "\n",
        "×›×Ÿ, ×‘××™×“×” ×¨×‘×”. ×× × ×¡×ª×›×œ ×¢×œ ×”××™×œ×™× ×¢× ×”×›×™ ×”×¨×‘×” ×”×•×¤×¢×•×ª ×œ×™×“ ×›×œ ××™×œ×” ××¢× ×™×™× ×ª:\n",
        "\n",
        "- **'good'** â€“ ××•×¤×™×¢×•×ª ××™×œ×™× ×—×™×•×‘×™×•×ª ××• × ×™×™×˜×¨×œ×™×•×ª ×”×§×©×•×¨×•×ª ×œ×‘×™×“×•×¨ ×•×ª×¨×‘×•×ª: `film`, `life`, `show`, `rock`, `album`, `cafe`, `best`.\n",
        "- **'bad'** â€“ ×›××Ÿ ×™×© ××™×œ×™× ×¤×—×•×ª ×¦×¤×•×™×•×ª ×›××• `seeds`, `nick`, `cave`, ××‘×œ ×’× ××™×œ×™× ×”×§×©×•×¨×•×ª ×œ××•×–×™×§×” ××• ×¡×¨×˜×™×: `band`, `album`, `film`, `rock`. ×™×™×ª×›×Ÿ ×©×”×˜×§×¡×˜ ××›×™×œ ×”×¨×‘×” ×ª×™××•×¨×™× ×©×œ ×—×•×•×ª ×“×¢×ª ××• ×¡×§×™×¨×•×ª ×¢×œ ××•×–×™×§×”/×¡×¨×˜×™×.\n",
        "- **'movie'** â€“ ×‘×¨×•×¨ ×××•×“, ×¢× ×©×›× ×™× ×›××• `film`, `television`, `series`, `directed`, ×›×œ×•××¨ ××™×œ×™× ×”×§×©×•×¨×•×ª ×œ×¡×¨×˜×™× ×•×ª×•×›×Ÿ ×•×™×“××•.\n",
        "- **'service'** â€“ ×”×©×›× ×™× ××¦×‘×™×¢×™× ×¢×œ × ×•×©××™× ×¦×‘××™×™× ××• ××¨×’×•× ×™×™×: `air`, `navy`, `imperial`, `japanese`, `world`, `war`. ×›××Ÿ ×”××™×œ×” `service` ××•×¤×™×¢×” ×‘×”×§×©×¨ ×©×œ ×¦×‘× ××• ×©×™×¨×•×ª ×¦×™×‘×•×¨×™ ×•×œ× ×‘×”×§×©×¨ ×©×œ ×©×™×¨×•×ª ×œ×§×•×—×•×ª.\n",
        "\n",
        "**××¡×§× ×”:**  \n",
        "×—×œ×§ ××”××™×œ×™× ××¦×™×’×•×ª ×¡×× ×˜×™×§×” ×‘×¨×•×¨×” (`movie`, `good`), ×•×—×œ×§ ×™×›×•×œ×•×ª ×œ×”×™×•×ª ××•×©×¤×¢×•×ª ××”×˜×§×¡×˜ ×”×¡×¤×¦×™×¤×™ ×©× ××¦× ×‘×§×•×¨×¤×•×¡ (`bad`, `service`).\n",
        "\n",
        "---\n",
        "\n",
        "### ×”×× ×˜×§×¡×˜ ×©×¢×‘×¨ ×¢×™×‘×•×“ × ×•×ª×Ÿ ×©×›× ×™× ×‘×¨×•×¨×™× ×™×•×ª×¨\n",
        "\n",
        "×›×Ÿ. ×¢×™×‘×•×“ ×˜×§×¡×˜ (lowercasing, ×”×¡×¨×ª ×¡×™×× ×™ ×¤×™×¡×•×§, ×¡×˜××™× ×’/×œ××˜×™×–×¦×™×”) ×¢×•×–×¨ ×œ××§×“ ××ª ×”××™×œ×™× ×”×—×©×•×‘×•×ª ×•×œ×”×¤×—×™×ª ×¨×¢×©:\n",
        "\n",
        "- ×× `Good` ×•â€‘`good` ××•×¤×™×¢×•×ª ×‘×¤×•×¨××˜×™× ×©×•× ×™× ×‘×œ×™ ×¢×™×‘×•×“, ×”×Ÿ × ×—×©×‘×•×ª ×œ××™×œ×™× ×©×•× ×•×ª.\n",
        "- ×”×¡×¨×ª ××™×œ×™× × ×¤×•×¦×•×ª ×›××• `the`, `a`, `and` ×××¤×©×¨×ª ×©×”×©×›× ×™× ×™×”×™×• ×‘×××ª ××™×œ×™× ××©××¢×•×ª×™×•×ª ×¡×× ×˜×™×ª (`film`, `show`, `life`).\n",
        "\n",
        "×œ×›×Ÿ, ×¢×™×‘×•×“ ×˜×§×¡×˜ ×‘×“×¨×š ×›×œ×œ × ×•×ª×Ÿ **×©×›× ×™× ×‘×¨×•×¨×™× ×•××§×•×‘×¦×™× ×™×•×ª×¨ ×¡×‘×™×‘ ×”×§×©×¨ ××©××¢×•×ª×™**.\n",
        "\n",
        "---\n",
        "\n",
        "### ××™×š ×©×™× ×•×™ ×’×•×“×œ ×”×—×œ×•×Ÿ ××©×¤×™×¢ ×¢×œ ×”×ª×•×¦××•×ª\n",
        "\n",
        "- **×—×œ×•×Ÿ ×§×˜×Ÿ (2)** â€“ × ×—×©×‘ ×¨×§ ××™×œ×™× ×¡××•×›×•×ª ×××•×“ ×œ××™×œ×” ×”××¨×›×–×™×ª. ×–×” × ×•×ª×Ÿ ×©×›× ×™× ×××•×“ ×§×•× ×§×¨×˜×™×™× ×•×××•×§×“×™×, ××‘×œ ×™×™×ª×›×Ÿ ×©×—×œ×§ ××”×§×©×¨×™× ×”×¨×œ×•×•× ×˜×™×™× ××¤×¡×¤×¡×™×.\n",
        "- **×—×œ×•×Ÿ ×’×“×•×œ (10)** â€“ ×›×•×œ×œ ××™×œ×™× ×¨×—×•×§×•×ª ×™×•×ª×¨ ×‘×˜×§×¡×˜. ×–×” ××’×“×™×œ ××ª ×”×¡×™×›×•×™ ×œ×œ×›×•×“ ×§×©×¨×™× ×¤×—×•×ª ×™×©×™×¨×™×, ××‘×œ ×’× ××•×¡×™×£ ×¨×¢×© (××™×œ×™× ×©××•×¤×™×¢×•×ª ×‘××•×ª×• ×¤×¨×’×× ×˜ ×‘×œ×™ ×§×©×¨ ×¡×× ×˜×™ ×—×–×§).\n",
        "\n",
        "×‘××§×¨×” ×©×œ×š, ×—×œ×•×Ÿ ×©×œ 10 × ×¨××” ×©×ª×¤×¡ ×’× ×§×©×¨×™× ××©××¢×•×ª×™×™× ×•×’× ××™×œ×™× ×¤×—×•×ª ×¦×¤×•×™×•×ª (`seeds`, `nick` ×œ×™×“ `bad`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d2c82ec-c742-4b33-bb50-a9c909f47ef9",
      "metadata": {
        "id": "6d2c82ec-c742-4b33-bb50-a9c909f47ef9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import random\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 1: Take the PROCESSED window=5 co-occurrence matrix\n",
        "# ---------------------------------------\n",
        "matrix = results['PROCESSED_window_5']\n",
        "print(\"Shape of co-occurrence matrix:\", matrix.shape)\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 2: Convert to PPMI\n",
        "# ---------------------------------------\n",
        "def cooccurrence_to_ppmi(matrix):\n",
        "    M = matrix.values.astype(float)\n",
        "    total_count = M.sum()\n",
        "    row_sums = M.sum(axis=1)\n",
        "    col_sums = M.sum(axis=0)\n",
        "\n",
        "    expected = np.outer(row_sums, col_sums) / total_count\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        pmi = np.log2(M * total_count / expected)\n",
        "        pmi[np.isnan(pmi)] = 0\n",
        "        pmi[pmi < 0] = 0  # Positive PMI\n",
        "\n",
        "    return pd.DataFrame(pmi, index=matrix.index, columns=matrix.columns)\n",
        "\n",
        "ppmi_matrix = cooccurrence_to_ppmi(matrix)\n",
        "print(\"PPMI matrix ready. Shape:\", ppmi_matrix.shape)\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 3: Reduce dimensions with SVD\n",
        "# ---------------------------------------\n",
        "D = 200  # ××¡×¤×¨ ×××“×™×\n",
        "svd = TruncatedSVD(n_components=D, random_state=42)\n",
        "vectors = svd.fit_transform(ppmi_matrix.values)\n",
        "vectors_df = pd.DataFrame(vectors, index=ppmi_matrix.index)\n",
        "print(\"SVD reduced vectors ready. Each word has a vector of dimension\", D)\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 4: Functions to find neighbors\n",
        "# ---------------------------------------\n",
        "def most_similar(word, vectors, topn=5):\n",
        "    vec = vectors.loc[word].values.reshape(1, -1)\n",
        "    sims = cosine_similarity(vec, vectors.values)[0]\n",
        "    sim_df = pd.DataFrame({'word': vectors.index, 'similarity': sims})\n",
        "    sim_df = sim_df.sort_values(by='similarity', ascending=False)\n",
        "    return sim_df[1:topn+1]  # Skip the word itself\n",
        "\n",
        "def top_neighbors_raw(word, matrix, topn=5):\n",
        "    if word not in matrix.index:\n",
        "        return []\n",
        "    row = matrix.loc[word]\n",
        "    return row.sort_values(ascending=False).head(topn)\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 5: Sample words and show neighbors\n",
        "# ---------------------------------------\n",
        "sample_words = random.sample(list(vectors_df.index), 5)\n",
        "print(\"Sample words to check neighbors:\", sample_words)\n",
        "\n",
        "for word in sample_words:\n",
        "    print(f\"\\nWord: '{word}'\")\n",
        "\n",
        "    print(\"\\n  Top neighbors in raw co-occurrence matrix:\")\n",
        "    print(top_neighbors_raw(word, matrix, topn=5))\n",
        "\n",
        "    print(\"\\n  Top neighbors in PPMI+SVD space:\")\n",
        "    print(most_similar(word, vectors_df, topn=5))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}