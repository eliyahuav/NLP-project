{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "8baca3dd-ec1a-4dee-bc5b-aa9e48680865",
      "metadata": {
        "id": "8baca3dd-ec1a-4dee-bc5b-aa9e48680865",
        "outputId": "4067fb3f-81fb-479e-b93c-188bb48aff28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== NLP Text Processing Pipeline (20k subset) ===\n",
            "Python: 3.12.12 | Platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "Run started at: 2025-11-16T15:33:52\n"
          ]
        }
      ],
      "source": [
        "#ROI ğŸ“˜ ×ª× â€œ×”×•×›×—×ª ×¨×™×¦×”â€ + ×§×•× ×¤×™×’×•×¨×¦×™×” ×‘×¡×™×¡×™×ª (×ª××™×“ ×™×“×¤×™×¡ ×—×•×ª××ª-×–××Ÿ)\n",
        "import sys, platform, datetime\n",
        "print(\"=== NLP Text Processing Pipeline (20k subset) ===\")\n",
        "print(\"Python:\", sys.version.split()[0], \"| Platform:\", platform.platform())\n",
        "print(\"Run started at:\", datetime.datetime.now().isoformat(timespec='seconds'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "9ff5669d-2694-4447-ae63-2f03c12be22a",
      "metadata": {
        "id": "9ff5669d-2694-4447-ae63-2f03c12be22a",
        "outputId": "ec999ee9-ab4b-4210-fb40-9e30f5b239d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK resources are ready.\n"
          ]
        }
      ],
      "source": [
        "# ×ª× 1 â€” ×˜×¢×™× ×ª ×¡×¤×¨×™×•×ª ×•××©××‘×™ NLTK\n",
        "# ğŸ“˜ ×˜×¢×™× ×ª ×¡×¤×¨×™×•×ª ×•××©××‘×™ NLTK (×™×•×¨×™×“×• ××•×˜×•××˜×™×ª ×× ×—×¡×¨×™×)\n",
        "import pandas as pd\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "print(\"NLTK resources are ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "b8b8f3a3-2f14-42ee-91e9-6aafd64e94ab",
      "metadata": {
        "id": "b8b8f3a3-2f14-42ee-91e9-6aafd64e94ab",
        "outputId": "80277288-050d-48fd-91d9-cc8ea826e864",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset from: /content/train.xls\n",
            "Loaded rows: 90425\n",
            "ID column: id\n",
            "Text columns to process: ['question', 'context', 'answer', 'level', 'type', 'quotes']\n"
          ]
        }
      ],
      "source": [
        "# ×ª× 2 â€” ×˜×¢×™× ×ª ×”Ö¾Dataset, ×–×™×”×•×™ ×¢××•×“×ª ID ×•×¢××•×“×•×ª ×˜×§×¡×˜\n",
        "# ğŸ“˜ ×˜×¢×™× ×ª ×”×“××˜×”, ×–×™×”×•×™ ×¢××•×“×ª ×”-ID (×× ×§×™×™××ª), ×•×–×™×”×•×™ ×¢××•×“×•×ª ×˜×§×¡×˜ (object) ×œ×¢×™×‘×•×“\n",
        "DATA_PATH = \"/content/train.xls\"\n",
        "print(f\"Loading dataset from: {DATA_PATH}\")\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(\"Loaded rows:\", len(df))\n",
        "\n",
        "# ×–×™×”×•×™ ×¢××•×“×ª ID\n",
        "id_candidates = ['id', 'Id', 'ID']\n",
        "ID_COL = next((c for c in id_candidates if c in df.columns), None)\n",
        "print(\"ID column:\", ID_COL if ID_COL else \"None\")\n",
        "\n",
        "# ×¢××•×“×•×ª ×˜×§×¡×˜ (×›×œ ×¢××•×“×” ××¡×•×’ object ×—×•×¥ ××”-ID)\n",
        "text_cols = [c for c in df.columns if df[c].dtype == 'object' and c != ID_COL]\n",
        "print(\"Text columns to process:\", text_cols if text_cols else \"None\")\n",
        "if not text_cols:\n",
        "    raise ValueError(\"No text columns found to process.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "87597e0d-4fea-4e30-b344-6084d73c5766",
      "metadata": {
        "id": "87597e0d-4fea-4e30-b344-6084d73c5766",
        "outputId": "61615417-150b-405b-da16-0ac1e101fd0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtering rows to keep only those with at least one non-empty text column...\n",
            "Rows after non-empty filter: 20000\n",
            "Rows capped at 500: 500\n"
          ]
        }
      ],
      "source": [
        "# ×ª× 3 â€” ×¡×™× ×•×Ÿ ×©×•×¨×•×ª ×œ× ×¨×™×§×•×ª ×•×‘×—×™×¨×” ×‘Ö¾20,000 ×©×•×¨×•×ª\n",
        "# ğŸ“˜ ×©××™×¨×” ×¢×œ ×©×•×¨×•×ª ×©×‘×”×Ÿ ×œ×¤×—×•×ª ××—×ª ××¢××•×“×•×ª ×”×˜×§×¡×˜ ××™× ×” ×¨×™×§×”/×¨×™×§×”-××—×¨×™-strip\n",
        "def non_empty_any(row):\n",
        "    for c in text_cols:\n",
        "        v = row[c]\n",
        "        if isinstance(v, str) and v.strip():\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "print(\"Filtering rows to keep only those with at least one non-empty text column...\")\n",
        "mask = df.apply(non_empty_any, axis=1)\n",
        "df = df[mask]\n",
        "\n",
        "print(\"Rows after non-empty filter:\", len(df))\n",
        "df = df.head(500)  # ×‘×—×™×¨×” ×‘Ö¾20,000 ×©×•×¨×•×ª ×”×¨××©×•× ×•×ª\n",
        "print(\"Rows capped at 500:\", len(df))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfUnprossed = df.copy()\n"
      ],
      "metadata": {
        "id": "zyhBLrFSU4-l"
      },
      "id": "zyhBLrFSU4-l",
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "id": "f75c3ea9-2602-433b-9934-cccac5f026ff",
      "metadata": {
        "id": "f75c3ea9-2602-433b-9934-cccac5f026ff"
      },
      "outputs": [],
      "source": [
        "# ×ª× 4 â€” ×¤×•× ×§×¦×™×•×ª ×¢×™×‘×•×“: × ×™×§×•×™, Lemmatization ×¢× POS, ×•×”×•×¦××ª Stopwords\n",
        "# ğŸ“˜ ×¤×•× ×§×¦×™×•×ª ×¢×™×‘×•×“ ×˜×§×¡×˜ ×œ×›×œ ×¢××•×“×” ×‘× ×¤×¨×“:\n",
        "#    - ×”×¡×¨×ª URL/Email/Handles\n",
        "#    - ×˜×•×§× ×™×–×¦×™×”\n",
        "#    - POS tagging + Lemmatization (×¢× ××™×¤×•×™ ×œ-WordNet)\n",
        "#    - ×“×™×œ×•×’ ×¢×œ ×©××•×ª ×¤×¨×˜×™×™× (NNP/NNPS)\n",
        "#    - × ×¨××•×œ ×¦×•×¨×•×ª 'be' (am/is/are/was/were/been/being -> be)\n",
        "#    - ×”×—×œ×¤×ª ×¡×¤×¨×•×ª ×‘-_number\n",
        "#    - ×”×¡×¨×ª ×ª×•×•×™× ×©××™× × ××•×ª×™×•×ª ×œ×˜×™× ×™×•×ª/underscore/×¨×•×•×—\n",
        "#    - ×”×¡×¨×ª stopwords\n",
        "#    - ×”×•×¨×“×ª ×¨×™×©×™×•×ª\n",
        "#\n",
        "# ×”×¢×¨×”: ×œ× × ×•×¦×¨×™× ×¢××•×“×•×ª ×—×“×©×•×ª â€” ×”×¤×•× ×§×¦×™×” ×ª×—×–×™×¨ ××—×¨×•×–×ª ××¢×•×‘×“×ª ×©×ª×—×œ×™×£ ××ª ×ª×•×›×Ÿ ×”×¢××•×“×”.\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "eng_stops = set(stopwords.words('english'))\n",
        "BE_FORMS = {\"am\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\"}\n",
        "\n",
        "def get_wordnet_pos(tag: str):\n",
        "    if tag.startswith('J'): return wordnet.ADJ\n",
        "    if tag.startswith('V'): return wordnet.VERB\n",
        "    if tag.startswith('N'): return wordnet.NOUN\n",
        "    if tag.startswith('R'): return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "url_email_handle_re = re.compile(r'(https?://\\S+|www\\.\\S+|\\S+@\\S+|[@#]\\w+)', re.IGNORECASE)\n",
        "digits_re = re.compile(r'\\d+')           # ×¡×¤×¨×•×ª -> _number\n",
        "non_letter_re = re.compile(r'[^a-z_ ]+') # ××—×¨×™ lowercase, × ×©××™×¨ a-z, ×¨×•×•×—, underscore\n",
        "\n",
        "def process_text_value(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    t = text\n",
        "\n",
        "    # ×”×¡×¨×” ×¨××©×•× ×™×ª ×©×œ URL/Emails/Handles/Hashtags ×›×“×™ ×œ× ×œ×”×¨×•×¡ POS\n",
        "    t = url_email_handle_re.sub(' ', t)\n",
        "\n",
        "    # ×˜×•×§× ×™×–×¦×™×” + POS ×¢×œ ×”×˜×§×¡×˜ ×”××§×•×¨×™ (×œ×¤× ×™ lowercase) ×œ×˜×•×‘×ª Proper Nouns ×˜×•×‘ ×™×•×ª×¨\n",
        "    tokens = word_tokenize(t)\n",
        "    tagged = pos_tag(tokens)\n",
        "\n",
        "    # Lemmatization ×¢× POS + ×“×™×œ×•×’ ×¢×œ Proper Nouns + × ×¨××•×œ 'be'\n",
        "    lemmas = []\n",
        "    for tok, pos in tagged:\n",
        "        # × ×¨××•×œ ××•×§×“× ×œ-be\n",
        "        if tok.lower() in BE_FORMS:\n",
        "            lemmas.append(\"be\")\n",
        "            continue\n",
        "        if pos in (\"NNP\", \"NNPS\"):   # ×”×©××¨×ª ×©××•×ª ×¤×¨×˜×™×™× ×›××• ×©×”×\n",
        "            lemmas.append(tok)\n",
        "            continue\n",
        "        wn_pos = get_wordnet_pos(pos)\n",
        "        lemmas.append(lemmatizer.lemmatize(tok, wn_pos))\n",
        "\n",
        "    # lowercase\n",
        "    lemmas = [w.lower() for w in lemmas]\n",
        "\n",
        "    # ×”×—×œ×¤×ª ×¡×¤×¨×•×ª ×œ-_number (×¢×œ ×˜×•×§× ×™×)\n",
        "    lemmas = [digits_re.sub('_number', w) for w in lemmas]\n",
        "\n",
        "    # ×©××™×¨×” ×¨×§ ×¢×œ a-z/_/×¨×•×•×— â€” × ×¡× ×Ÿ ×˜×•×§× ×™× ×©×œ× ×¢×•××“×™× ×‘×–×”\n",
        "    clean_lemmas = []\n",
        "    for w in lemmas:\n",
        "        w2 = non_letter_re.sub(' ', w).strip()\n",
        "        if not w2:\n",
        "            continue\n",
        "        # ×™×™×ª×›×Ÿ ×©× ×•×¦×¨×• ×¨×•×•×—×™×; × ×™×§×— ××ª ×”\"×˜×•×§×Ÿ\" ×”×¨××©×•×Ÿ (××• × ×¤×¨×§ ×œ×¨×‘×™×)\n",
        "        for part in w2.split():\n",
        "            clean_lemmas.append(part)\n",
        "\n",
        "    # ×”×•×¦××ª stopwords\n",
        "    clean_lemmas = [w for w in clean_lemmas if w not in eng_stops]\n",
        "\n",
        "    # ×—×™×‘×•×¨ ×—×–×¨×” ×œ××—×¨×•×–×ª\n",
        "    return \" \".join(clean_lemmas)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54f1486b"
      },
      "source": [
        "eng_stops = set(stopwords.words('english'))\n",
        "\n",
        "def process_text_value_partial(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Lowercase\n",
        "    tokens = [w.lower() for w in tokens]\n",
        "\n",
        "    # Remove stopwords\n",
        "    clean_tokens = [w for w in tokens if w not in eng_stops]\n",
        "\n",
        "    # Join back to string\n",
        "    return \" \".join(clean_tokens)"
      ],
      "id": "54f1486b",
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ed83cb9"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "\n",
        "X_WORDS_TO_DISPLAY = 20 # You can change this value\n",
        "\n",
        "all_tokens = []\n",
        "# Assuming text_cols contains the names of the text columns in df_out\n",
        "# If text_cols is not defined or accurate, you might need to adjust this.\n",
        "# For robustness, we can iterate through all object columns.\n",
        "for col in df_out.columns:\n",
        "    if df_out[col].dtype == 'object' and col != ID_COL: # Exclude ID column\n",
        "        for text_val in df_out[col].dropna():\n",
        "            all_tokens.extend(text_val.split())\n",
        "\n",
        "token_counts = Counter(all_tokens)\n",
        "top_x_words = token_counts.most_common(X_WORDS_TO_DISPLAY)\n",
        "\n",
        "df_top_words = pd.DataFrame(top_x_words, columns=['Word', 'Frequency'])\n",
        "display(df_top_words)"
      ],
      "id": "3ed83cb9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "id": "652a8306-090b-4343-bc63-cef14329582a",
      "metadata": {
        "id": "652a8306-090b-4343-bc63-cef14329582a",
        "outputId": "4e978b16-c1bf-46b5-f798-819e84756926",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing text columns independently (no new columns will be added)...\n",
            "Processing column: question\n",
            "Processing column: context\n",
            "Processing column: answer\n",
            "Processing column: level\n",
            "Processing column: type\n",
            "Processing column: quotes\n",
            "All text columns processed.\n"
          ]
        }
      ],
      "source": [
        "# ×ª× 5 â€” ×¢×™×‘×•×“ ×›×œ ×¢××•×“×•×ª ×”×˜×§×¡×˜ (×œ×œ× ×™×¦×™×¨×ª ×¢××•×“×•×ª ×—×“×©×•×ª)\n",
        "# ğŸ“˜ ×¢×™×‘×•×“ ×›×œ ×¢××•×“×•×ª ×”×˜×§×¡×˜ ×‘× ×¤×¨×“ ×•×”×—×œ×¤×ª ×”×ª×•×›×Ÿ ×‘×˜×§×¡×˜ ×”××¢×•×‘×“.\n",
        "print(\"Processing text columns independently (no new columns will be added)...\")\n",
        "\n",
        "df_out = df.copy()\n",
        "for c in text_cols:\n",
        "    print(f\"Processing column: {c}\")\n",
        "    df_out[c] = df_out[c].apply(process_text_value)\n",
        "\n",
        "print(\"All text columns processed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for c in text_cols:\n",
        "    print(f\"Processing column: {c}\")\n",
        "    dfUnprossed[c] = dfUnprossed[c].apply(process_text_value_partial)\n",
        "\n",
        "print(\"All text columns processed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nI3eNSCYNBt",
        "outputId": "8bd77380-2833-49b7-9e91-d1d24deacaa2"
      },
      "id": "3nI3eNSCYNBt",
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing column: question\n",
            "Processing column: context\n",
            "Processing column: answer\n",
            "Processing column: level\n",
            "Processing column: type\n",
            "Processing column: quotes\n",
            "All text columns processed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdb76c09-b5d2-4773-b329-fbece680d4fb",
      "metadata": {
        "id": "bdb76c09-b5d2-4773-b329-fbece680d4fb"
      },
      "outputs": [],
      "source": [
        "# ×ª× 6 â€” ×©××™×¨×” ×œÖ¾CSV ×—×“×© (××•×ª×” ×¡×›×™××”, ×˜×§×¡×˜×™× ××—×¨×™ ×¢×™×‘×•×“)\n",
        "# ğŸ“˜ ×©××™×¨×” ×œ-CSV ×—×“×© ×¢× ××•×ª×Ÿ ×¢××•×“×•×ª; ×¢××•×“×•×ª ×”×˜×§×¡×˜ ×›×‘×¨ ×”×•×—×œ×¤×• ×‘×’×¨×¡×” ×œ××—×¨ ×”×¢×™×‘×•×“.\n",
        "OUT_PATH = Path(\"hotpotqa_csv/processed_train_20k.csv\")\n",
        "df_out.to_csv(OUT_PATH, index=False, encoding='utf-8')\n",
        "print(f\"Processed CSV saved to: {OUT_PATH.resolve()}\")\n",
        "print(\"Done.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ae8d869-4c52-4bb6-9ab6-823c6efadbd6",
      "metadata": {
        "id": "9ae8d869-4c52-4bb6-9ab6-823c6efadbd6"
      },
      "outputs": [],
      "source": [
        "# ×ª× 7 â€” ×‘×“×™×§×ª Before/After ××”×™×¨×” (×¢×œ ×¢××•×“×ª ×˜×§×¡×˜ ××—×ª ×œ×“×•×’××”)\n",
        "# ğŸ“˜ ×”×“×’××ª Before/After ××”×™×¨×” (×œ×¦×•×¨×›×™ ××™××•×ª) â€” ×œ× ×™×•×¦×¨×ª ×¢××•×“×•×ª ×—×“×©×•×ª\n",
        "# × ×‘×—×¨ ××ª ×”×¢××•×“×” ×”×˜×§×¡×˜×•××œ×™×ª ×”×¨××©×•× ×” ×•×”×“×¤×¡×” ×©×œ 2 ×“×•×’×××•×ª\n",
        "demo_col = text_cols[0]\n",
        "print(f\"Demo on column: {demo_col}\")\n",
        "\n",
        "orig_samples = df[demo_col].head(2).tolist()\n",
        "proc_samples = df_out[demo_col].head(2).tolist()\n",
        "\n",
        "for i, (orig, proc) in enumerate(zip(orig_samples, proc_samples), start=1):\n",
        "    print(f\"\\nğŸ”¸ Example {i}\")\n",
        "    print(\"Before:\", str(orig)[:200])\n",
        "    print(\"After: \", str(proc)[:200])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9888323a-9cb4-44c9-809b-a34787e0ca1a",
      "metadata": {
        "id": "9888323a-9cb4-44c9-809b-a34787e0ca1a"
      },
      "outputs": [],
      "source": [
        "# ×ª× â€“ Exploratory & Visual Statistics\n",
        "# ğŸ“Š ×¡×˜×˜×™×¡×˜×™×§×•×ª ×•×™×–×•××œ×™×•×ª ×•× ×™×ª×•×— ×¨××©×•× ×™ ×¢×œ ×”×“××˜×” ×œ××—×¨ ×”×¢×™×‘×•×“\n",
        "# ------------------------------------------------------------\n",
        "# ×›×•×œ×œ:\n",
        "# 1. ×’×¨×£ ×”×ª×¤×œ×’×•×ª ×§×˜×’×•×¨×™×•×ª ×›×œ×œ×™×•×ª (labels)\n",
        "# 2. ×’×¨×£ ××™×•×—×“ ×œ×¢××•×“×ª 'level'\n",
        "# 3. ×”×™×¡×˜×•×’×¨××ª ××•×¨×š ×˜×§×¡×˜×™×\n",
        "# 4. ×¨×©×™××ª ××™×œ×™× ×•×‘×™×˜×•×™×™× ×©×›×™×—×™× (unigrams/bigrams)\n",
        "# 5. ×˜×‘×œ×ª ×¡×™×›×•× ×©×œ×‘×™×\n",
        "# 6. ×“×•×’×××•×ª ××™×›×•×ª ×œ×¤×™ ×§×˜×’×•×¨×™×”\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "\n",
        "plt.rcParams.update({'axes.unicode_minus': False})\n",
        "\n",
        "# -----------------------------------\n",
        "# 1. ×’×¨×£ ×”×ª×¤×œ×’×•×ª ×§×˜×’×•×¨×™×•×ª ×›×œ×œ×™×•×ª\n",
        "# -----------------------------------\n",
        "print(\"ğŸ”¹ Checking for categorical (label) columns...\")\n",
        "candidate_labels = ['label','category','topic','type','sentiment','class','answer']\n",
        "label_col = next((c for c in candidate_labels if c in df_out.columns and df_out[c].notna().any()), None)\n",
        "\n",
        "if label_col:\n",
        "    print(f\"Label column detected: {label_col}\")\n",
        "    label_counts = df_out[label_col].astype(str).value_counts()\n",
        "    print(\"\\nTop categories:\")\n",
        "    print(label_counts.head(10))\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    label_counts.plot(kind='bar', color='cornflowerblue')\n",
        "    plt.title(f\"Category Distribution â€“ {label_col}\")\n",
        "    plt.xlabel(\"Category\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No generic label column detected â€” skipping label distribution plot.\")\n",
        "\n",
        "# -----------------------------------\n",
        "# 2. ×’×¨×£ ×œ×¢××•×“×ª LEVEL\n",
        "# -----------------------------------\n",
        "if 'level' in df_out.columns:\n",
        "    print(\"\\nğŸ”¹ Found 'level' column â€“ displaying its distribution...\")\n",
        "\n",
        "    level_counts = df_out['level'].astype(str).value_counts()\n",
        "    print(level_counts.head(10))\n",
        "\n",
        "    # ×’×¨×£ ×¢××•×“×•×ª\n",
        "    plt.figure(figsize=(8,4))\n",
        "    level_counts.plot(kind='bar', color='mediumseagreen')\n",
        "    plt.title(\"Distribution of LEVEL categories\")\n",
        "    plt.xlabel(\"Level\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # ×’×¨×£ ×¢×•×’×” (×× ×™×© ×œ× ×™×•×ª×¨ ××“×™ ×§×˜×’×•×¨×™×•×ª)\n",
        "    if len(level_counts) <= 10:\n",
        "        plt.figure(figsize=(6,6))\n",
        "        plt.pie(level_counts.values, labels=level_counts.index, autopct='%1.1f%%', startangle=90)\n",
        "        plt.title(\"LEVEL category distribution (Pie chart)\")\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"\\nNo 'level' column found â€” skipping LEVEL charts.\")\n",
        "\n",
        "# -----------------------------------\n",
        "# 3. ×”×™×¡×˜×•×’×¨××ª ××•×¨×š ×˜×§×¡×˜×™×\n",
        "# -----------------------------------\n",
        "print(\"\\nğŸ”¹ Text length histogram (in tokens):\")\n",
        "\n",
        "def count_tokens(text):\n",
        "    if not isinstance(text, str):\n",
        "        return 0\n",
        "    return len(text.split())\n",
        "\n",
        "text_col = next((c for c in df_out.columns if c not in ['id','Id','ID','level',label_col] and df_out[c].dtype=='object'), None)\n",
        "if text_col:\n",
        "    df_out['text_length'] = df_out[text_col].apply(count_tokens)\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.hist(df_out['text_length'], bins=30, color='lightblue', edgecolor='black')\n",
        "    plt.title(f\"Histogram of text lengths ({text_col})\")\n",
        "    plt.xlabel(\"Number of tokens\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Average length:\", round(df_out['text_length'].mean(),2))\n",
        "    print(\"Median length:\", round(df_out['text_length'].median(),2))\n",
        "else:\n",
        "    print(\"No text column detected for histogram.\")\n",
        "\n",
        "# -----------------------------------\n",
        "# 4. ×¨×©×™××ª ××™×œ×™× ×•×‘×™×˜×•×™×™× ×©×›×™×—×™×\n",
        "# -----------------------------------\n",
        "print(\"\\nğŸ”¹ Most frequent tokens and bigrams:\")\n",
        "tokens = []\n",
        "for c in df_out.columns:\n",
        "    if df_out[c].dtype == 'object' and c not in [label_col,'level']:\n",
        "        for text in df_out[c].dropna():\n",
        "            tokens.extend(text.split())\n",
        "\n",
        "token_counts = Counter(tokens)\n",
        "top_tokens = token_counts.most_common(20)\n",
        "print(\"\\nTop 20 tokens:\")\n",
        "for w, f in top_tokens:\n",
        "    print(f\"{w:<15} {f}\")\n",
        "\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "bigram_counts = Counter(bigrams)\n",
        "top_bigrams = bigram_counts.most_common(10)\n",
        "print(\"\\nTop 10 bigrams:\")\n",
        "for (a,b), f in top_bigrams:\n",
        "    print(f\"{a} {b:<15} {f}\")\n",
        "\n",
        "# -----------------------------------\n",
        "# 5. ×˜×‘×œ×ª ×¡×™×›×•× × ×™×§×•×™ (×× ×§×™×™××ª)\n",
        "# -----------------------------------\n",
        "print(\"\\nğŸ”¹ Cleaning summary table (estimated):\")\n",
        "try:\n",
        "    summary\n",
        "    display(summary)\n",
        "except NameError:\n",
        "    print(\"(No detailed step metrics found in current notebook scope.)\")\n",
        "    print(f\"Current dataset size: {len(df_out)} rows\")\n",
        "\n",
        "# -----------------------------------\n",
        "# 6. ×“×•×’×××•×ª ××™×›×•×ª ×œ×¤×™ ×§×˜×’×•×¨×™×”\n",
        "# -----------------------------------\n",
        "print(\"\\nğŸ”¹ Sample examples per category:\")\n",
        "target_col = label_col or ('level' if 'level' in df_out.columns else None)\n",
        "if target_col:\n",
        "    for cat in df_out[target_col].astype(str).unique()[:5]:  # ×¢×“ 5 ×§×˜×’×•×¨×™×•×ª\n",
        "        print(f\"\\nCategory: {cat}\")\n",
        "        samples = df_out[df_out[target_col].astype(str) == cat][text_col].dropna().head(2)\n",
        "        for i, s in enumerate(samples, start=1):\n",
        "            print(f\" Example {i}: {s[:250]}\")\n",
        "else:\n",
        "    print(\"No label or level column available for examples.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce3bcad4-e41c-467d-a064-fddf45b21087",
      "metadata": {
        "id": "ce3bcad4-e41c-467d-a064-fddf45b21087"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}