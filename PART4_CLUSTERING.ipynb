{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***תרגיל 4   של הפרוייקט***"
      ],
      "metadata": {
        "id": "BWDmLmxFJX1B"
      },
      "id": "BWDmLmxFJX1B"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***א-1***"
      ],
      "metadata": {
        "id": "f_aW02ESJLOo"
      },
      "id": "f_aW02ESJLOo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Load data and basic inspection***\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5gm7encWEu_u"
      },
      "id": "5gm7encWEu_u"
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 1: Load data and basic inspection ===\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the filtered dataset from disk (Colab path)\n",
        "filtered_df = pd.read_csv(\"/content/train-filtered_question_level.csv\")\n",
        "\n",
        "# Remove duplicate questions to avoid biasing the model with repeated texts\n",
        "filtered_df = filtered_df.drop_duplicates(subset=[\"question\"], keep=\"first\")\n",
        "\n",
        "# Sanity check: show columns and first rows to verify the structure\n",
        "print(\"Columns in DataFrame:\")\n",
        "print(filtered_df.columns)\n",
        "\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(filtered_df.head())\n",
        "\n",
        "# Show global label distribution for 'level' (if exists), to understand dataset balance\n",
        "if \"level\" in filtered_df.columns:\n",
        "    print(\"\\nGlobal distribution of 'level':\")\n",
        "    print(filtered_df[\"level\"].value_counts(normalize=True))\n",
        "else:\n",
        "    print(\"\\nColumn 'level' not found in DataFrame.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxymC_hSmq8K",
        "outputId": "daff464a-f193-480f-b989-39a31ec44ea8"
      },
      "id": "sxymC_hSmq8K",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in DataFrame:\n",
            "Index(['question', 'level'], dtype='object')\n",
            "\n",
            "First 5 rows:\n",
            "                                            question   level\n",
            "0  Which magazine was started first Arthur's Maga...  medium\n",
            "1  The Oberoi family is part of a hotel company t...  medium\n",
            "2  Musician and satirist Allie Goertz wrote a son...    hard\n",
            "3    What nationality was James Henry Miller's wife?  medium\n",
            "4  Cadmium Chloride is slightly soluble in this c...  medium\n",
            "\n",
            "Global distribution of 'level':\n",
            "level\n",
            "medium    0.628149\n",
            "easy      0.198688\n",
            "hard      0.173162\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***חלוקה מאוזנת ל־train / validation / test (עם stratify)***"
      ],
      "metadata": {
        "id": "2x9zwKKfpeX-"
      },
      "id": "2x9zwKKfpeX-"
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define split proportions\n",
        "# TEST_SIZE = 0.15      # 15% of total data for test\n",
        "# VAL_SIZE = 0.15       # 15% of total data for validation\n",
        "# RANDOM_STATE = 42     # For reproducibility\n",
        "\n",
        "# # Compute validation size relative to the remaining data after test split\n",
        "# val_size_relative = VAL_SIZE / (1 - TEST_SIZE)  # e.g., 0.15 / 0.85\n",
        "\n",
        "# print(\"Relative validation size (from train_val):\", val_size_relative)\n",
        "\n",
        "# # Step 1: Split into train_val and test with stratification on 'level'\n",
        "# train_val_df, test_df = train_test_split(\n",
        "#     filtered_df,\n",
        "#     test_size=TEST_SIZE,\n",
        "#     stratify=filtered_df[\"level\"],\n",
        "#     random_state=RANDOM_STATE\n",
        "# )\n",
        "\n",
        "# # Step 2: Split train_val into train and validation with stratification on 'level'\n",
        "# train_df, val_df = train_test_split(\n",
        "#     train_val_df,\n",
        "#     test_size=val_size_relative,\n",
        "#     stratify=train_val_df[\"level\"],\n",
        "#     random_state=RANDOM_STATE\n",
        "# )\n",
        "\n",
        "# print(\"Finished stratified split into train / validation / test.\")\n"
      ],
      "metadata": {
        "id": "m176qvlxpdZJ"
      },
      "id": "m176qvlxpdZJ",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***בדיקה שהחלוקה מאוזנת (stratified) ושיש לנו את היחסים הרצויים***"
      ],
      "metadata": {
        "id": "rpBACoIoplWC"
      },
      "id": "rpBACoIoplWC"
    },
    {
      "cell_type": "code",
      "source": [
        "# def print_split_info(df, name):\n",
        "#     print(f\"\\n{name}:\")\n",
        "#     print(\"Number of rows:\", len(df))\n",
        "#     print(\"Label distribution for 'level':\")\n",
        "#     print(df[\"level\"].value_counts(normalize=True))\n",
        "\n",
        "# print(\"Total rows in original filtered_df:\", len(filtered_df))\n",
        "\n",
        "# print_split_info(train_df, \"Train set\")\n",
        "# print_split_info(val_df, \"Validation set\")\n",
        "# print_split_info(test_df, \"Test set\")\n"
      ],
      "metadata": {
        "id": "pev31-BLplxP"
      },
      "id": "pev31-BLplxP",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KtfJcpMAJofI"
      },
      "id": "KtfJcpMAJofI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Text preprocessing (tokenization + lemmatization)***"
      ],
      "metadata": {
        "id": "zeHPEQzQJsf6"
      },
      "id": "zeHPEQzQJsf6"
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 2 (HARD FIX): NLTK setup and robust preprocessing ===\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag\n",
        "\n",
        "# Download required NLTK resources (run once per runtime)\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "eng_stops = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Normalize all forms of the verb \"to be\" into a single token \"be\"\n",
        "BE_FORMS = {\"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\"}\n",
        "\n",
        "\n",
        "def get_wordnet_pos(tag: str):\n",
        "    \"\"\"\n",
        "    Map POS tag from nltk.pos_tag to a WordNet POS tag.\n",
        "    This helps the lemmatizer pick the correct base form.\n",
        "    \"\"\"\n",
        "    if tag.startswith(\"J\"):\n",
        "        return wordnet.ADJ\n",
        "    if tag.startswith(\"V\"):\n",
        "        return wordnet.VERB\n",
        "    if tag.startswith(\"N\"):\n",
        "        return wordnet.NOUN\n",
        "    if tag.startswith(\"R\"):\n",
        "        return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "\n",
        "# Regex patterns for cleaning\n",
        "# Remove URLs, emails, @handles, #hashtags\n",
        "url_email_handle_re = re.compile(r\"(https?://\\S+|www\\.\\S+|\\S+@\\S+|[@#]\\w+)\", re.IGNORECASE)\n",
        "\n",
        "# Detect any digit inside a token\n",
        "digits_re = re.compile(r\"\\d\")\n",
        "\n",
        "# For NON-numeric tokens: remove everything except [a-z] and spaces\n",
        "non_letter_re = re.compile(r\"[^a-z ]+\")\n",
        "\n",
        "\n",
        "def process_text_value(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Full preprocessing for a single text value:\n",
        "    - Remove URLs, emails, and @handles/#hashtags\n",
        "    - Tokenize\n",
        "    - POS tagging\n",
        "    - Lemmatization with POS\n",
        "    - Normalize all 'be' verb forms to 'be'\n",
        "    - Any token that contains at least one digit -> '_number' (entire token)\n",
        "    - For other tokens: strip punctuation/non-letters, keep only [a-z] and spaces\n",
        "    - Finally, any token that still contains the substring 'number' is collapsed to '_number'\n",
        "    - (Optional) Remove stopwords [currently commented out]\n",
        "    - Lowercase\n",
        "    Returns a cleaned string with space-separated tokens.\n",
        "    \"\"\"\n",
        "    # Safely handle missing or non-string values\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove URLs, emails, handles, hashtags\n",
        "    t = url_email_handle_re.sub(\" \", text)\n",
        "\n",
        "    # Tokenize and POS-tag on original (cleaned) text\n",
        "    tokens = word_tokenize(t)\n",
        "    tagged = pos_tag(tokens)\n",
        "\n",
        "    lemmas = []\n",
        "    for tok, pos in tagged:\n",
        "        # Normalize 'be' forms early to reduce sparsity\n",
        "        if tok.lower() in BE_FORMS:\n",
        "            lemmas.append(\"be\")\n",
        "            continue\n",
        "\n",
        "        # Map POS tag to WordNet POS tag and lemmatize\n",
        "        wn_pos = get_wordnet_pos(pos)\n",
        "        lemma = lemmatizer.lemmatize(tok, wn_pos)\n",
        "        lemmas.append(lemma)\n",
        "\n",
        "    # Lowercase all tokens\n",
        "    lemmas = [w.lower() for w in lemmas]\n",
        "\n",
        "    intermediate = []\n",
        "    for w in lemmas:\n",
        "        # If the token contains ANY digit, replace the entire token with '_number'\n",
        "        if digits_re.search(w):\n",
        "            intermediate.append(\"_number\")\n",
        "            continue\n",
        "\n",
        "        # For non-numeric tokens: remove punctuation and non-letters\n",
        "        w2 = non_letter_re.sub(\" \", w).strip()\n",
        "        if not w2:\n",
        "            # Skip tokens that became empty after cleaning\n",
        "            continue\n",
        "\n",
        "        # If cleaning produced multiple parts (e.g. \"word-word\" -> \"word word\")\n",
        "        for part in w2.split():\n",
        "            if not part:\n",
        "                continue\n",
        "            intermediate.append(part)\n",
        "\n",
        "    # Final pass: collapse any token that still contains 'number' into '_number'\n",
        "    # This guarantees we do not get '_numbera', '_numberkm', etc.\n",
        "    clean_lemmas = []\n",
        "    for w in intermediate:\n",
        "        if \"number\" in w:\n",
        "            clean_lemmas.append(\"_number\")\n",
        "        else:\n",
        "            clean_lemmas.append(w)\n",
        "\n",
        "    # If you want to remove stopwords, uncomment the next line\n",
        "    # clean_lemmas = [w for w in clean_lemmas if w not in eng_stops]\n",
        "\n",
        "    # Join tokens back into a single cleaned string\n",
        "    return \" \".join(clean_lemmas)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toAKXIEoMcY3",
        "outputId": "e39fa0ce-e2a5-4991-adc0-e71217574516"
      },
      "id": "toAKXIEoMcY3",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Apply preprocessing to all questions***"
      ],
      "metadata": {
        "id": "qwOP_ovgRKwO"
      },
      "id": "qwOP_ovgRKwO"
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 3: Apply preprocessing to all questions ===\n",
        "\n",
        "# Ensure the 'question' column exists before applying preprocessing\n",
        "if \"question\" not in filtered_df.columns:\n",
        "    raise KeyError(\"The DataFrame does not contain a 'question' column.\")\n",
        "\n",
        "# Apply the preprocessing function to every question in the dataset\n",
        "# This creates a new column 'question_clean' that contains the normalized text\n",
        "filtered_df[\"question_clean\"] = filtered_df[\"question\"].apply(process_text_value)\n",
        "\n",
        "# Inspect a few examples to verify that preprocessing works as expected\n",
        "print(\"Original vs. cleaned examples:\\n\")\n",
        "for i in range(5):\n",
        "    print(f\"--- Example {i+1} ---\")\n",
        "    print(\"Original :\", filtered_df.loc[filtered_df.index[i], \"question\"])\n",
        "    print(\"Cleaned  :\", filtered_df.loc[filtered_df.index[i], \"question_clean\"])\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "VPjsviBsQxIg",
        "outputId": "3cf10594-1da1-4c09-ea63-9b5c98f5bb87"
      },
      "id": "VPjsviBsQxIg",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1405751402.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Apply the preprocessing function to every question in the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# This creates a new column 'question_clean' that contains the normalized text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfiltered_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question_clean\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_text_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Inspect a few examples to verify that preprocessing works as expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-444403778.py\u001b[0m in \u001b[0;36mprocess_text_value\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m# Tokenize and POS-tag on original (cleaned) text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mtagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***TF-IDF vectorization of the preprocessed questions***"
      ],
      "metadata": {
        "id": "ovO3e28_RrXX"
      },
      "id": "ovO3e28_RrXX"
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 4: TF-IDF vectorization for ALL questions (no train/val/test split) ===\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Safety check: make sure 'clean_text' exists\n",
        "if \"clean_text\" not in filtered_df.columns:\n",
        "    raise KeyError(\"The DataFrame does not contain a 'clean_text' column. \"\n",
        "                   \"Run the preprocessing cell first.\")\n",
        "\n",
        "# Define a TF-IDF vectorizer\n",
        "# max_features limits vocabulary size to the most frequent terms\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=10000,   # limit vocabulary size (you can tune this later)\n",
        "    ngram_range=(1, 1),   # unigrams only\n",
        ")\n",
        "\n",
        "# Fit TF-IDF on the entire cleaned corpus and transform it to a sparse matrix\n",
        "# Each row = one question, each column = one term from the vocabulary\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(filtered_df[\"clean_text\"])\n",
        "\n",
        "print(\"TF-IDF matrix shape (n_samples, n_features):\", X_tfidf.shape)\n",
        "print(\"(Num of documents, max_features)\")\n",
        "\n",
        "# Optional: extract labels if you need them later for supervised models / evaluation\n",
        "if \"level\" in filtered_df.columns:\n",
        "    y = filtered_df[\"level\"].values\n",
        "    print(\"Labels vector shape:\", y.shape)\n",
        "else:\n",
        "    y = None\n",
        "    print(\"No 'level' column found. y is set to None.\")\n",
        "\n",
        "# Show a small sample of feature names for sanity check\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "print(\"\\nVocabulary size (len(feature_names)):\", len(feature_names))\n",
        "print(\"First 30 features:\\n\", feature_names[:60])\n"
      ],
      "metadata": {
        "id": "tqDPZN-mRvtC"
      },
      "id": "tqDPZN-mRvtC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Run K-Means for different K values***"
      ],
      "metadata": {
        "id": "EcDOCcPqfNof"
      },
      "id": "EcDOCcPqfNof"
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 5: Run K-Means for several K values ===\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Choose several K values\n",
        "k_values = [2, 3, 7, 15]\n",
        "# k_values = [2]\n",
        "inertia_scores = []\n",
        "silhouette_scores = []\n",
        "\n",
        "print(\"Running K-Means on TF-IDF matrix... (may take a bit)\")\n",
        "\n",
        "for k in k_values:\n",
        "    print(f\"\\n--- K = {k} ---\")\n",
        "\n",
        "    # KMeans (using smart initialization k-means++)\n",
        "    kmeans = KMeans(\n",
        "        n_clusters=k,\n",
        "        init=\"k-means++\",\n",
        "        max_iter=300,\n",
        "        random_state=42,\n",
        "        n_init=10\n",
        "    )\n",
        "\n",
        "    # Fit on full TF-IDF matrix\n",
        "    kmeans.fit(X_tfidf)\n",
        "\n",
        "    # Inertia (Elbow)\n",
        "    inertia = kmeans.inertia_\n",
        "    inertia_scores.append(inertia)\n",
        "\n",
        "    # Silhouette score (requires >1 cluster)\n",
        "    sil_score = silhouette_score(X_tfidf, kmeans.labels_, metric='euclidean')\n",
        "    silhouette_scores.append(sil_score)\n",
        "\n",
        "    print(f\"Inertia: {inertia}\")\n",
        "    print(f\"Silhouette Score: {sil_score}\")\n"
      ],
      "metadata": {
        "id": "maiFWWISfQ8i"
      },
      "id": "maiFWWISfQ8i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Dimensionality reduction for clustering***"
      ],
      "metadata": {
        "id": "aZHPgbrym66T"
      },
      "id": "aZHPgbrym66T"
    },
    {
      "cell_type": "code",
      "source": [
        "# # === Cell 5: Dimensionality reduction for clustering (TruncatedSVD) ===\n",
        "\n",
        "# from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# # We reduce dimensionality because TF-IDF has many features and is sparse.\n",
        "# # TruncatedSVD is PCA-like but works directly on sparse matrices.\n",
        "# svd = TruncatedSVD(\n",
        "#     n_components=50,   # number of latent dimensions (you can tune this)\n",
        "#     random_state=42\n",
        "# )\n",
        "\n",
        "# # Fit SVD on the TF-IDF matrix and transform it to a dense lower-dimensional space\n",
        "# X_svd = svd.fit_transform(X_tfidf)\n",
        "\n",
        "# print(\"Original TF-IDF shape :\", X_tfidf.shape)\n",
        "# print(\"Reduced SVD shape     :\", X_svd.shape)\n",
        "\n",
        "# # Sum of explained variance ratio gives an idea how much information we kept\n",
        "# explained = svd.explained_variance_ratio_.sum()\n",
        "# print(f\"Total explained variance (approx): {explained:.3f}\")\n"
      ],
      "metadata": {
        "id": "hARCTg3ilzVP"
      },
      "id": "hARCTg3ilzVP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***DBSCAN clustering on reduced space and comparison***"
      ],
      "metadata": {
        "id": "CaaQVZ42nC-r"
      },
      "id": "CaaQVZ42nC-r"
    },
    {
      "cell_type": "code",
      "source": [
        "# # === Cell 6: DBSCAN clustering on SVD-reduced data ===\n",
        "\n",
        "# from sklearn.cluster import DBSCAN\n",
        "# from sklearn.metrics import silhouette_score\n",
        "# import numpy as np\n",
        "\n",
        "# # Define DBSCAN hyperparameters\n",
        "# # eps controls neighborhood radius; min_samples controls how many neighbors define a \"dense\" region\n",
        "# dbscan = DBSCAN(\n",
        "#     eps=1.0,          # you can tune this (e.g., 0.5, 0.7, 1.2, ...)\n",
        "#     min_samples=5,    # minimum number of points to form a dense region\n",
        "#     metric=\"euclidean\",\n",
        "#     n_jobs=-1         # use all available cores for distance computations\n",
        "# )\n",
        "\n",
        "# print(\"Fitting DBSCAN on SVD-reduced data (this may take some time)...\")\n",
        "# dbscan_labels = dbscan.fit_predict(X_svd)\n",
        "\n",
        "# # Count how many points fell into each cluster (including noise = -1)\n",
        "# unique_labels, counts = np.unique(dbscan_labels, return_counts=True)\n",
        "# label_counts = dict(zip(unique_labels, counts))\n",
        "\n",
        "# print(\"\\nCluster label counts (including noise label = -1):\")\n",
        "# print(label_counts)\n",
        "\n",
        "# # Filter out noise points (-1) before computing Silhouette score\n",
        "# mask = dbscan_labels != -1\n",
        "# num_clusters = len(set(dbscan_labels[mask]))\n",
        "\n",
        "# if num_clusters < 2:\n",
        "#     # Silhouette score is not defined if there is fewer than 2 clusters\n",
        "#     print(\"\\nDBSCAN found fewer than 2 clusters (after removing noise).\")\n",
        "#     print(\"Silhouette score is not defined in this case.\")\n",
        "# else:\n",
        "#     # Silhouette score on the non-noise points only\n",
        "#     dbscan_sil = silhouette_score(X_svd[mask], dbscan_labels[mask])\n",
        "#     print(f\"\\nDBSCAN Silhouette Score (on non-noise points): {dbscan_sil:.4f}\")\n",
        "#     print(\"\\nYou can compare this value to the Silhouette scores you got from K-Means.\")\n"
      ],
      "metadata": {
        "id": "U7aVlLXJnE4y"
      },
      "id": "U7aVlLXJnE4y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***==========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================***"
      ],
      "metadata": {
        "id": "AIYAEt4gYCnL"
      },
      "id": "AIYAEt4gYCnL"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}