{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***תרגיל 4   של הפרוייקט***"
      ],
      "metadata": {
        "id": "BWDmLmxFJX1B"
      },
      "id": "BWDmLmxFJX1B"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***חלק א***"
      ],
      "metadata": {
        "id": "f_aW02ESJLOo"
      },
      "id": "f_aW02ESJLOo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Cell 1: Load data and basic inspection***\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5gm7encWEu_u"
      },
      "id": "5gm7encWEu_u"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the filtered dataset from disk (Colab path)\n",
        "filtered_df = pd.read_csv(\"/content/train-filtered_question_level.csv\")\n",
        "\n",
        "# Remove duplicate questions to avoid biasing the model with repeated texts\n",
        "filtered_df = filtered_df.drop_duplicates(subset=[\"question\"], keep=\"first\")\n",
        "\n",
        "# Sanity check: show columns and first rows to verify the structure\n",
        "print(\"Columns in DataFrame:\")\n",
        "print(filtered_df.columns)\n",
        "\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(filtered_df.head())\n",
        "\n",
        "# Show global label distribution for 'level' (if exists), to understand dataset balance\n",
        "if \"level\" in filtered_df.columns:\n",
        "    print(\"\\nGlobal distribution of 'level':\")\n",
        "    print(filtered_df[\"level\"].value_counts(normalize=True))\n",
        "else:\n",
        "    print(\"\\nColumn 'level' not found in DataFrame.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxymC_hSmq8K",
        "outputId": "f7aad0b7-0bba-4c9d-ffca-17d01eb06c35"
      },
      "id": "sxymC_hSmq8K",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in DataFrame:\n",
            "Index(['question', 'level'], dtype='object')\n",
            "\n",
            "First 5 rows:\n",
            "                                            question   level\n",
            "0  Which magazine was started first Arthur's Maga...  medium\n",
            "1  The Oberoi family is part of a hotel company t...  medium\n",
            "2  Musician and satirist Allie Goertz wrote a son...    hard\n",
            "3    What nationality was James Henry Miller's wife?  medium\n",
            "4  Cadmium Chloride is slightly soluble in this c...  medium\n",
            "\n",
            "Global distribution of 'level':\n",
            "level\n",
            "medium    0.628149\n",
            "easy      0.198688\n",
            "hard      0.173162\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Cell 2: Text preprocessing (tokenization + lemmatization)***"
      ],
      "metadata": {
        "id": "zeHPEQzQJsf6"
      },
      "id": "zeHPEQzQJsf6"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag\n",
        "\n",
        "# Download required NLTK resources (run once per runtime)\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "eng_stops = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Normalize all forms of the verb \"to be\" into a single token \"be\"\n",
        "BE_FORMS = {\"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\"}\n",
        "\n",
        "\n",
        "def get_wordnet_pos(tag: str):\n",
        "    \"\"\"\n",
        "    Map POS tag from nltk.pos_tag to a WordNet POS tag.\n",
        "    This helps the lemmatizer pick the correct base form.\n",
        "    \"\"\"\n",
        "    if tag.startswith(\"J\"):\n",
        "        return wordnet.ADJ\n",
        "    if tag.startswith(\"V\"):\n",
        "        return wordnet.VERB\n",
        "    if tag.startswith(\"N\"):\n",
        "        return wordnet.NOUN\n",
        "    if tag.startswith(\"R\"):\n",
        "        return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "\n",
        "# Regex patterns for cleaning\n",
        "# Remove URLs, emails, @handles, #hashtags\n",
        "url_email_handle_re = re.compile(r\"(https?://\\S+|www\\.\\S+|\\S+@\\S+|[@#]\\w+)\", re.IGNORECASE)\n",
        "\n",
        "# Detect any digit inside a token\n",
        "digits_re = re.compile(r\"\\d\")\n",
        "\n",
        "# For NON-numeric tokens: remove everything except [a-z] and spaces\n",
        "non_letter_re = re.compile(r\"[^a-z ]+\")\n",
        "\n",
        "\n",
        "def process_text_value(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Full preprocessing for a single text value:\n",
        "    - Remove URLs, emails, and @handles/#hashtags\n",
        "    - Tokenize\n",
        "    - POS tagging\n",
        "    - Lemmatization with POS\n",
        "    - Normalize all 'be' verb forms to 'be'\n",
        "    - Any token that contains at least one digit -> '_number' (entire token)\n",
        "    - For other tokens: strip punctuation/non-letters, keep only [a-z] and spaces\n",
        "    - Finally, any token that still contains the substring 'number' is collapsed to '_number'\n",
        "    - (Optional) Remove stopwords [currently commented out]\n",
        "    - Lowercase\n",
        "    Returns a cleaned string with space-separated tokens.\n",
        "    \"\"\"\n",
        "    # Safely handle missing or non-string values\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove URLs, emails, handles, hashtags\n",
        "    t = url_email_handle_re.sub(\" \", text)\n",
        "\n",
        "    # Tokenize and POS-tag on original (cleaned) text\n",
        "    tokens = word_tokenize(t)\n",
        "    tagged = pos_tag(tokens)\n",
        "\n",
        "    lemmas = []\n",
        "    for tok, pos in tagged:\n",
        "        # Normalize 'be' forms early to reduce sparsity\n",
        "        if tok.lower() in BE_FORMS:\n",
        "            lemmas.append(\"be\")\n",
        "            continue\n",
        "\n",
        "        # Map POS tag to WordNet POS tag and lemmatize\n",
        "        wn_pos = get_wordnet_pos(pos)\n",
        "        lemma = lemmatizer.lemmatize(tok, wn_pos)\n",
        "        lemmas.append(lemma)\n",
        "\n",
        "    # Lowercase all tokens\n",
        "    lemmas = [w.lower() for w in lemmas]\n",
        "\n",
        "    intermediate = []\n",
        "    for w in lemmas:\n",
        "        # If the token contains ANY digit, replace the entire token with '_number'\n",
        "        if digits_re.search(w):\n",
        "            intermediate.append(\"_number\")\n",
        "            continue\n",
        "\n",
        "        # For non-numeric tokens: remove punctuation and non-letters\n",
        "        w2 = non_letter_re.sub(\" \", w).strip()\n",
        "        if not w2:\n",
        "            # Skip tokens that became empty after cleaning\n",
        "            continue\n",
        "\n",
        "        # If cleaning produced multiple parts (e.g. \"word-word\" -> \"word word\")\n",
        "        for part in w2.split():\n",
        "            if not part:\n",
        "                continue\n",
        "            intermediate.append(part)\n",
        "\n",
        "    # Final pass: collapse any token that still contains 'number' into '_number'\n",
        "    # This guarantees we do not get '_numbera', '_numberkm', etc.\n",
        "    clean_lemmas = []\n",
        "    for w in intermediate:\n",
        "        if \"number\" in w:\n",
        "            clean_lemmas.append(\"_number\")\n",
        "        else:\n",
        "            clean_lemmas.append(w)\n",
        "\n",
        "    # If you want to remove stopwords, uncomment the next line\n",
        "    # clean_lemmas = [w for w in clean_lemmas if w not in eng_stops]\n",
        "\n",
        "    # Join tokens back into a single cleaned string\n",
        "    return \" \".join(clean_lemmas)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toAKXIEoMcY3",
        "outputId": "d6d6e944-b9d1-418f-f6a2-f200b8eeda3b"
      },
      "id": "toAKXIEoMcY3",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Cell 3: Apply preprocessing to all questions***"
      ],
      "metadata": {
        "id": "qwOP_ovgRKwO"
      },
      "id": "qwOP_ovgRKwO"
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the 'question' column exists before applying preprocessing\n",
        "if \"question\" not in filtered_df.columns:\n",
        "    raise KeyError(\"The DataFrame does not contain a 'question' column.\")\n",
        "\n",
        "# Apply the preprocessing function to every question in the dataset\n",
        "# This creates a new column 'question_clean' that contains the normalized text\n",
        "filtered_df[\"clean_text\"] = filtered_df[\"question\"].apply(process_text_value)\n",
        "\n",
        "# Inspect a few examples to verify that preprocessing works as expected\n",
        "print(\"Original vs. cleaned examples:\\n\")\n",
        "for i in range(5):\n",
        "    print(f\"--- Example {i+1} ---\")\n",
        "    print(\"Original :\", filtered_df.loc[filtered_df.index[i], \"question\"])\n",
        "    print(\"Cleaned  :\", filtered_df.loc[filtered_df.index[i], \"clean_text\"])\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPjsviBsQxIg",
        "outputId": "b080143a-4812-451c-fe68-646adccb83ae"
      },
      "id": "VPjsviBsQxIg",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original vs. cleaned examples:\n",
            "\n",
            "--- Example 1 ---\n",
            "Original : Which magazine was started first Arthur's Magazine or First for Women?\n",
            "Cleaned  : which magazine be start first arthur s magazine or first for women\n",
            "\n",
            "--- Example 2 ---\n",
            "Original : The Oberoi family is part of a hotel company that has a head office in what city?\n",
            "Cleaned  : the oberoi family be part of a hotel company that have a head office in what city\n",
            "\n",
            "--- Example 3 ---\n",
            "Original : Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
            "Cleaned  : musician and satirist allie goertz write a song about the the simpsons character milhouse who matt groening name after who\n",
            "\n",
            "--- Example 4 ---\n",
            "Original :  What nationality was James Henry Miller's wife?\n",
            "Cleaned  : what nationality be james henry miller s wife\n",
            "\n",
            "--- Example 5 ---\n",
            "Original : Cadmium Chloride is slightly soluble in this chemical, it is also called what?\n",
            "Cleaned  : cadmium chloride be slightly soluble in this chemical it be also call what\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Cell 4: TF-IDF vectorization of the preprocessed questions***"
      ],
      "metadata": {
        "id": "ovO3e28_RrXX"
      },
      "id": "ovO3e28_RrXX"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Safety check: make sure 'clean_text' exists\n",
        "if \"clean_text\" not in filtered_df.columns:\n",
        "    raise KeyError(\"The DataFrame does not contain a 'clean_text' column. \"\n",
        "                   \"Run the preprocessing cell first.\")\n",
        "\n",
        "# Define a TF-IDF vectorizer\n",
        "# max_features limits vocabulary size to the most frequent terms\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=10000,   # limit vocabulary size (you can tune this later)\n",
        "    ngram_range=(1, 1),   # unigrams only\n",
        ")\n",
        "\n",
        "# Fit TF-IDF on the entire cleaned corpus and transform it to a sparse matrix\n",
        "# Each row = one question, each column = one term from the vocabulary\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(filtered_df[\"clean_text\"])\n",
        "\n",
        "print(\"TF-IDF matrix shape (n_samples, n_features):\", X_tfidf.shape)\n",
        "print(\"(Num of documents, max_features)\")\n",
        "\n",
        "# Optional: extract labels if you need them later for supervised models / evaluation\n",
        "if \"level\" in filtered_df.columns:\n",
        "    y = filtered_df[\"level\"].values\n",
        "    print(\"Labels vector shape:\", y.shape)\n",
        "else:\n",
        "    y = None\n",
        "    print(\"No 'level' column found. y is set to None.\")\n",
        "\n",
        "# Show a small sample of feature names for sanity check\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "print(\"\\nVocabulary size (len(feature_names)):\", len(feature_names))\n",
        "print(\"First 30 features:\\n\", feature_names[:60])\n"
      ],
      "metadata": {
        "id": "tqDPZN-mRvtC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ad0174b-85c6-498f-890d-0f881d015d73"
      },
      "id": "tqDPZN-mRvtC",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF matrix shape (n_samples, n_features): (90418, 10000)\n",
            "(Num of documents, max_features)\n",
            "Labels vector shape: (90418,)\n",
            "\n",
            "Vocabulary size (len(feature_names)): 10000\n",
            "First 30 features:\n",
            " ['_number' 'aaa' 'aaron' 'ab' 'abandon' 'abashidze' 'abba' 'abbey' 'abbot'\n",
            " 'abbott' 'abbreviate' 'abbreviation' 'abc' 'abdication' 'abduct' 'abdul'\n",
            " 'abe' 'abel' 'aberdeen' 'abigail' 'ability' 'able' 'aboard'\n",
            " 'abolitionist' 'aboriginal' 'about' 'above' 'abraham' 'abrams' 'absent'\n",
            " 'absorb' 'abstract' 'abu' 'abuse' 'ac' 'academic' 'academy' 'accept'\n",
            " 'access' 'accessible' 'accessory' 'accident' 'acclaim' 'acclaimed'\n",
            " 'accompany' 'accomplished' 'accomplishment' 'accord' 'according'\n",
            " 'account' 'accra' 'accredit' 'accuse' 'ace' 'achieve' 'achievement'\n",
            " 'acid' 'acknowledge' 'acorn' 'acoustic']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Cell 5: Run K-Means for different K values***"
      ],
      "metadata": {
        "id": "EcDOCcPqfNof"
      },
      "id": "EcDOCcPqfNof"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Choose several K values\n",
        "k_values = [2, 3, 7, 15]\n",
        "# k_values = [2]\n",
        "inertia_scores = []\n",
        "silhouette_scores = []\n",
        "\n",
        "print(\"Running K-Means on TF-IDF matrix... (may take a bit)\")\n",
        "\n",
        "for k in k_values:\n",
        "    print(f\"\\n--- K = {k} ---\")\n",
        "\n",
        "    # KMeans (using smart initialization k-means++)\n",
        "    kmeans = KMeans(\n",
        "        n_clusters=k,\n",
        "        init=\"k-means++\",\n",
        "        max_iter=300,\n",
        "        random_state=42,\n",
        "        n_init=10\n",
        "    )\n",
        "\n",
        "    # Fit on full TF-IDF matrix\n",
        "    kmeans.fit(X_tfidf)\n",
        "\n",
        "    # Inertia (Elbow)\n",
        "    inertia = kmeans.inertia_\n",
        "    inertia_scores.append(inertia)\n",
        "\n",
        "    # Silhouette score (requires >1 cluster)\n",
        "    sil_score = silhouette_score(X_tfidf, kmeans.labels_, metric='euclidean')\n",
        "    silhouette_scores.append(sil_score)\n",
        "\n",
        "    print(f\"Inertia: {inertia}\")\n",
        "    print(f\"Silhouette Score: {sil_score}\")\n"
      ],
      "metadata": {
        "id": "maiFWWISfQ8i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aa29fad-f36b-4302-bc5e-1931a2ad6033"
      },
      "id": "maiFWWISfQ8i",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running K-Means on TF-IDF matrix... (may take a bit)\n",
            "\n",
            "--- K = 2 ---\n",
            "Inertia: 86997.84242688587\n",
            "Silhouette Score: 0.006657060134126678\n",
            "\n",
            "--- K = 3 ---\n",
            "Inertia: 86574.06021907461\n",
            "Silhouette Score: 0.004180111159384997\n",
            "\n",
            "--- K = 7 ---\n",
            "Inertia: 85277.72843629566\n",
            "Silhouette Score: 0.006383348812444887\n",
            "\n",
            "--- K = 15 ---\n",
            "Inertia: 83893.26229810694\n",
            "Silhouette Score: 0.009616516930486471\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cell 6: Dimensionality reduction for clustering (TruncatedSVD)**"
      ],
      "metadata": {
        "id": "aZHPgbrym66T"
      },
      "id": "aZHPgbrym66T"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# We reduce dimensionality because TF-IDF has many features and is sparse.\n",
        "# TruncatedSVD is PCA-like but works directly on sparse matrices.\n",
        "svd = TruncatedSVD(\n",
        "    n_components=50,   # number of latent dimensions (you can tune this)\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit SVD on the TF-IDF matrix and transform it to a dense lower-dimensional space\n",
        "X_svd = svd.fit_transform(X_tfidf)\n",
        "\n",
        "print(\"Original TF-IDF shape :\", X_tfidf.shape)\n",
        "print(\"Reduced SVD shape     :\", X_svd.shape)\n",
        "\n",
        "# Sum of explained variance ratio gives an idea how much information we kept\n",
        "explained = svd.explained_variance_ratio_.sum()\n",
        "print(f\"Total explained variance (approx): {explained:.3f}\")\n"
      ],
      "metadata": {
        "id": "hARCTg3ilzVP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "015b453d-9c05-4b3d-8963-b1fbff50a685"
      },
      "id": "hARCTg3ilzVP",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original TF-IDF shape : (90418, 10000)\n",
            "Reduced SVD shape     : (90418, 50)\n",
            "Total explained variance (approx): 0.156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Cell 7: DBSCAN clustering on reduced space and comparison***"
      ],
      "metadata": {
        "id": "CaaQVZ42nC-r"
      },
      "id": "CaaQVZ42nC-r"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# הגדרת גודל המדגם\n",
        "SAMPLE_SIZE = 20000\n",
        "\n",
        "print(f\"Original dataset size: {X_svd.shape[0]}\")\n",
        "\n",
        "# 1. דגימה (אם צריך)\n",
        "if X_svd.shape[0] > SAMPLE_SIZE:\n",
        "    print(f\"Dataset is too large. Sampling {SAMPLE_SIZE} random points...\")\n",
        "    np.random.seed(42)\n",
        "    indices = np.random.choice(X_svd.shape[0], SAMPLE_SIZE, replace=False)\n",
        "    X_subset = X_svd[indices]\n",
        "    df_subset = filtered_df.iloc[indices].copy()\n",
        "else:\n",
        "    X_subset = X_svd\n",
        "    df_subset = filtered_df.copy()\n",
        "\n",
        "print(f\"Running DBSCAN on {X_subset.shape[0]} samples...\")\n",
        "\n",
        "# 2. הרצת DBSCAN\n",
        "# הערה: אם את מקבלת רק אשכול אחד, כדאי לנסות לשנות את eps (למשל ל-0.3 או 0.7)\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5, metric='euclidean', n_jobs=-1)\n",
        "dbscan_labels = dbscan.fit_predict(X_subset)\n",
        "print(\"DBSCAN finished!\")\n",
        "\n",
        "# 3. שמירת תוצאות\n",
        "df_subset['dbscan_cluster'] = dbscan_labels\n",
        "\n",
        "# 4. ניתוח בסיסי\n",
        "unique_labels, counts = np.unique(dbscan_labels, return_counts=True)\n",
        "print(\"\\nCluster counts (Label -1 represents 'Noise'):\")\n",
        "print(dict(zip(unique_labels, counts)))\n",
        "\n",
        "# 5. חישוב Silhouette Score (בטוח)\n",
        "# בדיקה כמה אשכולות יש שאינם רעש\n",
        "n_clusters_real = len(set(dbscan_labels) - {-1})\n",
        "\n",
        "if n_clusters_real >= 2:\n",
        "    non_noise_mask = dbscan_labels != -1\n",
        "    sil = silhouette_score(X_subset[non_noise_mask], dbscan_labels[non_noise_mask])\n",
        "    print(f\"\\nDBSCAN Silhouette Score (excluding noise): {sil:.4f}\")\n",
        "else:\n",
        "    print(f\"\\nCould not calculate Silhouette Score: Found {n_clusters_real} real clusters.\")\n",
        "    print(\"Silhouette Score requires at least 2 distinct clusters.\")\n",
        "    print(\"Try adjusting 'eps' (lower it to split clusters) or 'min_samples'.\")"
      ],
      "metadata": {
        "id": "U7aVlLXJnE4y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfccf5b0-78c2-49bf-97ed-6a3af79b34a4"
      },
      "id": "U7aVlLXJnE4y",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Reducing dimensions with SVD...\n",
            "Reduced shape: (90418, 50)\n",
            "\n",
            "Step 2: Running DBSCAN...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Cell 8: Analyze DBSCAN Clusters (Top Keywords & Sample Questions)***"
      ],
      "metadata": {
        "id": "amY2JQJ69tLK"
      },
      "id": "amY2JQJ69tLK"
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 8: Semantic Analysis for DBSCAN Sample ===\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"Analyzing DBSCAN Clusters (on Sample)...\")\n",
        "\n",
        "# 1. וידוא שמשתני המדגם קיימים\n",
        "if 'df_subset' not in locals() or 'indices' not in locals():\n",
        "    # אם לא נעשתה דגימה (כי הדאטה היה קטן), נשתמש במקוריים\n",
        "    df_subset = filtered_df\n",
        "    indices = np.arange(X_tfidf.shape[0])\n",
        "\n",
        "# 2. הוספת התוויות לדאטה-פריים של המדגם (ולא למקורי!)\n",
        "df_subset['dbscan_cluster'] = dbscan_labels\n",
        "\n",
        "# 3. יצירת מטריצת TF-IDF שמתאימה רק לשורות שבמדגם\n",
        "# זה קריטי כדי שנוכל למצוא את המילים הנכונות\n",
        "X_tfidf_subset = X_tfidf[indices]\n",
        "\n",
        "unique_labels = np.unique(dbscan_labels)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# פונקציה מותאמת למדגם\n",
        "def get_top_keywords_for_subset(cluster_mask, tfidf_sub, features, n_top=10):\n",
        "    # חישוב ממוצע ה-TF-IDF עבור כל המילים באשכול הזה\n",
        "    # cluster_mask הוא בוליאני (True/False) ביחס למדגם\n",
        "    cluster_mean = np.array(tfidf_sub[cluster_mask].mean(axis=0)).flatten()\n",
        "    sorted_indices = cluster_mean.argsort()[::-1][:n_top]\n",
        "    return [features[i] for i in sorted_indices]\n",
        "\n",
        "# מעבר על כל אשכול והצגת הנתונים\n",
        "for label in unique_labels:\n",
        "    print(f\"\\n{'='*40}\")\n",
        "\n",
        "    # טיפול באשכול רעש (-1)\n",
        "    if label == -1:\n",
        "        print(f\"Cluster {label} (NOISE / OUTLIERS)\")\n",
        "        print(f\"{'='*40}\")\n",
        "        n_noise = np.sum(dbscan_labels == -1)\n",
        "        print(f\"Contains {n_noise} documents considered as noise.\")\n",
        "\n",
        "        # דוגמאות לרעש\n",
        "        print(\"\\nSample Noise Questions:\")\n",
        "        sample = df_subset[df_subset['dbscan_cluster'] == label]['question'].sample(min(5, n_noise), random_state=42).values\n",
        "        for i, q in enumerate(sample):\n",
        "            print(f\"  {i+1}. {q}\")\n",
        "        continue\n",
        "\n",
        "    # טיפול באשכולות אמיתיים\n",
        "    print(f\"Cluster {label}\")\n",
        "    print(f\"{'='*40}\")\n",
        "\n",
        "    # יצירת מסכה לאשכול הנוכחי\n",
        "    cluster_mask = (df_subset['dbscan_cluster'] == label).values\n",
        "    cluster_size = np.sum(cluster_mask)\n",
        "    print(f\"Size: {cluster_size} documents\")\n",
        "\n",
        "    # שליפת המילים המובילות (מתוך המטריצה החתוכה)\n",
        "    top_words = get_top_keywords_for_subset(cluster_mask, X_tfidf_subset, feature_names, n_top=15)\n",
        "    print(f\"Top Keywords: {', '.join(top_words)}\")\n",
        "\n",
        "    # שליפת דוגמאות למשפטים (מתוך ה-subset)\n",
        "    sample_size = min(5, cluster_size)\n",
        "    sample_questions = df_subset[df_subset['dbscan_cluster'] == label]['question'].sample(sample_size, random_state=42).values\n",
        "\n",
        "    print(\"\\nSample Questions:\")\n",
        "    for i, q in enumerate(sample_questions):\n",
        "        print(f\"  {i+1}. {q}\")"
      ],
      "metadata": {
        "id": "vgP9kghQ8hcl"
      },
      "id": "vgP9kghQ8hcl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "b8tdge-A-PAZ"
      },
      "id": "b8tdge-A-PAZ"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CEvVfVNW-Pit"
      },
      "id": "CEvVfVNW-Pit",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}