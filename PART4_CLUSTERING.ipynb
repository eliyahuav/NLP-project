{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***תרגיל 4   של הפרוייקט***"
      ],
      "metadata": {
        "id": "BWDmLmxFJX1B"
      },
      "id": "BWDmLmxFJX1B"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***א-1***"
      ],
      "metadata": {
        "id": "f_aW02ESJLOo"
      },
      "id": "f_aW02ESJLOo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Load data and basic inspection***\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5gm7encWEu_u"
      },
      "id": "5gm7encWEu_u"
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 1: Load data and basic inspection ===\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the filtered dataset from disk (Colab path)\n",
        "filtered_df = pd.read_csv(\"/content/train-filtered_question_level.csv\")\n",
        "\n",
        "# Remove duplicate questions to avoid biasing the model with repeated texts\n",
        "filtered_df = filtered_df.drop_duplicates(subset=[\"question\"], keep=\"first\")\n",
        "\n",
        "# Sanity check: show columns and first rows to verify the structure\n",
        "print(\"Columns in DataFrame:\")\n",
        "print(filtered_df.columns)\n",
        "\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(filtered_df.head())\n",
        "\n",
        "# Show global label distribution for 'level' (if exists), to understand dataset balance\n",
        "if \"level\" in filtered_df.columns:\n",
        "    print(\"\\nGlobal distribution of 'level':\")\n",
        "    print(filtered_df[\"level\"].value_counts(normalize=True))\n",
        "else:\n",
        "    print(\"\\nColumn 'level' not found in DataFrame.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxymC_hSmq8K",
        "outputId": "9a9e3505-160e-48aa-86c6-1ad47972f946"
      },
      "id": "sxymC_hSmq8K",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in DataFrame:\n",
            "Index(['question', 'level'], dtype='object')\n",
            "\n",
            "First 5 rows:\n",
            "                                            question   level\n",
            "0  Which magazine was started first Arthur's Maga...  medium\n",
            "1  The Oberoi family is part of a hotel company t...  medium\n",
            "2  Musician and satirist Allie Goertz wrote a son...    hard\n",
            "3    What nationality was James Henry Miller's wife?  medium\n",
            "4  Cadmium Chloride is slightly soluble in this c...  medium\n",
            "\n",
            "Global distribution of 'level':\n",
            "level\n",
            "medium    0.628149\n",
            "easy      0.198688\n",
            "hard      0.173162\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***חלוקה מאוזנת ל־train / validation / test (עם stratify)***"
      ],
      "metadata": {
        "id": "2x9zwKKfpeX-"
      },
      "id": "2x9zwKKfpeX-"
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define split proportions\n",
        "# TEST_SIZE = 0.15      # 15% of total data for test\n",
        "# VAL_SIZE = 0.15       # 15% of total data for validation\n",
        "# RANDOM_STATE = 42     # For reproducibility\n",
        "\n",
        "# # Compute validation size relative to the remaining data after test split\n",
        "# val_size_relative = VAL_SIZE / (1 - TEST_SIZE)  # e.g., 0.15 / 0.85\n",
        "\n",
        "# print(\"Relative validation size (from train_val):\", val_size_relative)\n",
        "\n",
        "# # Step 1: Split into train_val and test with stratification on 'level'\n",
        "# train_val_df, test_df = train_test_split(\n",
        "#     filtered_df,\n",
        "#     test_size=TEST_SIZE,\n",
        "#     stratify=filtered_df[\"level\"],\n",
        "#     random_state=RANDOM_STATE\n",
        "# )\n",
        "\n",
        "# # Step 2: Split train_val into train and validation with stratification on 'level'\n",
        "# train_df, val_df = train_test_split(\n",
        "#     train_val_df,\n",
        "#     test_size=val_size_relative,\n",
        "#     stratify=train_val_df[\"level\"],\n",
        "#     random_state=RANDOM_STATE\n",
        "# )\n",
        "\n",
        "# print(\"Finished stratified split into train / validation / test.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m176qvlxpdZJ",
        "outputId": "2dc51e91-dbc9-4b88-90e2-c0f4eca93f2a"
      },
      "id": "m176qvlxpdZJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relative validation size (from train_val): 0.17647058823529413\n",
            "Finished stratified split into train / validation / test.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***בדיקה שהחלוקה מאוזנת (stratified) ושיש לנו את היחסים הרצויים***"
      ],
      "metadata": {
        "id": "rpBACoIoplWC"
      },
      "id": "rpBACoIoplWC"
    },
    {
      "cell_type": "code",
      "source": [
        "# def print_split_info(df, name):\n",
        "#     print(f\"\\n{name}:\")\n",
        "#     print(\"Number of rows:\", len(df))\n",
        "#     print(\"Label distribution for 'level':\")\n",
        "#     print(df[\"level\"].value_counts(normalize=True))\n",
        "\n",
        "# print(\"Total rows in original filtered_df:\", len(filtered_df))\n",
        "\n",
        "# print_split_info(train_df, \"Train set\")\n",
        "# print_split_info(val_df, \"Validation set\")\n",
        "# print_split_info(test_df, \"Test set\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pev31-BLplxP",
        "outputId": "186f29b0-3c21-4f02-d027-428aa2062b7a"
      },
      "id": "pev31-BLplxP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows in original filtered_df: 90418\n",
            "\n",
            "Train set:\n",
            "Number of rows: 63292\n",
            "Label distribution for 'level':\n",
            "level\n",
            "medium    0.628152\n",
            "easy      0.198682\n",
            "hard      0.173166\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Validation set:\n",
            "Number of rows: 13563\n",
            "Label distribution for 'level':\n",
            "level\n",
            "medium    0.628180\n",
            "easy      0.198702\n",
            "hard      0.173118\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Test set:\n",
            "Number of rows: 13563\n",
            "Label distribution for 'level':\n",
            "level\n",
            "medium    0.628106\n",
            "easy      0.198702\n",
            "hard      0.173192\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KtfJcpMAJofI"
      },
      "id": "KtfJcpMAJofI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Text preprocessing (tokenization + lemmatization)***"
      ],
      "metadata": {
        "id": "zeHPEQzQJsf6"
      },
      "id": "zeHPEQzQJsf6"
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 2 (HARD FIX): NLTK setup and robust preprocessing ===\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag\n",
        "\n",
        "# Download required NLTK resources (run once per runtime)\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "eng_stops = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Normalize all forms of the verb \"to be\" into a single token \"be\"\n",
        "BE_FORMS = {\"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\"}\n",
        "\n",
        "\n",
        "def get_wordnet_pos(tag: str):\n",
        "    \"\"\"\n",
        "    Map POS tag from nltk.pos_tag to a WordNet POS tag.\n",
        "    This helps the lemmatizer pick the correct base form.\n",
        "    \"\"\"\n",
        "    if tag.startswith(\"J\"):\n",
        "        return wordnet.ADJ\n",
        "    if tag.startswith(\"V\"):\n",
        "        return wordnet.VERB\n",
        "    if tag.startswith(\"N\"):\n",
        "        return wordnet.NOUN\n",
        "    if tag.startswith(\"R\"):\n",
        "        return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "\n",
        "# Regex patterns for cleaning\n",
        "# Remove URLs, emails, @handles, #hashtags\n",
        "url_email_handle_re = re.compile(r\"(https?://\\S+|www\\.\\S+|\\S+@\\S+|[@#]\\w+)\", re.IGNORECASE)\n",
        "\n",
        "# Detect any digit inside a token\n",
        "digits_re = re.compile(r\"\\d\")\n",
        "\n",
        "# For NON-numeric tokens: remove everything except [a-z] and spaces\n",
        "non_letter_re = re.compile(r\"[^a-z ]+\")\n",
        "\n",
        "\n",
        "def process_text_value(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Full preprocessing for a single text value:\n",
        "    - Remove URLs, emails, and @handles/#hashtags\n",
        "    - Tokenize\n",
        "    - POS tagging\n",
        "    - Lemmatization with POS\n",
        "    - Normalize all 'be' verb forms to 'be'\n",
        "    - Any token that contains at least one digit -> '_number' (entire token)\n",
        "    - For other tokens: strip punctuation/non-letters, keep only [a-z] and spaces\n",
        "    - Finally, any token that still contains the substring 'number' is collapsed to '_number'\n",
        "    - (Optional) Remove stopwords [currently commented out]\n",
        "    - Lowercase\n",
        "    Returns a cleaned string with space-separated tokens.\n",
        "    \"\"\"\n",
        "    # Safely handle missing or non-string values\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove URLs, emails, handles, hashtags\n",
        "    t = url_email_handle_re.sub(\" \", text)\n",
        "\n",
        "    # Tokenize and POS-tag on original (cleaned) text\n",
        "    tokens = word_tokenize(t)\n",
        "    tagged = pos_tag(tokens)\n",
        "\n",
        "    lemmas = []\n",
        "    for tok, pos in tagged:\n",
        "        # Normalize 'be' forms early to reduce sparsity\n",
        "        if tok.lower() in BE_FORMS:\n",
        "            lemmas.append(\"be\")\n",
        "            continue\n",
        "\n",
        "        # Map POS tag to WordNet POS tag and lemmatize\n",
        "        wn_pos = get_wordnet_pos(pos)\n",
        "        lemma = lemmatizer.lemmatize(tok, wn_pos)\n",
        "        lemmas.append(lemma)\n",
        "\n",
        "    # Lowercase all tokens\n",
        "    lemmas = [w.lower() for w in lemmas]\n",
        "\n",
        "    intermediate = []\n",
        "    for w in lemmas:\n",
        "        # If the token contains ANY digit, replace the entire token with '_number'\n",
        "        if digits_re.search(w):\n",
        "            intermediate.append(\"_number\")\n",
        "            continue\n",
        "\n",
        "        # For non-numeric tokens: remove punctuation and non-letters\n",
        "        w2 = non_letter_re.sub(\" \", w).strip()\n",
        "        if not w2:\n",
        "            # Skip tokens that became empty after cleaning\n",
        "            continue\n",
        "\n",
        "        # If cleaning produced multiple parts (e.g. \"word-word\" -> \"word word\")\n",
        "        for part in w2.split():\n",
        "            if not part:\n",
        "                continue\n",
        "            intermediate.append(part)\n",
        "\n",
        "    # Final pass: collapse any token that still contains 'number' into '_number'\n",
        "    # This guarantees we do not get '_numbera', '_numberkm', etc.\n",
        "    clean_lemmas = []\n",
        "    for w in intermediate:\n",
        "        if \"number\" in w:\n",
        "            clean_lemmas.append(\"_number\")\n",
        "        else:\n",
        "            clean_lemmas.append(w)\n",
        "\n",
        "    # If you want to remove stopwords, uncomment the next line\n",
        "    # clean_lemmas = [w for w in clean_lemmas if w not in eng_stops]\n",
        "\n",
        "    # Join tokens back into a single cleaned string\n",
        "    return \" \".join(clean_lemmas)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toAKXIEoMcY3",
        "outputId": "c3711278-82e2-46d3-c7df-54a316a13eb6"
      },
      "id": "toAKXIEoMcY3",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Apply preprocessing to all questions***"
      ],
      "metadata": {
        "id": "qwOP_ovgRKwO"
      },
      "id": "qwOP_ovgRKwO"
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 3: Apply preprocessing to all questions ===\n",
        "\n",
        "# Ensure the 'question' column exists before applying preprocessing\n",
        "if \"question\" not in filtered_df.columns:\n",
        "    raise KeyError(\"The DataFrame does not contain a 'question' column.\")\n",
        "\n",
        "# Apply the preprocessing function to every question in the dataset\n",
        "# This creates a new column 'question_clean' that contains the normalized text\n",
        "filtered_df[\"question_clean\"] = filtered_df[\"question\"].apply(process_text_value)\n",
        "\n",
        "# Inspect a few examples to verify that preprocessing works as expected\n",
        "print(\"Original vs. cleaned examples:\\n\")\n",
        "for i in range(5):\n",
        "    print(f\"--- Example {i+1} ---\")\n",
        "    print(\"Original :\", filtered_df.loc[filtered_df.index[i], \"question\"])\n",
        "    print(\"Cleaned  :\", filtered_df.loc[filtered_df.index[i], \"question_clean\"])\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPjsviBsQxIg",
        "outputId": "cb35e349-8a26-449c-9767-cd6494f9cb06"
      },
      "id": "VPjsviBsQxIg",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original vs. cleaned examples:\n",
            "\n",
            "--- Example 1 ---\n",
            "Original : Which magazine was started first Arthur's Magazine or First for Women?\n",
            "Cleaned  : which magazine be start first arthur s magazine or first for women\n",
            "\n",
            "--- Example 2 ---\n",
            "Original : The Oberoi family is part of a hotel company that has a head office in what city?\n",
            "Cleaned  : the oberoi family be part of a hotel company that have a head office in what city\n",
            "\n",
            "--- Example 3 ---\n",
            "Original : Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
            "Cleaned  : musician and satirist allie goertz write a song about the the simpsons character milhouse who matt groening name after who\n",
            "\n",
            "--- Example 4 ---\n",
            "Original :  What nationality was James Henry Miller's wife?\n",
            "Cleaned  : what nationality be james henry miller s wife\n",
            "\n",
            "--- Example 5 ---\n",
            "Original : Cadmium Chloride is slightly soluble in this chemical, it is also called what?\n",
            "Cleaned  : cadmium chloride be slightly soluble in this chemical it be also call what\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***TF-IDF vectorization of the preprocessed questions***"
      ],
      "metadata": {
        "id": "ovO3e28_RrXX"
      },
      "id": "ovO3e28_RrXX"
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 4: TF-IDF vectorization for ALL questions (no train/val/test split) ===\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Safety check: make sure 'clean_text' exists\n",
        "if \"clean_text\" not in filtered_df.columns:\n",
        "    raise KeyError(\"The DataFrame does not contain a 'clean_text' column. \"\n",
        "                   \"Run the preprocessing cell first.\")\n",
        "\n",
        "# Define a TF-IDF vectorizer\n",
        "# max_features limits vocabulary size to the most frequent terms\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=10000,   # limit vocabulary size (you can tune this later)\n",
        "    ngram_range=(1, 1),   # unigrams only\n",
        ")\n",
        "\n",
        "# Fit TF-IDF on the entire cleaned corpus and transform it to a sparse matrix\n",
        "# Each row = one question, each column = one term from the vocabulary\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(filtered_df[\"clean_text\"])\n",
        "\n",
        "print(\"TF-IDF matrix shape (n_samples, n_features):\", X_tfidf.shape)\n",
        "print(\"(Num of documents, max_features)\")\n",
        "\n",
        "# Optional: extract labels if you need them later for supervised models / evaluation\n",
        "if \"level\" in filtered_df.columns:\n",
        "    y = filtered_df[\"level\"].values\n",
        "    print(\"Labels vector shape:\", y.shape)\n",
        "else:\n",
        "    y = None\n",
        "    print(\"No 'level' column found. y is set to None.\")\n",
        "\n",
        "# Show a small sample of feature names for sanity check\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "print(\"\\nVocabulary size (len(feature_names)):\", len(feature_names))\n",
        "print(\"First 30 features:\\n\", feature_names[:60])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqDPZN-mRvtC",
        "outputId": "affe5aba-9b8f-4071-d23e-469792d99884"
      },
      "id": "tqDPZN-mRvtC",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF matrix shape (n_samples, n_features): (90418, 10000)\n",
            "(Num of documents, max_features)\n",
            "Labels vector shape: (90418,)\n",
            "\n",
            "Vocabulary size (len(feature_names)): 10000\n",
            "First 30 features:\n",
            " ['_number' '_numbera' '_numberb' '_numberd' '_numberers' '_numberg'\n",
            " '_numberk' '_numberk_number' '_numberkm' '_numberm' '_numbernd'\n",
            " '_numberrd' '_numbers' '_numberst' '_numberth' '_numberx_number'\n",
            " 'a_number' 'aaa' 'aaron' 'ab' 'abandon' 'abba' 'abbey' 'abbot' 'abbott'\n",
            " 'abbreviate' 'abbreviation' 'abc' 'abdication' 'abduct' 'abdul' 'abe'\n",
            " 'abel' 'aberdeen' 'abigail' 'ability' 'able' 'aboard' 'abolitionist'\n",
            " 'aboriginal' 'about' 'above' 'abraham' 'abrams' 'absent' 'absorb'\n",
            " 'abstract' 'abu' 'abuse' 'ac' 'academic' 'academy' 'accept' 'access'\n",
            " 'accessible' 'accessory' 'accident' 'acclaim' 'acclaimed' 'accompany']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Run K-Means for different K values***"
      ],
      "metadata": {
        "id": "EcDOCcPqfNof"
      },
      "id": "EcDOCcPqfNof"
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 5: Run K-Means for several K values ===\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Choose several K values (must include 2 and 3 as required)\n",
        "# k_values = [2, 3, 5, 7, 10, 15]\n",
        "k_values = [2]\n",
        "inertia_scores = []\n",
        "silhouette_scores = []\n",
        "\n",
        "print(\"Running K-Means on TF-IDF matrix... (may take a bit)\")\n",
        "\n",
        "for k in k_values:\n",
        "    print(f\"\\n--- K = {k} ---\")\n",
        "\n",
        "    # KMeans (using smart initialization k-means++)\n",
        "    kmeans = KMeans(\n",
        "        n_clusters=k,\n",
        "        init=\"k-means++\",\n",
        "        max_iter=300,\n",
        "        random_state=42,\n",
        "        n_init=10\n",
        "    )\n",
        "\n",
        "    # Fit on full TF-IDF matrix\n",
        "    kmeans.fit(X_tfidf)\n",
        "\n",
        "    # Inertia (Elbow)\n",
        "    inertia = kmeans.inertia_\n",
        "    inertia_scores.append(inertia)\n",
        "\n",
        "    # Silhouette score (requires >1 cluster)\n",
        "    sil_score = silhouette_score(X_tfidf, kmeans.labels_, metric='euclidean')\n",
        "    silhouette_scores.append(sil_score)\n",
        "\n",
        "    print(f\"Inertia: {inertia}\")\n",
        "    print(f\"Silhouette Score: {sil_score}\")\n"
      ],
      "metadata": {
        "id": "maiFWWISfQ8i",
        "outputId": "0fa35c5e-f8fe-43c1-88d1-1075f0d5c9fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "maiFWWISfQ8i",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running K-Means on TF-IDF matrix... (may take a bit)\n",
            "\n",
            "--- K = 2 ---\n",
            "Inertia: 87018.17861386671\n",
            "Silhouette Score: 0.006595422542578891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hARCTg3ilzVP",
        "outputId": "26badc20-452b-40c7-e10d-02e4094c6279",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "id": "hARCTg3ilzVP",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running K-Means on TF-IDF matrix... (may take a bit)\n",
            "\n",
            "--- K = 3 ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1929802829.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Silhouette score (requires >1 cluster)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0msil_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msilhouette_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'euclidean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0msilhouette_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msil_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/cluster/_unsupervised.py\u001b[0m in \u001b[0;36msilhouette_score\u001b[0;34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msilhouette_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mglobal_skip_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"skip_parameter_validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mfunc_sig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/cluster/_unsupervised.py\u001b[0m in \u001b[0;36msilhouette_samples\u001b[0;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0m_silhouette_reduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_freqs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_freqs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     )\n\u001b[0;32m--> 303\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpairwise_distances_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m     \u001b[0mintra_clust_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minter_clust_dists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0mintra_clust_dists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintra_clust_dists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   2250\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2251\u001b[0m             \u001b[0mX_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2252\u001b[0;31m         \u001b[0mD_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpairwise_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2253\u001b[0m         if (X is Y or Y is None) and PAIRWISE_DISTANCE_FUNCTIONS.get(\n\u001b[1;32m   2254\u001b[0m             \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mglobal_skip_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"skip_parameter_validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mfunc_sig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, force_all_finite, ensure_all_finite, **kwds)\u001b[0m\n\u001b[1;32m   2478\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2480\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1972\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0meffective_n_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1973\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1975\u001b[0m     \u001b[0;31m# enforce a threading backend to prevent data communication overhead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mglobal_skip_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"skip_parameter_validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mfunc_sig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    386\u001b[0m             )\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_euclidean_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_norm_squared\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_norm_squared\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36m_euclidean_distances\u001b[0;34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m     \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_modify_in_place_if_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m_modify_in_place_if_numpy\u001b[0;34m(xp, func, out, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_modify_in_place_if_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_numpy_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***==========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================***"
      ],
      "metadata": {
        "id": "AIYAEt4gYCnL"
      },
      "id": "AIYAEt4gYCnL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Build document vectors from Word2Vec (TF-IDF weighted average)***"
      ],
      "metadata": {
        "id": "5ke7Pi-KUlGD"
      },
      "id": "5ke7Pi-KUlGD"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Build a dictionary: word -> IDF score, based on the TF-IDF vocabulary\n",
        "idf_scores = dict(zip(tfidf_vectorizer.get_feature_names_out(),\n",
        "                      tfidf_vectorizer.idf_))\n",
        "\n",
        "def document_vector(tokens, use_tfidf_weight=True):\n",
        "    \"\"\"\n",
        "    Compute a single document vector from word vectors.\n",
        "    By default uses TF-IDF weights as recommended.\n",
        "    - tokens: list of preprocessed, lemmatized tokens\n",
        "    - use_tfidf_weight: if True, weight each word vector by its IDF\n",
        "    \"\"\"\n",
        "    vectors = []\n",
        "    weights = []\n",
        "\n",
        "    for tok in tokens:\n",
        "        if tok in w2v_model.wv:\n",
        "            vec = w2v_model.wv[tok]\n",
        "            if use_tfidf_weight:\n",
        "                weight = idf_scores.get(tok, 1.0)\n",
        "            else:\n",
        "                weight = 1.0\n",
        "            vectors.append(vec * weight)\n",
        "            weights.append(weight)\n",
        "\n",
        "    if not vectors:\n",
        "        # If no token has a vector, return a zero vector\n",
        "        return np.zeros(w2v_model.vector_size, dtype=np.float32)\n",
        "\n",
        "    vectors = np.vstack(vectors)\n",
        "    weights = np.array(weights, dtype=np.float32)\n",
        "\n",
        "    # Weighted average: sum(w_i * v_i) / sum(w_i)\n",
        "    return vectors.sum(axis=0) / weights.sum()\n",
        "\n",
        "# Build document-level vectors for each split\n",
        "X_train_w2v = np.vstack(train_df[\"tokens\"].apply(lambda toks: document_vector(toks, use_tfidf_weight=True)))\n",
        "X_val_w2v   = np.vstack(val_df[\"tokens\"].apply(lambda toks: document_vector(toks, use_tfidf_weight=True)))\n",
        "X_test_w2v  = np.vstack(test_df[\"tokens\"].apply(lambda toks: document_vector(toks, use_tfidf_weight=True)))\n",
        "\n",
        "print(\"Word2Vec document matrices shapes:\")\n",
        "print(\"X_train_w2v:\", X_train_w2v.shape)\n",
        "print(\"X_val_w2v:  \", X_val_w2v.shape)\n",
        "print(\"X_test_w2v: \", X_test_w2v.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwRZS0yDUudb",
        "outputId": "7f38ab2a-8b80-4974-8ebf-47ed998dab44"
      },
      "id": "ZwRZS0yDUudb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec document matrices shapes:\n",
            "X_train_w2v: (63292, 100)\n",
            "X_val_w2v:   (13563, 100)\n",
            "X_test_w2v:  (13563, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***ב-1-סיווג בינארי***"
      ],
      "metadata": {
        "id": "2HJK_nb4xcES"
      },
      "id": "2HJK_nb4xcES"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Filter to binary classes (easy, hard)***"
      ],
      "metadata": {
        "id": "AdkAzZ-Q11NJ"
      },
      "id": "AdkAzZ-Q11NJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep only 'easy' and 'hard' classes\n",
        "binary_train = train_df[train_df[\"level\"].isin([\"easy\", \"hard\"])].copy()\n",
        "binary_val   = val_df[val_df[\"level\"].isin([\"easy\", \"hard\"])].copy()\n",
        "binary_test  = test_df[test_df[\"level\"].isin([\"easy\", \"hard\"])].copy()\n",
        "\n",
        "print(\"Train size:\", len(binary_train))\n",
        "print(\"Validation size:\", len(binary_val))\n",
        "print(\"Test size:\", len(binary_test))\n",
        "\n",
        "print(\"\\nTrain label distribution:\")\n",
        "print(binary_train[\"level\"].value_counts(normalize=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcjvK2Ndxcf8",
        "outputId": "c851c3f5-b2f6-4831-931a-0cb9e87c2745"
      },
      "id": "NcjvK2Ndxcf8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 23535\n",
            "Validation size: 5043\n",
            "Test size: 5044\n",
            "\n",
            "Train label distribution:\n",
            "level\n",
            "easy    0.534311\n",
            "hard    0.465689\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Encode labels (easy=0, hard=1)***"
      ],
      "metadata": {
        "id": "ZaCcDxS601Pd"
      },
      "id": "ZaCcDxS601Pd"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "y_train = le.fit_transform(binary_train[\"level\"])\n",
        "y_val   = le.transform(binary_val[\"level\"])\n",
        "y_test  = le.transform(binary_test[\"level\"])\n",
        "\n",
        "print(\"Label classes:\", le.classes_)  # ['easy' 'hard']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egO2tI2e07eI",
        "outputId": "69ade137-8d33-4ff5-aae4-d8435abb7758"
      },
      "id": "egO2tI2e07eI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label classes: ['easy' 'hard']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Build TF-IDF for the binary subsets***"
      ],
      "metadata": {
        "id": "kkibpNFb08Gx"
      },
      "id": "kkibpNFb08Gx"
    },
    {
      "cell_type": "code",
      "source": [
        "# Reuse the same TF-IDF vectorizer that was already fitted on full train_df\n",
        "X_train_tfidf_bin = tfidf_vectorizer.transform(binary_train[\"clean_text\"])\n",
        "X_val_tfidf_bin   = tfidf_vectorizer.transform(binary_val[\"clean_text\"])\n",
        "X_test_tfidf_bin  = tfidf_vectorizer.transform(binary_test[\"clean_text\"])\n",
        "\n",
        "print(\"Binary TF-IDF shapes:\")\n",
        "print(\"X_train_tfidf_bin:\", X_train_tfidf_bin.shape)\n",
        "print(\"X_val_tfidf_bin:  \", X_val_tfidf_bin.shape)\n",
        "print(\"X_test_tfidf_bin: \", X_test_tfidf_bin.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2qL3tCy08wD",
        "outputId": "de41eb05-6343-4491-c5f1-4f480f58fd70"
      },
      "id": "O2qL3tCy08wD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary TF-IDF shapes:\n",
            "X_train_tfidf_bin: (23535, 10000)\n",
            "X_val_tfidf_bin:   (5043, 10000)\n",
            "X_test_tfidf_bin:  (5044, 10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Build Word2Vec document vectors for the binary subsets***"
      ],
      "metadata": {
        "id": "6JT2DoNx23au"
      },
      "id": "6JT2DoNx23au"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming you already have w2v_model and document_vector() defined\n",
        "\n",
        "X_train_w2v_bin = np.vstack(\n",
        "    binary_train[\"tokens\"].apply(lambda toks: document_vector(toks, use_tfidf_weight=True))\n",
        ")\n",
        "X_val_w2v_bin = np.vstack(\n",
        "    binary_val[\"tokens\"].apply(lambda toks: document_vector(toks, use_tfidf_weight=True))\n",
        ")\n",
        "X_test_w2v_bin = np.vstack(\n",
        "    binary_test[\"tokens\"].apply(lambda toks: document_vector(toks, use_tfidf_weight=True))\n",
        ")\n",
        "\n",
        "print(\"Binary Word2Vec shapes:\")\n",
        "print(\"X_train_w2v_bin:\", X_train_w2v_bin.shape)\n",
        "print(\"X_val_w2v_bin:  \", X_val_w2v_bin.shape)\n",
        "print(\"X_test_w2v_bin: \", X_test_w2v_bin.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdtdkzqG21nt",
        "outputId": "e4594114-e9ce-4ffb-a7a4-31712d8f6b8b"
      },
      "id": "YdtdkzqG21nt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary Word2Vec shapes:\n",
            "X_train_w2v_bin: (23535, 100)\n",
            "X_val_w2v_bin:   (5043, 100)\n",
            "X_test_w2v_bin:  (5044, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Utility: evaluate model***"
      ],
      "metadata": {
        "id": "wgrwWdZi1LvQ"
      },
      "id": "wgrwWdZi1LvQ"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "def evaluate_model(model_name, representation_name, y_true, y_pred):\n",
        "    print(f\"\\n=== {model_name} + {representation_name} ===\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "    print(\"F1 Score:\", f1_score(y_true, y_pred))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n"
      ],
      "metadata": {
        "id": "9DI7lMjR1Q07"
      },
      "id": "9DI7lMjR1Q07",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***TF-IDF + Naive Bayes***"
      ],
      "metadata": {
        "id": "ycZgqE4Q1ZIl"
      },
      "id": "ycZgqE4Q1ZIl"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "nb_tfidf = MultinomialNB()\n",
        "nb_tfidf.fit(X_train_tfidf_bin, y_train)\n",
        "\n",
        "pred_val = nb_tfidf.predict(X_val_tfidf_bin)\n",
        "\n",
        "evaluate_model(\"Naive Bayes\", \"TF-IDF\", y_val, pred_val)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WN4QUQm11dqL",
        "outputId": "d474aec7-8382-4433-eefb-3ce8784a0f64"
      },
      "id": "WN4QUQm11dqL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Naive Bayes + TF-IDF ===\n",
            "Accuracy: 0.6365258774538964\n",
            "F1 Score: 0.5534713763702801\n",
            "Confusion Matrix:\n",
            " [[2074  621]\n",
            " [1212 1136]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***TF-IDF + Logistic Regression***"
      ],
      "metadata": {
        "id": "W5XLMNuQ1eFx"
      },
      "id": "W5XLMNuQ1eFx"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr_tfidf = LogisticRegression(max_iter=2000)\n",
        "lr_tfidf.fit(X_train_tfidf_bin, y_train)\n",
        "\n",
        "pred_val = lr_tfidf.predict(X_val_tfidf_bin)\n",
        "\n",
        "evaluate_model(\"Logistic Regression\", \"TF-IDF\", y_val, pred_val)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lSuSL5Q1erE",
        "outputId": "0e156c6a-e829-4811-d724-6a8abb7d4048"
      },
      "id": "6lSuSL5Q1erE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Logistic Regression + TF-IDF ===\n",
            "Accuracy: 0.7083085465000991\n",
            "F1 Score: 0.6860192102454642\n",
            "Confusion Matrix:\n",
            " [[1965  730]\n",
            " [ 741 1607]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Word2Vec + Naive Bayes***"
      ],
      "metadata": {
        "id": "BDQ-NH5I1fi_"
      },
      "id": "BDQ-NH5I1fi_"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "nb_w2v = GaussianNB()\n",
        "nb_w2v.fit(X_train_w2v_bin, y_train)\n",
        "\n",
        "pred_val = nb_w2v.predict(X_val_w2v_bin)\n",
        "\n",
        "evaluate_model(\"Naive Bayes (Gaussian)\", \"Word2Vec\", y_val, pred_val)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHl9DRK-1p3W",
        "outputId": "f23fe0dc-9326-4488-ffcf-aa7704021b73"
      },
      "id": "UHl9DRK-1p3W",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Naive Bayes (Gaussian) + Word2Vec ===\n",
            "Accuracy: 0.5847709696609161\n",
            "F1 Score: 0.4765\n",
            "Confusion Matrix:\n",
            " [[1996  699]\n",
            " [1395  953]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Word2Vec + Logistic Regression***"
      ],
      "metadata": {
        "id": "iMBIEtHy1roq"
      },
      "id": "iMBIEtHy1roq"
    },
    {
      "cell_type": "code",
      "source": [
        "lr_w2v = LogisticRegression(max_iter=2000)\n",
        "lr_w2v.fit(X_train_w2v_bin, y_train)\n",
        "\n",
        "pred_val = lr_w2v.predict(X_val_w2v_bin)\n",
        "\n",
        "evaluate_model(\"Logistic Regression\", \"Word2Vec\", y_val, pred_val)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FT8d4Wnv1ul2",
        "outputId": "a01cd4a1-5cf2-4994-a493-aaf941e58731"
      },
      "id": "FT8d4Wnv1ul2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Logistic Regression + Word2Vec ===\n",
            "Accuracy: 0.6603212373587151\n",
            "F1 Score: 0.6046618970690053\n",
            "Confusion Matrix:\n",
            " [[2020  675]\n",
            " [1038 1310]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***ב-1- סיווג רב מחלקתי כלומר 3***"
      ],
      "metadata": {
        "id": "faKA2iZN5NsH"
      },
      "id": "faKA2iZN5NsH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Build multi-class subsets (easy, medium, hard)***"
      ],
      "metadata": {
        "id": "dpfRTFW25cZ5"
      },
      "id": "dpfRTFW25cZ5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep only the three target classes\n",
        "target_levels = [\"easy\", \"medium\", \"hard\"]\n",
        "\n",
        "multi_train = train_df[train_df[\"level\"].isin(target_levels)].copy()\n",
        "multi_val   = val_df[val_df[\"level\"].isin(target_levels)].copy()\n",
        "multi_test  = test_df[test_df[\"level\"].isin(target_levels)].copy()\n",
        "\n",
        "print(\"Train size:\", len(multi_train))\n",
        "print(\"Validation size:\", len(multi_val))\n",
        "print(\"Test size:\", len(multi_test))\n",
        "\n",
        "print(\"\\nTrain label distribution:\")\n",
        "print(multi_train[\"level\"].value_counts(normalize=True))\n",
        "\n",
        "print(\"\\nUnique levels in all splits:\")\n",
        "print(\"Train:\", multi_train[\"level\"].unique())\n",
        "print(\"Val:  \", multi_val[\"level\"].unique())\n",
        "print(\"Test: \", multi_test[\"level\"].unique())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRO-qxBV5c0J",
        "outputId": "a826f25e-5266-434e-ccc7-84f430f9fe2b"
      },
      "id": "GRO-qxBV5c0J",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 63292\n",
            "Validation size: 13563\n",
            "Test size: 13563\n",
            "\n",
            "Train label distribution:\n",
            "level\n",
            "medium    0.628152\n",
            "easy      0.198682\n",
            "hard      0.173166\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Unique levels in all splits:\n",
            "Train: ['medium' 'hard' 'easy']\n",
            "Val:   ['hard' 'medium' 'easy']\n",
            "Test:  ['medium' 'easy' 'hard']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Encode labels (3 classes)***"
      ],
      "metadata": {
        "id": "5hSpOfJ95ocX"
      },
      "id": "5hSpOfJ95ocX"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le_multi = LabelEncoder()\n",
        "\n",
        "y_train_multi = le_multi.fit_transform(multi_train[\"level\"])\n",
        "y_val_multi   = le_multi.transform(multi_val[\"level\"])\n",
        "y_test_multi  = le_multi.transform(multi_test[\"level\"])\n",
        "\n",
        "print(\"Label classes (order):\", le_multi.classes_)  # expects ['easy' 'hard' 'medium'] or similar\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9v-DDlP5owa",
        "outputId": "faeaab18-2296-460d-ac4f-23a327d164d7"
      },
      "id": "u9v-DDlP5owa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label classes (order): ['easy' 'hard' 'medium']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***TF-IDF representation for multi-class***"
      ],
      "metadata": {
        "id": "ukMMIGQQ5uQv"
      },
      "id": "ukMMIGQQ5uQv"
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform clean_text into TF-IDF vectors using the existing fitted vectorizer\n",
        "X_train_tfidf_multi = tfidf_vectorizer.transform(multi_train[\"clean_text\"])\n",
        "X_val_tfidf_multi   = tfidf_vectorizer.transform(multi_val[\"clean_text\"])\n",
        "X_test_tfidf_multi  = tfidf_vectorizer.transform(multi_test[\"clean_text\"])\n",
        "\n",
        "print(\"TF-IDF shapes (multi-class):\")\n",
        "print(\"X_train_tfidf_multi:\", X_train_tfidf_multi.shape)\n",
        "print(\"X_val_tfidf_multi:  \", X_val_tfidf_multi.shape)\n",
        "print(\"X_test_tfidf_multi: \", X_test_tfidf_multi.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BykdoaMe5uk1",
        "outputId": "f14cd91c-d30b-4baf-e382-71a31b2bb9fe"
      },
      "id": "BykdoaMe5uk1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF shapes (multi-class):\n",
            "X_train_tfidf_multi: (63292, 10000)\n",
            "X_val_tfidf_multi:   (13563, 10000)\n",
            "X_test_tfidf_multi:  (13563, 10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Word2Vec document vectors for multi-class***"
      ],
      "metadata": {
        "id": "2tdJzxda527C"
      },
      "id": "2tdJzxda527C"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Build document-level vectors using the existing Word2Vec model\n",
        "X_train_w2v_multi = np.vstack(\n",
        "    multi_train[\"tokens\"].apply(lambda toks: document_vector(toks, use_tfidf_weight=True))\n",
        ")\n",
        "X_val_w2v_multi = np.vstack(\n",
        "    multi_val[\"tokens\"].apply(lambda toks: document_vector(toks, use_tfidf_weight=True))\n",
        ")\n",
        "X_test_w2v_multi = np.vstack(\n",
        "    multi_test[\"tokens\"].apply(lambda toks: document_vector(toks, use_tfidf_weight=True))\n",
        ")\n",
        "\n",
        "print(\"Word2Vec document shapes (multi-class):\")\n",
        "print(\"X_train_w2v_multi:\", X_train_w2v_multi.shape)\n",
        "print(\"X_val_w2v_multi:  \", X_val_w2v_multi.shape)\n",
        "print(\"X_test_w2v_multi: \", X_test_w2v_multi.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pg9D9Qy553Py",
        "outputId": "f9b7bc46-dcd4-4b66-cd3f-e56a44bfc379"
      },
      "id": "pg9D9Qy553Py",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec document shapes (multi-class):\n",
            "X_train_w2v_multi: (63292, 100)\n",
            "X_val_w2v_multi:   (13563, 100)\n",
            "X_test_w2v_multi:  (13563, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Evaluation helper (Accuracy, macro-F1, confusion matrix)***"
      ],
      "metadata": {
        "id": "0aNI705N5-dR"
      },
      "id": "0aNI705N5-dR"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "def evaluate_multi(model_name, representation_name, y_true, y_pred, label_encoder):\n",
        "    \"\"\"\n",
        "    Print accuracy, macro F1, and confusion matrix for a multi-class setting.\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== {model_name} + {representation_name} ===\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "    print(\"Macro F1:\", f1_score(y_true, y_pred, average=\"macro\"))\n",
        "    print(\"\\nConfusion Matrix (rows=true, cols=pred):\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(\"Label order:\", label_encoder.classes_)\n"
      ],
      "metadata": {
        "id": "5Gzkgjdw5_u-"
      },
      "id": "5Gzkgjdw5_u-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***TF-IDF + Multinomial Naive Bayes (3 classes)***"
      ],
      "metadata": {
        "id": "RIe_Y8Lz6DFw"
      },
      "id": "RIe_Y8Lz6DFw"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "nb_tfidf_multi = MultinomialNB()\n",
        "nb_tfidf_multi.fit(X_train_tfidf_multi, y_train_multi)\n",
        "\n",
        "pred_val_nb_tfidf = nb_tfidf_multi.predict(X_val_tfidf_multi)\n",
        "\n",
        "evaluate_multi(\"Naive Bayes (Multinomial)\", \"TF-IDF\", y_val_multi, pred_val_nb_tfidf, le_multi)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVV05-U-6EO6",
        "outputId": "7a7a2464-9f64-46c8-d915-02272132e4a3"
      },
      "id": "IVV05-U-6EO6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Naive Bayes (Multinomial) + TF-IDF ===\n",
            "Accuracy: 0.6345203863452039\n",
            "Macro F1: 0.28640088063166885\n",
            "\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[ 117    5 2573]\n",
            " [  13    3 2332]\n",
            " [  26    8 8486]]\n",
            "Label order: ['easy' 'hard' 'medium']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***TF-IDF + Logistic Regression (3 classes)***"
      ],
      "metadata": {
        "id": "xeA7FIvb6GOD"
      },
      "id": "xeA7FIvb6GOD"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr_tfidf_multi = LogisticRegression(max_iter=2000)\n",
        "lr_tfidf_multi.fit(X_train_tfidf_multi, y_train_multi)\n",
        "\n",
        "pred_val_lr_tfidf = lr_tfidf_multi.predict(X_val_tfidf_multi)\n",
        "\n",
        "evaluate_multi(\"Logistic Regression\", \"TF-IDF\", y_val_multi, pred_val_lr_tfidf, le_multi)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gls-UQsv6Gmt",
        "outputId": "b406c33a-caeb-43bf-d8b7-5eedcf0497cc"
      },
      "id": "Gls-UQsv6Gmt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Logistic Regression + TF-IDF ===\n",
            "Accuracy: 0.6656344466563444\n",
            "Macro F1: 0.43182842804011773\n",
            "\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[1067   24 1604]\n",
            " [ 151   34 2163]\n",
            " [ 496   97 7927]]\n",
            "Label order: ['easy' 'hard' 'medium']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Word2Vec + Gaussian Naive Bayes (3 classes)***"
      ],
      "metadata": {
        "id": "MMubVXrC6Q6P"
      },
      "id": "MMubVXrC6Q6P"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "nb_w2v_multi = GaussianNB()\n",
        "nb_w2v_multi.fit(X_train_w2v_multi, y_train_multi)\n",
        "\n",
        "pred_val_nb_w2v = nb_w2v_multi.predict(X_val_w2v_multi)\n",
        "\n",
        "evaluate_multi(\"Naive Bayes (Gaussian)\", \"Word2Vec\", y_val_multi, pred_val_nb_w2v, le_multi)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byh1RqoS6SYy",
        "outputId": "a9996e33-cf23-415e-f503-ac26d79364e7"
      },
      "id": "byh1RqoS6SYy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Naive Bayes (Gaussian) + Word2Vec ===\n",
            "Accuracy: 0.46988129469881296\n",
            "Macro F1: 0.385821969230117\n",
            "\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[1400  368  927]\n",
            " [ 835  408 1105]\n",
            " [2668 1287 4565]]\n",
            "Label order: ['easy' 'hard' 'medium']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Word2Vec + Logistic Regression (3 classes)***"
      ],
      "metadata": {
        "id": "T2lZMuV_6Vnp"
      },
      "id": "T2lZMuV_6Vnp"
    },
    {
      "cell_type": "code",
      "source": [
        "lr_w2v_multi = LogisticRegression(max_iter=2000)\n",
        "lr_w2v_multi.fit(X_train_w2v_multi, y_train_multi)\n",
        "\n",
        "pred_val_lr_w2v = lr_w2v_multi.predict(X_val_w2v_multi)\n",
        "\n",
        "evaluate_multi(\"Logistic Regression\", \"Word2Vec\", y_val_multi, pred_val_lr_w2v, le_multi)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9E8l5hE66XVc",
        "outputId": "ab4e3c40-de1b-4b3d-f53f-23336e6274f9"
      },
      "id": "9E8l5hE66XVc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Logistic Regression + Word2Vec ===\n",
            "Accuracy: 0.6348890363488904\n",
            "Macro F1: 0.3234048437580385\n",
            "\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[ 329    0 2366]\n",
            " [  54    0 2294]\n",
            " [ 238    0 8282]]\n",
            "Label order: ['easy' 'hard' 'medium']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Since our experiments clearly showed that TF-IDF consistently outperforms Word2Vec across all models and evaluation metrics, we decided to discontinue the use of Word2Vec and proceed exclusively with TF-IDF representations in the following stages***"
      ],
      "metadata": {
        "id": "wLHtEXAg0hlU"
      },
      "id": "wLHtEXAg0hlU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***הגדרת פונקצייה לניסויים בהיפר-פרמטרים***"
      ],
      "metadata": {
        "id": "leffEAvJ9v08"
      },
      "id": "leffEAvJ9v08"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def evaluate_scores(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute accuracy and macro F1 score.\n",
        "    \"\"\"\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
        "    return acc, f1\n",
        "\n",
        "\n",
        "def tune_nb_tfidf(X_train, y_train, X_val, y_val, alphas, representation_name=\"TF-IDF\"):\n",
        "    \"\"\"\n",
        "    Hyperparameter tuning for Multinomial Naive Bayes on TF-IDF features.\n",
        "    Varies the smoothing parameter 'alpha' and prints validation performance.\n",
        "    Returns a list of results (alpha, accuracy, f1_macro).\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    print(f\"\\n=== Naive Bayes (Multinomial) + {representation_name} — alpha sweep ===\")\n",
        "    for a in alphas:\n",
        "        model = MultinomialNB(alpha=a)\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_val)\n",
        "        acc, f1 = evaluate_scores(y_val, y_pred)\n",
        "        results.append({\"alpha\": a, \"accuracy\": acc, \"f1_macro\": f1})\n",
        "        print(f\"alpha = {a:>4}  ->  Accuracy = {acc:.4f},  Macro F1 = {f1:.4f}\")\n",
        "    # Print best by F1\n",
        "    best = max(results, key=lambda r: r[\"f1_macro\"])\n",
        "    print(f\"\\nBest alpha by macro F1: {best['alpha']} (Accuracy={best['accuracy']:.4f}, F1={best['f1_macro']:.4f})\")\n",
        "    return results\n",
        "\n",
        "\n",
        "def tune_logistic(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    X_val,\n",
        "    y_val,\n",
        "    Cs,\n",
        "    max_iter=1000,\n",
        "    representation_name=\"TF-IDF\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Hyperparameter tuning for Logistic Regression on TF-IDF features.\n",
        "    Varies the regularization strength C and prints validation performance.\n",
        "    Returns a list of results (C, accuracy, f1_macro).\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    print(f\"\\n=== Logistic Regression + {representation_name} — C sweep (max_iter={max_iter}) ===\")\n",
        "    for c in Cs:\n",
        "        clf = LogisticRegression(C=c, max_iter=max_iter)\n",
        "        clf.fit(X_train, y_train)\n",
        "        y_pred = clf.predict(X_val)\n",
        "        acc, f1 = evaluate_scores(y_val, y_pred)\n",
        "        results.append({\"C\": c, \"accuracy\": acc, \"f1_macro\": f1})\n",
        "        print(f\"C = {c:>5}  ->  Accuracy = {acc:.4f},  Macro F1 = {f1:.4f}\")\n",
        "    # Print best by F1\n",
        "    best = max(results, key=lambda r: r[\"f1_macro\"])\n",
        "    print(f\"\\nBest C by macro F1: {best['C']} (Accuracy={best['accuracy']:.4f}, F1={best['f1_macro']:.4f})\")\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "NSL6hdnX910h"
      },
      "id": "NSL6hdnX910h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **תזכורת:**"
      ],
      "metadata": {
        "id": "krPxzvEhA9OL"
      },
      "id": "krPxzvEhA9OL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✔ Accuracy (דיוק)\n",
        "כמה אחוז מהניבואים של המודל היו נכונים מתוך כלל הדוגמאות.\n",
        "\n",
        "**איך להבין את זה?**  \n",
        "אם המודל ניחש נכון 70% מהפעמים → Accuracy = 0.70\n",
        "\n",
        "**מתי זה טוב?**  \n",
        "כאשר הדאטה מאוזן*\n",
        "(כל המחלקות מופיעות בערך באותה כמות).\n",
        "\n",
        "**החיסרון:**  \n",
        "אם מחלקה אחת מופיעה הרבה יותר – המדד עלול להיות מטעה.\n",
        "\n",
        "---\n",
        "\n",
        "### ✔ F1 Score (מדד F1)\n",
        "מדד שמחבר בין\n",
        " Precision ו־Recall\n",
        "  למדד אחד מאוזן.\n",
        "\n",
        "**איך להבין את זה?**  \n",
        " גבוה = המודל גם מוצא נכון דוגמאות של המחלקה וגם לא טועה הרבה.  \n",
        " נמוך = או שהמודל מפספס הרבה דוגמאות, או שהוא טועה הרבה.\n",
        "\n",
        "**מתי משתמשים בו?**  \n",
        "כאשר חשוב לזהות כל מחלקה בצורה טובה במיוחד,\n",
        "או כאשר יש אי־איזון בין המחלקות.\n",
        "\n",
        "---\n",
        "\n",
        "### ✔ Macro F1 (מדד F1 מאקרו)\n",
        "מחשב את ה\n",
        "F1\n",
        " לכל מחלקה בנפרד, ואז עושה ממוצע פשוט ביניהן.\n",
        "\n",
        "**איך להבין את זה?**  \n",
        "כל מחלקה מקבלת משקל שווה — גם אם יש ממנה מעט דוגמאות.\n",
        "\n",
        "**למה זה חשוב?**  \n",
        "כי בבעיות שבהן חלק מהמחלקות מופיעות מעט ,  \n",
        "Accuracy\n",
        " יכול להטעות,\n",
        "אבל\n",
        "Macro F1\n",
        "מוודא שהמודל מצליח גם על המחלקות הקטנות.\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "XKxwMcfsBAUN"
      },
      "id": "XKxwMcfsBAUN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***ניסויים בהיפר פרמטרים***"
      ],
      "metadata": {
        "id": "dDp0khU2_rSd"
      },
      "id": "dDp0khU2_rSd"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Additional Hyperparameter Experiments (TF-IDF only)\n",
        "# ============================================\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 1) Naive Bayes + TF-IDF with more alpha values\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "nb_alphas_extended = [0.01, 0.1, 0.5, 1.0, 2.0]\n",
        "nb_tfidf_results_extended = tune_nb_tfidf(\n",
        "    X_train_tfidf_multi,\n",
        "    y_train_multi,\n",
        "    X_val_tfidf_multi,\n",
        "    y_val_multi,\n",
        "    alphas=nb_alphas_extended,\n",
        "    representation_name=\"TF-IDF (multi-class) — extended alpha\"\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 2) Logistic Regression + TF-IDF with extended C values\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "lr_C_extended = [0.01, 0.1, 1.0, 10.0, 50.0, 100.0]\n",
        "lr_tfidf_results_extended = tune_logistic(\n",
        "    X_train_tfidf_multi,\n",
        "    y_train_multi,\n",
        "    X_val_tfidf_multi,\n",
        "    y_val_multi,\n",
        "    Cs=lr_C_extended,\n",
        "    max_iter=3000,  # slightly higher, helps convergence\n",
        "    representation_name=\"TF-IDF (multi-class) — extended C\",\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 3) Logistic Regression + TF-IDF — small max_iter test\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "lr_tfidf_small_iter = tune_logistic(\n",
        "    X_train_tfidf_multi,\n",
        "    y_train_multi,\n",
        "    X_val_tfidf_multi,\n",
        "    y_val_multi,\n",
        "    Cs=[1.0],\n",
        "    max_iter=200,  # very small to check convergence behavior\n",
        "    representation_name=\"TF-IDF (multi-class) — small max_iter\",\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 4) Logistic Regression + TF-IDF — large max_iter test\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "lr_tfidf_large_iter = tune_logistic(\n",
        "    X_train_tfidf_multi,\n",
        "    y_train_multi,\n",
        "    X_val_tfidf_multi,\n",
        "    y_val_multi,\n",
        "    Cs=[1.0],\n",
        "    max_iter=5000,  # large enough to guarantee convergence\n",
        "    representation_name=\"TF-IDF (multi-class) — large max_iter\",\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FpUkxNY_uvG",
        "outputId": "4152858b-b287-4c48-f66a-6b8dab07abd5"
      },
      "id": "-FpUkxNY_uvG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Naive Bayes (Multinomial) + TF-IDF (multi-class) — extended alpha — alpha sweep ===\n",
            "alpha = 0.01  ->  Accuracy = 0.6325,  Macro F1 = 0.3088\n",
            "alpha =  0.1  ->  Accuracy = 0.6330,  Macro F1 = 0.3071\n",
            "alpha =  0.5  ->  Accuracy = 0.6347,  Macro F1 = 0.2962\n",
            "alpha =  1.0  ->  Accuracy = 0.6345,  Macro F1 = 0.2864\n",
            "alpha =  2.0  ->  Accuracy = 0.6328,  Macro F1 = 0.2750\n",
            "\n",
            "Best alpha by macro F1: 0.01 (Accuracy=0.6325, F1=0.3088)\n",
            "\n",
            "=== Logistic Regression + TF-IDF (multi-class) — extended C — C sweep (max_iter=3000) ===\n",
            "C =  0.01  ->  Accuracy = 0.6306,  Macro F1 = 0.2659\n",
            "C =   0.1  ->  Accuracy = 0.6661,  Macro F1 = 0.3947\n",
            "C =   1.0  ->  Accuracy = 0.6656,  Macro F1 = 0.4318\n",
            "C =  10.0  ->  Accuracy = 0.6293,  Macro F1 = 0.4513\n",
            "C =  50.0  ->  Accuracy = 0.6128,  Macro F1 = 0.4494\n",
            "C = 100.0  ->  Accuracy = 0.6104,  Macro F1 = 0.4519\n",
            "\n",
            "Best C by macro F1: 100.0 (Accuracy=0.6104, F1=0.4519)\n",
            "\n",
            "=== Logistic Regression + TF-IDF (multi-class) — small max_iter — C sweep (max_iter=200) ===\n",
            "C =   1.0  ->  Accuracy = 0.6656,  Macro F1 = 0.4318\n",
            "\n",
            "Best C by macro F1: 1.0 (Accuracy=0.6656, F1=0.4318)\n",
            "\n",
            "=== Logistic Regression + TF-IDF (multi-class) — large max_iter — C sweep (max_iter=5000) ===\n",
            "C =   1.0  ->  Accuracy = 0.6656,  Macro F1 = 0.4318\n",
            "\n",
            "Best C by macro F1: 1.0 (Accuracy=0.6656, F1=0.4318)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***עד לפה זה החדש =========================================================================================================================================================================================================***"
      ],
      "metadata": {
        "id": "sIysXP7YjYPj"
      },
      "id": "sIysXP7YjYPj"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}