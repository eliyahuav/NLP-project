{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***תרגיל 6 של הפרוייקט***"
      ],
      "metadata": {
        "id": "BWDmLmxFJX1B"
      },
      "id": "BWDmLmxFJX1B"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup & Install (Colab)**"
      ],
      "metadata": {
        "id": "h8wNbiD5cRou"
      },
      "id": "h8wNbiD5cRou"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Block 1 — Setup & Install (Colab)\n",
        "# ============================================================\n",
        "!pip -q install transformers datasets accelerate scikit-learn\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "CvwC7zZgb3V2",
        "outputId": "ccfc76df-7b0d-4bf2-a2d3-1b3bc92844ae"
      },
      "id": "CvwC7zZgb3V2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'AdamW' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-515994734.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m from transformers import (\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'AdamW' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reproducibility (Seeds)**"
      ],
      "metadata": {
        "id": "Ej2-eVH7cUq0"
      },
      "id": "Ej2-eVH7cUq0"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Block 2 — Reproducibility (Seeds)\n",
        "# ============================================================\n",
        "def set_seed(seed: int = 42) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n"
      ],
      "metadata": {
        "id": "TkmvAc_-b9i6"
      },
      "id": "TkmvAc_-b9i6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Config (Paths, Labels, Hyperparams)**"
      ],
      "metadata": {
        "id": "BNWLB8UEcWBz"
      },
      "id": "BNWLB8UEcWBz"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Block 3 — Config (Paths, Labels, Data Rules)\n",
        "# ============================================================\n",
        "CSV_PATH = \"/content/train-filtered_question_level.csv\"  # change if needed\n",
        "\n",
        "label2id = {\"easy\": 0, \"medium\": 1, \"hard\": 2}\n",
        "id2label = {v: k for k, v in label2id.items()}\n",
        "NUM_LABELS = 3\n",
        "\n",
        "# Based on your analysis: 95th percentile = 44 tokens\n",
        "MAX_LEN = 44\n",
        "\n",
        "# Data rules you decided\n",
        "SAMPLES_PER_CLASS = 7000\n",
        "\n",
        "# Split ratios (exact counts will be enforced per class)\n",
        "TRAIN_RATIO = 0.70\n",
        "VAL_RATIO   = 0.15\n",
        "TEST_RATIO  = 0.15\n"
      ],
      "metadata": {
        "id": "lCzXeYvVb_Dv"
      },
      "id": "lCzXeYvVb_Dv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Dataset (CSV) + Minimal Cleaning**"
      ],
      "metadata": {
        "id": "SOvb6N2gcZM5"
      },
      "id": "SOvb6N2gcZM5"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Block 4 — Load Dataset + Minimal Cleaning (NO manual lowercasing)\n",
        "# ============================================================\n",
        "if not os.path.exists(CSV_PATH):\n",
        "    raise FileNotFoundError(f\"CSV not found at: {CSV_PATH}\")\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "required_cols = {\"question\", \"level\"}\n",
        "if not required_cols.issubset(set(df.columns)):\n",
        "    raise ValueError(f\"CSV must contain columns: {required_cols}. Found: {df.columns}\")\n",
        "\n",
        "# Keep preprocessing minimal for Transformers\n",
        "df[\"question\"] = df[\"question\"].astype(str)\n",
        "df[\"level\"] = df[\"level\"].astype(str)\n",
        "\n",
        "# Remove duplicates by question\n",
        "df = df.drop_duplicates(subset=[\"question\"]).reset_index(drop=True)\n",
        "\n",
        "# Keep only valid labels\n",
        "df = df[df[\"level\"].isin(label2id.keys())].reset_index(drop=True)\n",
        "\n",
        "print(\"Raw dataset size:\", len(df))\n",
        "print(df[\"level\"].value_counts())\n"
      ],
      "metadata": {
        "id": "L79BYeCicAYb"
      },
      "id": "L79BYeCicAYb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Block 4.5 — Balance Dataset (Exactly 7000 per label) + Shuffle\n",
        "# ============================================================\n",
        "counts = df[\"level\"].value_counts()\n",
        "missing = [lbl for lbl in label2id.keys() if counts.get(lbl, 0) < SAMPLES_PER_CLASS]\n",
        "if missing:\n",
        "    raise ValueError(\n",
        "        f\"Not enough samples for labels: {missing}. \"\n",
        "        f\"Counts: {counts.to_dict()}\"\n",
        "    )\n",
        "\n",
        "balanced_df = (\n",
        "    df.groupby(\"level\", group_keys=False)\n",
        "      .apply(lambda g: g.sample(n=SAMPLES_PER_CLASS, random_state=42))\n",
        "      .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# Full shuffle after balancing\n",
        "df = balanced_df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"Balanced dataset size:\", len(df))\n",
        "print(df[\"level\"].value_counts())\n"
      ],
      "metadata": {
        "id": "-FYl2b13e-_H"
      },
      "id": "-FYl2b13e-_H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stratified Split (Train / Val / Test)**"
      ],
      "metadata": {
        "id": "-YPtv_dTccMV"
      },
      "id": "-YPtv_dTccMV"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Block 5 — Exact Balanced Split (Equal per label in Train/Val/Test)\n",
        "# ============================================================\n",
        "train_per_class = int(SAMPLES_PER_CLASS * TRAIN_RATIO)  # 4900\n",
        "val_per_class   = int(SAMPLES_PER_CLASS * VAL_RATIO)    # 1050\n",
        "test_per_class  = SAMPLES_PER_CLASS - train_per_class - val_per_class  # 1050\n",
        "\n",
        "assert train_per_class + val_per_class + test_per_class == SAMPLES_PER_CLASS\n",
        "\n",
        "def split_one_class(g: pd.DataFrame, seed: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    g = g.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
        "    train_g = g.iloc[:train_per_class]\n",
        "    val_g   = g.iloc[train_per_class:train_per_class + val_per_class]\n",
        "    test_g  = g.iloc[train_per_class + val_per_class:train_per_class + val_per_class + test_per_class]\n",
        "    return train_g, val_g, test_g\n",
        "\n",
        "train_parts, val_parts, test_parts = [], [], []\n",
        "\n",
        "for label in label2id.keys():\n",
        "    g = df[df[\"level\"] == label].reset_index(drop=True)\n",
        "    if len(g) != SAMPLES_PER_CLASS:\n",
        "        raise ValueError(f\"Expected {SAMPLES_PER_CLASS} samples for label={label}, got {len(g)}\")\n",
        "\n",
        "    tr_g, va_g, te_g = split_one_class(g, seed=42)\n",
        "    train_parts.append(tr_g)\n",
        "    val_parts.append(va_g)\n",
        "    test_parts.append(te_g)\n",
        "\n",
        "train_df = pd.concat(train_parts).sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "val_df   = pd.concat(val_parts).sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "test_df  = pd.concat(test_parts).sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"Train:\", len(train_df), \"Val:\", len(val_df), \"Test:\", len(test_df))\n",
        "print(\"Train distribution:\\n\", train_df[\"level\"].value_counts())\n",
        "print(\"Val distribution:\\n\", val_df[\"level\"].value_counts())\n",
        "print(\"Test distribution:\\n\", test_df[\"level\"].value_counts())\n"
      ],
      "metadata": {
        "id": "ZWMxtuuFcBzb"
      },
      "id": "ZWMxtuuFcBzb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PyTorch Dataset (Tokenizer-based)**"
      ],
      "metadata": {
        "id": "JGFIo_OJceej"
      },
      "id": "JGFIo_OJceej"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Block 6 — PyTorch Dataset (Tokenizer-based; NO manual vocab)\n",
        "# ============================================================\n",
        "class TriviaDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Tokenizes each question using the EXACT tokenizer of the chosen Transformer model.\n",
        "    Returns input_ids, attention_mask, and labels.\n",
        "    \"\"\"\n",
        "    def __init__(self, dataframe: pd.DataFrame, tokenizer, max_len: int):\n",
        "        self.df = dataframe.reset_index(drop=True)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        text = self.df.loc[idx, \"question\"]\n",
        "        label_str = self.df.loc[idx, \"level\"]\n",
        "        label_id = label2id[label_str]\n",
        "\n",
        "        encoded = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",     # explicit padding\n",
        "            truncation=True,          # explicit truncation\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        item = {\n",
        "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(label_id, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "        # BERT models return token_type_ids (segment ids)\n",
        "        if \"token_type_ids\" in encoded:\n",
        "            item[\"token_type_ids\"] = encoded[\"token_type_ids\"].squeeze(0)\n",
        "\n",
        "        return item\n"
      ],
      "metadata": {
        "id": "78iImGWocDRj"
      },
      "id": "78iImGWocDRj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Utilities (Accuracy, Loops, Plots)**"
      ],
      "metadata": {
        "id": "vRvS6KAHchh9"
      },
      "id": "vRvS6KAHchh9"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Block 7 — Training Utilities (Loss/Acc history + plots)\n",
        "# ============================================================\n",
        "def batch_accuracy(logits: torch.Tensor, labels: torch.Tensor) -> float:\n",
        "    preds = torch.argmax(logits, dim=1)\n",
        "    return (preds == labels).float().mean().item()\n",
        "\n",
        "@dataclass\n",
        "class TrainHistory:\n",
        "    train_loss: List[float]\n",
        "    val_loss: List[float]\n",
        "    train_acc: List[float]\n",
        "    val_acc: List[float]\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, scheduler) -> Tuple[float, float]:\n",
        "    model.train()\n",
        "    total_loss, total_acc, n_batches = 0.0, 0.0, 0\n",
        "\n",
        "    for batch in loader:\n",
        "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_acc += batch_accuracy(logits, batch[\"labels\"])\n",
        "        n_batches += 1\n",
        "\n",
        "    return total_loss / n_batches, total_acc / n_batches\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_one_epoch(model, loader) -> Tuple[float, float]:\n",
        "    model.eval()\n",
        "    total_loss, total_acc, n_batches = 0.0, 0.0, 0\n",
        "\n",
        "    for batch in loader:\n",
        "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_acc += batch_accuracy(logits, batch[\"labels\"])\n",
        "        n_batches += 1\n",
        "\n",
        "    return total_loss / n_batches, total_acc / n_batches\n",
        "\n",
        "def plot_history(history: TrainHistory, title: str) -> None:\n",
        "    epochs = range(1, len(history.train_loss) + 1)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, history.train_loss, label=\"train_loss\")\n",
        "    plt.plot(epochs, history.val_loss, label=\"val_loss\")\n",
        "    plt.title(title + \" — Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, history.train_acc, label=\"train_acc\")\n",
        "    plt.plot(epochs, history.val_acc, label=\"val_acc\")\n",
        "    plt.title(title + \" — Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "ZxYGQymWcFgd"
      },
      "id": "ZxYGQymWcFgd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation Utilities (Report + Confusion Matrix)**"
      ],
      "metadata": {
        "id": "kq3AzIkOckyA"
      },
      "id": "kq3AzIkOckyA"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Block 8 — Evaluation Utilities (Report + Confusion Matrix)\n",
        "# ============================================================\n",
        "@torch.no_grad()\n",
        "def predict(model, loader) -> Tuple[List[int], List[int]]:\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    for batch in loader:\n",
        "        labels_cpu = batch[\"labels\"].numpy().tolist()\n",
        "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "\n",
        "        outputs = model(**batch)\n",
        "        preds = torch.argmax(outputs.logits, dim=1).cpu().numpy().tolist()\n",
        "\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels_cpu)\n",
        "\n",
        "    return all_preds, all_labels\n",
        "\n",
        "def show_metrics(y_true: List[int], y_pred: List[int], title: str) -> None:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(title)\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(\n",
        "        y_true, y_pred,\n",
        "        target_names=[id2label[i] for i in range(NUM_LABELS)],\n",
        "        digits=4\n",
        "    ))\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(\"\\nConfusion Matrix (rows=true, cols=pred):\")\n",
        "    print(cm)\n"
      ],
      "metadata": {
        "id": "nqO46K4RcHHJ"
      },
      "id": "nqO46K4RcHHJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One Experiment Runner (Model + Tokenizer + Fine-Tuning)**"
      ],
      "metadata": {
        "id": "4VR5VyqscnWA"
      },
      "id": "4VR5VyqscnWA"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Block 9 — Experiment Runner (model + tokenizer + full fine-tuning)\n",
        "# ============================================================\n",
        "def run_experiment(\n",
        "    model_name: str,\n",
        "    run_name: str,\n",
        "    lr: float,\n",
        "    batch_size: int,\n",
        "    epochs: int,\n",
        "    optimizer_name: str = \"adamw\",\n",
        "    weight_decay: float = 0.01,\n",
        "    warmup_ratio: float = 0.10\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Runs one full fine-tuning experiment and returns metrics + trained model.\n",
        "    Designed to make hyperparameter experiments easy.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"#\"*90)\n",
        "    print(f\"Run: {run_name}\")\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Params: lr={lr}, batch={batch_size}, epochs={epochs}, optimizer={optimizer_name}, MAX_LEN={MAX_LEN}\")\n",
        "    print(\"#\"*90)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    train_ds = TriviaDataset(train_df, tokenizer, MAX_LEN)\n",
        "    val_ds   = TriviaDataset(val_df, tokenizer, MAX_LEN)\n",
        "    test_ds  = TriviaDataset(test_df, tokenizer, MAX_LEN)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    val_loader   = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "    test_loader  = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=NUM_LABELS,\n",
        "        label2id=label2id,\n",
        "        id2label=id2label\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # Optimizer choice (as requested by lecturer: Adam vs AdamW)\n",
        "    opt = optimizer_name.lower()\n",
        "    if opt == \"adamw\":\n",
        "        optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    elif opt == \"adam\":\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    else:\n",
        "        raise ValueError(\"optimizer_name must be 'adamw' or 'adam'\")\n",
        "\n",
        "    total_steps = len(train_loader) * epochs\n",
        "    warmup_steps = int(total_steps * warmup_ratio)\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=warmup_steps,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    history = TrainHistory(train_loss=[], val_loss=[], train_acc=[], val_acc=[])\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, scheduler)\n",
        "        va_loss, va_acc = eval_one_epoch(model, val_loader)\n",
        "\n",
        "        history.train_loss.append(tr_loss)\n",
        "        history.train_acc.append(tr_acc)\n",
        "        history.val_loss.append(va_loss)\n",
        "        history.val_acc.append(va_acc)\n",
        "\n",
        "        print(f\"Epoch {ep}/{epochs} | train_loss={tr_loss:.4f} train_acc={tr_acc:.4f} | val_loss={va_loss:.4f} val_acc={va_acc:.4f}\")\n",
        "\n",
        "    plot_history(history, title=run_name)\n",
        "\n",
        "    # Test evaluation\n",
        "    y_pred, y_true = predict(model, test_loader)\n",
        "    show_metrics(y_true, y_pred, title=f\"{run_name} — Test Evaluation\")\n",
        "\n",
        "    return {\n",
        "        \"run_name\": run_name,\n",
        "        \"model_name\": model_name,\n",
        "        \"lr\": lr,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": epochs,\n",
        "        \"optimizer\": optimizer_name,\n",
        "        \"history\": history,\n",
        "        \"y_true\": y_true,\n",
        "        \"y_pred\": y_pred,\n",
        "        \"model\": model,\n",
        "        \"tokenizer\": tokenizer\n",
        "    }\n"
      ],
      "metadata": {
        "id": "wfYxUJW2cHpd"
      },
      "id": "wfYxUJW2cHpd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "B3J-C7UJWJiZ"
      },
      "id": "B3J-C7UJWJiZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Block 10 — Experiment Manager (Add ONE block per experiment)\n",
        "# ============================================================\n",
        "SMALL_MODEL = \"bert-base-cased\"\n",
        "LARGE_MODEL = \"bert-large-cased\"\n",
        "\n",
        "# Each experiment is ONE dict.\n",
        "# To add a new experiment, you only add ONE new dict line.\n",
        "EXPERIMENTS = [\n",
        "    # ---- Small model (BERT base) runs ----\n",
        "    {\"run_name\": \"E1_BERT_BASE_adamw_lr3e-5_bs16_ep3\", \"model\": SMALL_MODEL, \"lr\": 3e-5, \"bs\": 16, \"ep\": 3, \"opt\": \"adamw\"},\n",
        "]\n",
        "\n",
        "results = []\n",
        "for exp in EXPERIMENTS:\n",
        "    out = run_experiment(\n",
        "        model_name=exp[\"model\"],\n",
        "        run_name=exp[\"run_name\"],\n",
        "        lr=exp[\"lr\"],\n",
        "        batch_size=exp[\"bs\"],\n",
        "        epochs=exp[\"ep\"],\n",
        "        optimizer_name=exp[\"opt\"]\n",
        "    )\n",
        "    results.append(out)\n"
      ],
      "metadata": {
        "id": "-8YdpKNcWKB2"
      },
      "id": "-8YdpKNcWKB2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "דוגמאותתתתתתתתתתתתתתתתתתתתת"
      ],
      "metadata": {
        "id": "B0ziPQzyWOQh"
      },
      "id": "B0ziPQzyWOQh"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Block 11 — Add More Experiments (Copy/Paste ONE dict per run)\n",
        "# ============================================================\n",
        "# Example experiments requested by lecturer:\n",
        "# - change LR\n",
        "# - change batch size\n",
        "# - change epochs\n",
        "# - change optimizer\n",
        "\n",
        "EXPERIMENTS_MORE = [\n",
        "    # LR experiments (keep everything else same)\n",
        "    {\"run_name\": \"E2_BERT_BASE_adamw_lr2e-5_bs16_ep3\", \"model\": SMALL_MODEL, \"lr\": 2e-5, \"bs\": 16, \"ep\": 3, \"opt\": \"adamw\"},\n",
        "    {\"run_name\": \"E3_BERT_BASE_adamw_lr5e-5_bs16_ep3\", \"model\": SMALL_MODEL, \"lr\": 5e-5, \"bs\": 16, \"ep\": 3, \"opt\": \"adamw\"},\n",
        "\n",
        "    # Batch size experiment (if GPU allows)\n",
        "    {\"run_name\": \"E4_BERT_BASE_adamw_lr3e-5_bs32_ep3\", \"model\": SMALL_MODEL, \"lr\": 3e-5, \"bs\": 32, \"ep\": 3, \"opt\": \"adamw\"},\n",
        "\n",
        "    # Epochs experiments\n",
        "    {\"run_name\": \"E5_BERT_BASE_adamw_lr3e-5_bs16_ep2\", \"model\": SMALL_MODEL, \"lr\": 3e-5, \"bs\": 16, \"ep\": 2, \"opt\": \"adamw\"},\n",
        "    {\"run_name\": \"E6_BERT_BASE_adamw_lr3e-5_bs16_ep4\", \"model\": SMALL_MODEL, \"lr\": 3e-5, \"bs\": 16, \"ep\": 4, \"opt\": \"adamw\"},\n",
        "\n",
        "    # Optimizer comparison\n",
        "    {\"run_name\": \"E7_BERT_BASE_adam_lr3e-5_bs16_ep3\",  \"model\": SMALL_MODEL, \"lr\": 3e-5, \"bs\": 16, \"ep\": 3, \"opt\": \"adam\"},\n",
        "]\n",
        "\n",
        "for exp in EXPERIMENTS_MORE:\n",
        "    out = run_experiment(\n",
        "        model_name=exp[\"model\"],\n",
        "        run_name=exp[\"run_name\"],\n",
        "        lr=exp[\"lr\"],\n",
        "        batch_size=exp[\"bs\"],\n",
        "        epochs=exp[\"ep\"],\n",
        "        optimizer_name=exp[\"opt\"]\n",
        "    )\n",
        "    results.append(out)\n"
      ],
      "metadata": {
        "id": "rkrQTvffWOis"
      },
      "id": "rkrQTvffWOis",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Large Model Run"
      ],
      "metadata": {
        "id": "T58gZ0DDWQZL"
      },
      "id": "T58gZ0DDWQZL"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Block 12 — Large Model Run (Use the best config from BERT-base)\n",
        "# ============================================================\n",
        "# After you decide what is \"best\" from BERT-base runs, run BERT-large once or twice.\n",
        "# Start conservative: smaller batch and slightly lower LR for stability.\n",
        "\n",
        "EXPERIMENT_LARGE = [\n",
        "    {\"run_name\": \"E8_BERT_LARGE_adamw_lr2e-5_bs8_ep3\", \"model\": LARGE_MODEL, \"lr\": 2e-5, \"bs\": 8, \"ep\": 3, \"opt\": \"adamw\"},\n",
        "    # Optional: an even safer LR if needed\n",
        "    # {\"run_name\": \"E9_BERT_LARGE_adamw_lr1e-5_bs8_ep3\", \"model\": LARGE_MODEL, \"lr\": 1e-5, \"bs\": 8, \"ep\": 3, \"opt\": \"adamw\"},\n",
        "]\n",
        "\n",
        "for exp in EXPERIMENT_LARGE:\n",
        "    out = run_experiment(\n",
        "        model_name=exp[\"model\"],\n",
        "        run_name=exp[\"run_name\"],\n",
        "        lr=exp[\"lr\"],\n",
        "        batch_size=exp[\"bs\"],\n",
        "        epochs=exp[\"ep\"],\n",
        "        optimizer_name=exp[\"opt\"]\n",
        "    )\n",
        "    results.append(out)\n"
      ],
      "metadata": {
        "id": "vz5V46kmWQ4J"
      },
      "id": "vz5V46kmWQ4J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results Table"
      ],
      "metadata": {
        "id": "49TfNRD7WTNR"
      },
      "id": "49TfNRD7WTNR"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Block 13 — (Optional) Results Table (Quick comparison)\n",
        "# ============================================================\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "summary_rows = []\n",
        "for r in results:\n",
        "    acc = accuracy_score(r[\"y_true\"], r[\"y_pred\"])\n",
        "    summary_rows.append({\n",
        "        \"run_name\": r[\"run_name\"],\n",
        "        \"model\": r[\"model_name\"],\n",
        "        \"lr\": r[\"lr\"],\n",
        "        \"batch\": r[\"batch_size\"],\n",
        "        \"epochs\": r[\"epochs\"],\n",
        "        \"optimizer\": r[\"optimizer\"],\n",
        "        \"test_acc\": acc\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_rows).sort_values(by=\"test_acc\", ascending=False)\n",
        "summary_df\n"
      ],
      "metadata": {
        "id": "5Zl53sDwWTf4"
      },
      "id": "5Zl53sDwWTf4",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}